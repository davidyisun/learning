{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/env python\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0005\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=20\n",
      "NUM_FILTERS=64\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "# validation数据集占比\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "# 正样本\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "# 负样本\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "# 词向量长度\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "# 卷积核大小\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "# 每一种卷积核个数\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 64, \"Number of filters per filter size (default: 128)\")\n",
    "# dropout参数\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "# l2正则化参数\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0005, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "# 批次大小\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "# 迭代周期\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 20, \"Number of training epochs (default: 200)\")\n",
    "# 多少step测试一次\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "# 多少step保存一次模型\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# 最多保存多少个模型\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "# tensorFlow 会自动选择一个存在并且支持的设备来运行 operation\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# 获取你的 operations 和 Tensor 被指派到哪个设备上运行\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "# flags解析\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "\n",
    "# 打印所有参数\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "('x_shape:', (10662, 56))\n",
      "('y_shape:', (10662, 2))\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n",
      "('x:', array([[ 4719,    59,   182,    34,   190,   804,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  129,  7044,   284,   146,    80,     3,  1116,    58,    84,\n",
      "         1386,   182,  1968,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  146,     3,   453,    88,    34,     1,   190,  8338, 18328,\n",
      "           12,  2320,  1573,  3840,  6569, 16227,   112,  1543,   246,\n",
      "           17,  1722,  5117,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [ 2718,   149,  7850,   503,   343,    87,  6515,   250,  2934,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0],\n",
      "       [  343,   133,     1,  1528,    34,  2691,   643, 16667,   125,\n",
      "            1,  4435,    34,   983,   740,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]]))\n",
      "('y:', array([[1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0],\n",
      "       [1, 0]]))\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "\n",
    "# Build vocabulary\n",
    "# 一行数据最多的词汇数\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(\"x_shape:\",x.shape)\n",
    "print(\"y_shape:\",y.shape)\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "# 数据集切分为两部分\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "print(\"x:\",x_train[0:5])\n",
    "print(\"y:\",y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511\n",
      "\n",
      "2017-06-30T10:08:33.660163: step 1, loss 0.892609, acc 0.3125\n",
      "2017-06-30T10:08:33.776272: step 2, loss 0.685469, acc 0.609375\n",
      "2017-06-30T10:08:33.893226: step 3, loss 0.76926, acc 0.484375\n",
      "2017-06-30T10:08:34.011040: step 4, loss 0.792836, acc 0.46875\n",
      "2017-06-30T10:08:34.130549: step 5, loss 0.687928, acc 0.609375\n",
      "2017-06-30T10:08:34.248218: step 6, loss 0.828383, acc 0.46875\n",
      "2017-06-30T10:08:34.365064: step 7, loss 0.746788, acc 0.53125\n",
      "2017-06-30T10:08:34.485105: step 8, loss 0.706887, acc 0.5625\n",
      "2017-06-30T10:08:34.602145: step 9, loss 0.756992, acc 0.546875\n",
      "2017-06-30T10:08:34.716536: step 10, loss 0.826606, acc 0.4375\n",
      "2017-06-30T10:08:34.835421: step 11, loss 0.775482, acc 0.515625\n",
      "2017-06-30T10:08:34.954225: step 12, loss 0.780587, acc 0.484375\n",
      "2017-06-30T10:08:35.070627: step 13, loss 0.774108, acc 0.53125\n",
      "2017-06-30T10:08:35.183381: step 14, loss 0.811075, acc 0.40625\n",
      "2017-06-30T10:08:35.305033: step 15, loss 0.778382, acc 0.5\n",
      "2017-06-30T10:08:35.423105: step 16, loss 0.744057, acc 0.53125\n",
      "2017-06-30T10:08:35.540773: step 17, loss 0.767885, acc 0.484375\n",
      "2017-06-30T10:08:35.657444: step 18, loss 0.744845, acc 0.5625\n",
      "2017-06-30T10:08:35.773325: step 19, loss 0.697381, acc 0.59375\n",
      "2017-06-30T10:08:35.891296: step 20, loss 0.805201, acc 0.421875\n",
      "2017-06-30T10:08:36.009126: step 21, loss 0.811007, acc 0.46875\n",
      "2017-06-30T10:08:36.125608: step 22, loss 0.834568, acc 0.453125\n",
      "2017-06-30T10:08:36.240936: step 23, loss 0.775554, acc 0.515625\n",
      "2017-06-30T10:08:36.357934: step 24, loss 0.769385, acc 0.5\n",
      "2017-06-30T10:08:36.472899: step 25, loss 0.747258, acc 0.546875\n",
      "2017-06-30T10:08:36.588230: step 26, loss 0.784209, acc 0.46875\n",
      "2017-06-30T10:08:36.704868: step 27, loss 0.742621, acc 0.546875\n",
      "2017-06-30T10:08:36.821600: step 28, loss 0.726382, acc 0.5625\n",
      "2017-06-30T10:08:36.935766: step 29, loss 0.704339, acc 0.59375\n",
      "2017-06-30T10:08:37.052794: step 30, loss 0.679005, acc 0.578125\n",
      "2017-06-30T10:08:37.174029: step 31, loss 0.690884, acc 0.578125\n",
      "2017-06-30T10:08:37.292886: step 32, loss 0.704754, acc 0.546875\n",
      "2017-06-30T10:08:37.413174: step 33, loss 0.757615, acc 0.515625\n",
      "2017-06-30T10:08:37.528648: step 34, loss 0.827433, acc 0.46875\n",
      "2017-06-30T10:08:37.640853: step 35, loss 0.781317, acc 0.515625\n",
      "2017-06-30T10:08:37.760059: step 36, loss 0.754329, acc 0.53125\n",
      "2017-06-30T10:08:37.878435: step 37, loss 0.711626, acc 0.578125\n",
      "2017-06-30T10:08:37.991662: step 38, loss 0.70937, acc 0.59375\n",
      "2017-06-30T10:08:38.108682: step 39, loss 0.908545, acc 0.375\n",
      "2017-06-30T10:08:38.224411: step 40, loss 0.765343, acc 0.484375\n",
      "2017-06-30T10:08:38.342098: step 41, loss 0.74687, acc 0.546875\n",
      "2017-06-30T10:08:38.456074: step 42, loss 0.748045, acc 0.546875\n",
      "2017-06-30T10:08:38.575932: step 43, loss 0.828476, acc 0.46875\n",
      "2017-06-30T10:08:38.691277: step 44, loss 0.757163, acc 0.5\n",
      "2017-06-30T10:08:38.806585: step 45, loss 0.815176, acc 0.421875\n",
      "2017-06-30T10:08:38.920210: step 46, loss 0.780728, acc 0.46875\n",
      "2017-06-30T10:08:39.039157: step 47, loss 0.780891, acc 0.46875\n",
      "2017-06-30T10:08:39.153925: step 48, loss 0.84271, acc 0.421875\n",
      "2017-06-30T10:08:39.268057: step 49, loss 0.727819, acc 0.5625\n",
      "2017-06-30T10:08:39.381819: step 50, loss 0.765671, acc 0.484375\n",
      "2017-06-30T10:08:39.498973: step 51, loss 0.733559, acc 0.546875\n",
      "2017-06-30T10:08:39.620363: step 52, loss 0.761033, acc 0.515625\n",
      "2017-06-30T10:08:39.739659: step 53, loss 0.681071, acc 0.546875\n",
      "2017-06-30T10:08:39.851952: step 54, loss 0.753916, acc 0.53125\n",
      "2017-06-30T10:08:39.973374: step 55, loss 0.690081, acc 0.609375\n",
      "2017-06-30T10:08:40.091543: step 56, loss 0.741589, acc 0.53125\n",
      "2017-06-30T10:08:40.207166: step 57, loss 0.782664, acc 0.5\n",
      "2017-06-30T10:08:40.322033: step 58, loss 0.753989, acc 0.5\n",
      "2017-06-30T10:08:40.438949: step 59, loss 0.737274, acc 0.5625\n",
      "2017-06-30T10:08:40.555896: step 60, loss 0.730776, acc 0.546875\n",
      "2017-06-30T10:08:40.674407: step 61, loss 0.793364, acc 0.5\n",
      "2017-06-30T10:08:40.786017: step 62, loss 0.857993, acc 0.40625\n",
      "2017-06-30T10:08:40.901890: step 63, loss 0.765843, acc 0.5\n",
      "2017-06-30T10:08:41.022255: step 64, loss 0.730985, acc 0.5\n",
      "2017-06-30T10:08:41.140139: step 65, loss 0.687491, acc 0.59375\n",
      "2017-06-30T10:08:41.253941: step 66, loss 0.718509, acc 0.578125\n",
      "2017-06-30T10:08:41.370250: step 67, loss 0.763788, acc 0.484375\n",
      "2017-06-30T10:08:41.490108: step 68, loss 0.786555, acc 0.46875\n",
      "2017-06-30T10:08:41.612338: step 69, loss 0.673845, acc 0.59375\n",
      "2017-06-30T10:08:41.732960: step 70, loss 0.677375, acc 0.640625\n",
      "2017-06-30T10:08:41.850715: step 71, loss 0.747979, acc 0.5\n",
      "2017-06-30T10:08:41.964295: step 72, loss 0.842163, acc 0.421875\n",
      "2017-06-30T10:08:42.081865: step 73, loss 0.684825, acc 0.59375\n",
      "2017-06-30T10:08:42.195332: step 74, loss 0.773084, acc 0.515625\n",
      "2017-06-30T10:08:42.312949: step 75, loss 0.789111, acc 0.484375\n",
      "2017-06-30T10:08:42.432528: step 76, loss 0.771333, acc 0.46875\n",
      "2017-06-30T10:08:42.549949: step 77, loss 0.745027, acc 0.515625\n",
      "2017-06-30T10:08:42.667211: step 78, loss 0.800757, acc 0.46875\n",
      "2017-06-30T10:08:42.780512: step 79, loss 0.749875, acc 0.546875\n",
      "2017-06-30T10:08:42.896831: step 80, loss 0.704297, acc 0.578125\n",
      "2017-06-30T10:08:43.009456: step 81, loss 0.770943, acc 0.5\n",
      "2017-06-30T10:08:43.125486: step 82, loss 0.756885, acc 0.53125\n",
      "2017-06-30T10:08:43.241010: step 83, loss 0.715712, acc 0.578125\n",
      "2017-06-30T10:08:43.356803: step 84, loss 0.723748, acc 0.578125\n",
      "2017-06-30T10:08:43.473716: step 85, loss 0.763415, acc 0.5\n",
      "2017-06-30T10:08:43.588629: step 86, loss 0.680176, acc 0.59375\n",
      "2017-06-30T10:08:43.707237: step 87, loss 0.837257, acc 0.421875\n",
      "2017-06-30T10:08:43.821840: step 88, loss 0.788365, acc 0.515625\n",
      "2017-06-30T10:08:43.938643: step 89, loss 0.707444, acc 0.5625\n",
      "2017-06-30T10:08:44.054048: step 90, loss 0.814184, acc 0.453125\n",
      "2017-06-30T10:08:44.171952: step 91, loss 0.743823, acc 0.53125\n",
      "2017-06-30T10:08:44.290641: step 92, loss 0.764119, acc 0.53125\n",
      "2017-06-30T10:08:44.407476: step 93, loss 0.72343, acc 0.578125\n",
      "2017-06-30T10:08:44.522617: step 94, loss 0.7213, acc 0.578125\n",
      "2017-06-30T10:08:44.639788: step 95, loss 0.846767, acc 0.421875\n",
      "2017-06-30T10:08:44.754850: step 96, loss 0.758751, acc 0.515625\n",
      "2017-06-30T10:08:44.868620: step 97, loss 0.86045, acc 0.40625\n",
      "2017-06-30T10:08:44.986282: step 98, loss 0.787121, acc 0.515625\n",
      "2017-06-30T10:08:45.101688: step 99, loss 0.836402, acc 0.421875\n",
      "2017-06-30T10:08:45.218453: step 100, loss 0.802644, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:08:45.522433: step 100, loss 0.793333, acc 0.490619\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-100\n",
      "\n",
      "2017-06-30T10:08:46.370740: step 101, loss 0.779967, acc 0.5\n",
      "2017-06-30T10:08:46.505872: step 102, loss 0.71231, acc 0.5625\n",
      "2017-06-30T10:08:46.647606: step 103, loss 0.691834, acc 0.59375\n",
      "2017-06-30T10:08:46.790234: step 104, loss 0.89298, acc 0.375\n",
      "2017-06-30T10:08:46.944941: step 105, loss 0.738091, acc 0.5625\n",
      "2017-06-30T10:08:47.081401: step 106, loss 0.686898, acc 0.59375\n",
      "2017-06-30T10:08:47.200784: step 107, loss 0.822011, acc 0.40625\n",
      "2017-06-30T10:08:47.323503: step 108, loss 0.790151, acc 0.453125\n",
      "2017-06-30T10:08:47.455130: step 109, loss 0.771975, acc 0.46875\n",
      "2017-06-30T10:08:47.587980: step 110, loss 0.805403, acc 0.5\n",
      "2017-06-30T10:08:47.717579: step 111, loss 0.740042, acc 0.53125\n",
      "2017-06-30T10:08:47.843615: step 112, loss 0.882297, acc 0.390625\n",
      "2017-06-30T10:08:47.970039: step 113, loss 0.744557, acc 0.5\n",
      "2017-06-30T10:08:48.093577: step 114, loss 0.748776, acc 0.546875\n",
      "2017-06-30T10:08:48.221880: step 115, loss 0.727118, acc 0.5625\n",
      "2017-06-30T10:08:48.342379: step 116, loss 0.705762, acc 0.59375\n",
      "2017-06-30T10:08:48.472086: step 117, loss 0.735195, acc 0.53125\n",
      "2017-06-30T10:08:48.614351: step 118, loss 0.746827, acc 0.5625\n",
      "2017-06-30T10:08:48.757945: step 119, loss 0.729006, acc 0.546875\n",
      "2017-06-30T10:08:48.904529: step 120, loss 0.72789, acc 0.53125\n",
      "2017-06-30T10:08:49.057050: step 121, loss 0.738305, acc 0.515625\n",
      "2017-06-30T10:08:49.202267: step 122, loss 0.768798, acc 0.515625\n",
      "2017-06-30T10:08:49.331745: step 123, loss 0.811584, acc 0.453125\n",
      "2017-06-30T10:08:49.467995: step 124, loss 0.687951, acc 0.625\n",
      "2017-06-30T10:08:49.604775: step 125, loss 0.753428, acc 0.578125\n",
      "2017-06-30T10:08:49.743594: step 126, loss 0.642883, acc 0.671875\n",
      "2017-06-30T10:08:49.871979: step 127, loss 0.720527, acc 0.5625\n",
      "2017-06-30T10:08:49.992647: step 128, loss 0.720342, acc 0.5625\n",
      "2017-06-30T10:08:50.122132: step 129, loss 0.746323, acc 0.53125\n",
      "2017-06-30T10:08:50.258194: step 130, loss 0.714291, acc 0.578125\n",
      "2017-06-30T10:08:50.387244: step 131, loss 0.796949, acc 0.484375\n",
      "2017-06-30T10:08:50.518953: step 132, loss 0.763168, acc 0.453125\n",
      "2017-06-30T10:08:50.638499: step 133, loss 0.748834, acc 0.5\n",
      "2017-06-30T10:08:50.761147: step 134, loss 0.793916, acc 0.5\n",
      "2017-06-30T10:08:50.887083: step 135, loss 0.719884, acc 0.546875\n",
      "2017-06-30T10:08:51.012002: step 136, loss 0.707649, acc 0.578125\n",
      "2017-06-30T10:08:51.142635: step 137, loss 0.734134, acc 0.5625\n",
      "2017-06-30T10:08:51.268759: step 138, loss 0.724861, acc 0.546875\n",
      "2017-06-30T10:08:51.394710: step 139, loss 0.713394, acc 0.5625\n",
      "2017-06-30T10:08:51.529388: step 140, loss 0.704042, acc 0.53125\n",
      "2017-06-30T10:08:51.651241: step 141, loss 0.782519, acc 0.484375\n",
      "2017-06-30T10:08:51.770398: step 142, loss 0.742991, acc 0.546875\n",
      "2017-06-30T10:08:51.899302: step 143, loss 0.725318, acc 0.546875\n",
      "2017-06-30T10:08:52.028014: step 144, loss 0.783303, acc 0.5\n",
      "2017-06-30T10:08:52.152616: step 145, loss 0.764807, acc 0.546875\n",
      "2017-06-30T10:08:52.277739: step 146, loss 0.756557, acc 0.515625\n",
      "2017-06-30T10:08:52.406768: step 147, loss 0.745157, acc 0.578125\n",
      "2017-06-30T10:08:52.534486: step 148, loss 0.756945, acc 0.5\n",
      "2017-06-30T10:08:52.656297: step 149, loss 0.713952, acc 0.578125\n",
      "2017-06-30T10:08:52.779509: step 150, loss 0.788846, acc 0.466667\n",
      "2017-06-30T10:08:52.907331: step 151, loss 0.678115, acc 0.59375\n",
      "2017-06-30T10:08:53.039505: step 152, loss 0.689503, acc 0.609375\n",
      "2017-06-30T10:08:53.168697: step 153, loss 0.691246, acc 0.5625\n",
      "2017-06-30T10:08:53.299011: step 154, loss 0.711007, acc 0.546875\n",
      "2017-06-30T10:08:53.434034: step 155, loss 0.762779, acc 0.484375\n",
      "2017-06-30T10:08:53.562779: step 156, loss 0.787057, acc 0.5\n",
      "2017-06-30T10:08:53.691040: step 157, loss 0.685607, acc 0.578125\n",
      "2017-06-30T10:08:53.818480: step 158, loss 0.733214, acc 0.546875\n",
      "2017-06-30T10:08:53.950086: step 159, loss 0.734547, acc 0.53125\n",
      "2017-06-30T10:08:54.077388: step 160, loss 0.752979, acc 0.515625\n",
      "2017-06-30T10:08:54.205535: step 161, loss 0.689907, acc 0.59375\n",
      "2017-06-30T10:08:54.335784: step 162, loss 0.63475, acc 0.640625\n",
      "2017-06-30T10:08:54.457455: step 163, loss 0.673573, acc 0.609375\n",
      "2017-06-30T10:08:54.575173: step 164, loss 0.751338, acc 0.5\n",
      "2017-06-30T10:08:54.695298: step 165, loss 0.69189, acc 0.5625\n",
      "2017-06-30T10:08:54.820202: step 166, loss 0.712784, acc 0.546875\n",
      "2017-06-30T10:08:54.944516: step 167, loss 0.779269, acc 0.484375\n",
      "2017-06-30T10:08:55.068514: step 168, loss 0.694603, acc 0.609375\n",
      "2017-06-30T10:08:55.197549: step 169, loss 0.698066, acc 0.5625\n",
      "2017-06-30T10:08:55.329352: step 170, loss 0.659961, acc 0.609375\n",
      "2017-06-30T10:08:55.467777: step 171, loss 0.692798, acc 0.59375\n",
      "2017-06-30T10:08:55.596558: step 172, loss 0.701466, acc 0.609375\n",
      "2017-06-30T10:08:55.737812: step 173, loss 0.770526, acc 0.515625\n",
      "2017-06-30T10:08:55.884695: step 174, loss 0.702799, acc 0.59375\n",
      "2017-06-30T10:08:56.015802: step 175, loss 0.72107, acc 0.5625\n",
      "2017-06-30T10:08:56.153061: step 176, loss 0.683254, acc 0.59375\n",
      "2017-06-30T10:08:56.279282: step 177, loss 0.745525, acc 0.515625\n",
      "2017-06-30T10:08:56.406762: step 178, loss 0.690317, acc 0.5625\n",
      "2017-06-30T10:08:56.554540: step 179, loss 0.716561, acc 0.578125\n",
      "2017-06-30T10:08:56.702398: step 180, loss 0.809802, acc 0.453125\n",
      "2017-06-30T10:08:56.847081: step 181, loss 0.709486, acc 0.59375\n",
      "2017-06-30T10:08:57.001565: step 182, loss 0.731826, acc 0.578125\n",
      "2017-06-30T10:08:57.132984: step 183, loss 0.706691, acc 0.578125\n",
      "2017-06-30T10:08:57.259169: step 184, loss 0.762806, acc 0.53125\n",
      "2017-06-30T10:08:57.386325: step 185, loss 0.753163, acc 0.546875\n",
      "2017-06-30T10:08:57.529305: step 186, loss 0.690837, acc 0.59375\n",
      "2017-06-30T10:08:57.686634: step 187, loss 0.673202, acc 0.625\n",
      "2017-06-30T10:08:57.847171: step 188, loss 0.743542, acc 0.53125\n",
      "2017-06-30T10:08:57.997273: step 189, loss 0.686588, acc 0.59375\n",
      "2017-06-30T10:08:58.159981: step 190, loss 0.801749, acc 0.46875\n",
      "2017-06-30T10:08:58.309397: step 191, loss 0.657302, acc 0.625\n",
      "2017-06-30T10:08:58.449940: step 192, loss 0.713146, acc 0.578125\n",
      "2017-06-30T10:08:58.587686: step 193, loss 0.682921, acc 0.609375\n",
      "2017-06-30T10:08:58.723599: step 194, loss 0.759959, acc 0.53125\n",
      "2017-06-30T10:08:58.864178: step 195, loss 0.694718, acc 0.578125\n",
      "2017-06-30T10:08:58.992074: step 196, loss 0.754797, acc 0.515625\n",
      "2017-06-30T10:08:59.135758: step 197, loss 0.650803, acc 0.640625\n",
      "2017-06-30T10:08:59.275311: step 198, loss 0.601961, acc 0.703125\n",
      "2017-06-30T10:08:59.406742: step 199, loss 0.742388, acc 0.546875\n",
      "2017-06-30T10:08:59.530166: step 200, loss 0.737581, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:08:59.855266: step 200, loss 0.706997, acc 0.572233\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-200\n",
      "\n",
      "2017-06-30T10:09:00.655848: step 201, loss 0.729849, acc 0.5625\n",
      "2017-06-30T10:09:00.786648: step 202, loss 0.687257, acc 0.625\n",
      "2017-06-30T10:09:00.919082: step 203, loss 0.718667, acc 0.59375\n",
      "2017-06-30T10:09:01.057506: step 204, loss 0.642392, acc 0.6875\n",
      "2017-06-30T10:09:01.195632: step 205, loss 0.727665, acc 0.5625\n",
      "2017-06-30T10:09:01.339488: step 206, loss 0.725048, acc 0.5625\n",
      "2017-06-30T10:09:01.472176: step 207, loss 0.770272, acc 0.515625\n",
      "2017-06-30T10:09:01.605782: step 208, loss 0.672752, acc 0.609375\n",
      "2017-06-30T10:09:01.734062: step 209, loss 0.717472, acc 0.546875\n",
      "2017-06-30T10:09:01.860791: step 210, loss 0.707487, acc 0.59375\n",
      "2017-06-30T10:09:02.013999: step 211, loss 0.772182, acc 0.484375\n",
      "2017-06-30T10:09:02.167268: step 212, loss 0.689033, acc 0.59375\n",
      "2017-06-30T10:09:02.317753: step 213, loss 0.711201, acc 0.5625\n",
      "2017-06-30T10:09:02.454413: step 214, loss 0.63746, acc 0.65625\n",
      "2017-06-30T10:09:02.590407: step 215, loss 0.666849, acc 0.65625\n",
      "2017-06-30T10:09:02.723619: step 216, loss 0.6739, acc 0.609375\n",
      "2017-06-30T10:09:02.848528: step 217, loss 0.743682, acc 0.546875\n",
      "2017-06-30T10:09:02.975512: step 218, loss 0.742248, acc 0.53125\n",
      "2017-06-30T10:09:03.107667: step 219, loss 0.71592, acc 0.5625\n",
      "2017-06-30T10:09:03.237145: step 220, loss 0.673504, acc 0.625\n",
      "2017-06-30T10:09:03.390421: step 221, loss 0.694902, acc 0.609375\n",
      "2017-06-30T10:09:03.538314: step 222, loss 0.64613, acc 0.640625\n",
      "2017-06-30T10:09:03.689999: step 223, loss 0.756064, acc 0.5625\n",
      "2017-06-30T10:09:03.830095: step 224, loss 0.74019, acc 0.53125\n",
      "2017-06-30T10:09:03.959612: step 225, loss 0.720288, acc 0.578125\n",
      "2017-06-30T10:09:04.085042: step 226, loss 0.624244, acc 0.640625\n",
      "2017-06-30T10:09:04.216929: step 227, loss 0.609252, acc 0.703125\n",
      "2017-06-30T10:09:04.363135: step 228, loss 0.676044, acc 0.609375\n",
      "2017-06-30T10:09:04.497275: step 229, loss 0.699472, acc 0.578125\n",
      "2017-06-30T10:09:04.638292: step 230, loss 0.709632, acc 0.578125\n",
      "2017-06-30T10:09:04.782507: step 231, loss 0.709339, acc 0.546875\n",
      "2017-06-30T10:09:04.917213: step 232, loss 0.66936, acc 0.59375\n",
      "2017-06-30T10:09:05.052221: step 233, loss 0.683068, acc 0.609375\n",
      "2017-06-30T10:09:05.200007: step 234, loss 0.6643, acc 0.640625\n",
      "2017-06-30T10:09:05.340669: step 235, loss 0.689005, acc 0.5625\n",
      "2017-06-30T10:09:05.470617: step 236, loss 0.695954, acc 0.609375\n",
      "2017-06-30T10:09:05.622925: step 237, loss 0.672506, acc 0.625\n",
      "2017-06-30T10:09:05.774324: step 238, loss 0.673719, acc 0.59375\n",
      "2017-06-30T10:09:05.927573: step 239, loss 0.678616, acc 0.59375\n",
      "2017-06-30T10:09:06.062689: step 240, loss 0.718035, acc 0.53125\n",
      "2017-06-30T10:09:06.189560: step 241, loss 0.723965, acc 0.546875\n",
      "2017-06-30T10:09:06.320155: step 242, loss 0.682358, acc 0.609375\n",
      "2017-06-30T10:09:06.464012: step 243, loss 0.688987, acc 0.625\n",
      "2017-06-30T10:09:06.613109: step 244, loss 0.775281, acc 0.515625\n",
      "2017-06-30T10:09:06.752643: step 245, loss 0.673404, acc 0.625\n",
      "2017-06-30T10:09:06.884032: step 246, loss 0.669406, acc 0.65625\n",
      "2017-06-30T10:09:07.026330: step 247, loss 0.655918, acc 0.640625\n",
      "2017-06-30T10:09:07.164191: step 248, loss 0.687058, acc 0.578125\n",
      "2017-06-30T10:09:07.296882: step 249, loss 0.696987, acc 0.625\n",
      "2017-06-30T10:09:07.439424: step 250, loss 0.660342, acc 0.640625\n",
      "2017-06-30T10:09:07.579436: step 251, loss 0.719425, acc 0.578125\n",
      "2017-06-30T10:09:07.721628: step 252, loss 0.648844, acc 0.625\n",
      "2017-06-30T10:09:07.874546: step 253, loss 0.68291, acc 0.640625\n",
      "2017-06-30T10:09:08.017471: step 254, loss 0.805044, acc 0.453125\n",
      "2017-06-30T10:09:08.163169: step 255, loss 0.680632, acc 0.609375\n",
      "2017-06-30T10:09:08.305858: step 256, loss 0.616545, acc 0.671875\n",
      "2017-06-30T10:09:08.446975: step 257, loss 0.758425, acc 0.5625\n",
      "2017-06-30T10:09:08.602853: step 258, loss 0.587592, acc 0.75\n",
      "2017-06-30T10:09:08.742814: step 259, loss 0.652037, acc 0.609375\n",
      "2017-06-30T10:09:08.892923: step 260, loss 0.641056, acc 0.671875\n",
      "2017-06-30T10:09:09.040555: step 261, loss 0.734538, acc 0.546875\n",
      "2017-06-30T10:09:09.181895: step 262, loss 0.758352, acc 0.53125\n",
      "2017-06-30T10:09:09.315170: step 263, loss 0.726539, acc 0.53125\n",
      "2017-06-30T10:09:09.452333: step 264, loss 0.674181, acc 0.578125\n",
      "2017-06-30T10:09:09.597260: step 265, loss 0.712448, acc 0.609375\n",
      "2017-06-30T10:09:09.740099: step 266, loss 0.696923, acc 0.59375\n",
      "2017-06-30T10:09:09.876359: step 267, loss 0.642661, acc 0.640625\n",
      "2017-06-30T10:09:10.018159: step 268, loss 0.722868, acc 0.546875\n",
      "2017-06-30T10:09:10.165540: step 269, loss 0.650334, acc 0.65625\n",
      "2017-06-30T10:09:10.304209: step 270, loss 0.670141, acc 0.59375\n",
      "2017-06-30T10:09:10.430372: step 271, loss 0.602465, acc 0.671875\n",
      "2017-06-30T10:09:10.560603: step 272, loss 0.745655, acc 0.53125\n",
      "2017-06-30T10:09:10.691171: step 273, loss 0.691082, acc 0.609375\n",
      "2017-06-30T10:09:10.822297: step 274, loss 0.66099, acc 0.625\n",
      "2017-06-30T10:09:10.963982: step 275, loss 0.671612, acc 0.640625\n",
      "2017-06-30T10:09:11.121516: step 276, loss 0.73225, acc 0.5625\n",
      "2017-06-30T10:09:11.262008: step 277, loss 0.646571, acc 0.671875\n",
      "2017-06-30T10:09:11.403418: step 278, loss 0.659301, acc 0.625\n",
      "2017-06-30T10:09:11.540053: step 279, loss 0.565086, acc 0.765625\n",
      "2017-06-30T10:09:11.671353: step 280, loss 0.615732, acc 0.703125\n",
      "2017-06-30T10:09:11.799727: step 281, loss 0.605367, acc 0.671875\n",
      "2017-06-30T10:09:11.925235: step 282, loss 0.62106, acc 0.6875\n",
      "2017-06-30T10:09:12.055763: step 283, loss 0.773297, acc 0.484375\n",
      "2017-06-30T10:09:12.206541: step 284, loss 0.652813, acc 0.640625\n",
      "2017-06-30T10:09:12.355111: step 285, loss 0.634249, acc 0.671875\n",
      "2017-06-30T10:09:12.493919: step 286, loss 0.700352, acc 0.578125\n",
      "2017-06-30T10:09:12.638877: step 287, loss 0.717999, acc 0.515625\n",
      "2017-06-30T10:09:12.791533: step 288, loss 0.679441, acc 0.609375\n",
      "2017-06-30T10:09:12.936644: step 289, loss 0.72457, acc 0.578125\n",
      "2017-06-30T10:09:13.083395: step 290, loss 0.659151, acc 0.640625\n",
      "2017-06-30T10:09:13.230961: step 291, loss 0.716714, acc 0.578125\n",
      "2017-06-30T10:09:13.377672: step 292, loss 0.688259, acc 0.609375\n",
      "2017-06-30T10:09:13.521713: step 293, loss 0.660565, acc 0.625\n",
      "2017-06-30T10:09:13.668446: step 294, loss 0.670764, acc 0.625\n",
      "2017-06-30T10:09:13.817434: step 295, loss 0.536853, acc 0.78125\n",
      "2017-06-30T10:09:13.959566: step 296, loss 0.732847, acc 0.5625\n",
      "2017-06-30T10:09:14.097935: step 297, loss 0.684143, acc 0.609375\n",
      "2017-06-30T10:09:14.231886: step 298, loss 0.733811, acc 0.53125\n",
      "2017-06-30T10:09:14.390296: step 299, loss 0.724985, acc 0.59375\n",
      "2017-06-30T10:09:14.548642: step 300, loss 0.696314, acc 0.616667\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:09:14.945132: step 300, loss 0.679733, acc 0.597561\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-300\n",
      "\n",
      "2017-06-30T10:09:15.754301: step 301, loss 0.703167, acc 0.625\n",
      "2017-06-30T10:09:15.886298: step 302, loss 0.582216, acc 0.703125\n",
      "2017-06-30T10:09:16.017914: step 303, loss 0.689226, acc 0.609375\n",
      "2017-06-30T10:09:16.152284: step 304, loss 0.636924, acc 0.65625\n",
      "2017-06-30T10:09:16.281999: step 305, loss 0.709011, acc 0.5625\n",
      "2017-06-30T10:09:16.410820: step 306, loss 0.620685, acc 0.703125\n",
      "2017-06-30T10:09:16.546989: step 307, loss 0.642799, acc 0.65625\n",
      "2017-06-30T10:09:16.689414: step 308, loss 0.711382, acc 0.53125\n",
      "2017-06-30T10:09:16.844650: step 309, loss 0.619347, acc 0.65625\n",
      "2017-06-30T10:09:16.984412: step 310, loss 0.695345, acc 0.609375\n",
      "2017-06-30T10:09:17.126081: step 311, loss 0.599531, acc 0.6875\n",
      "2017-06-30T10:09:17.264895: step 312, loss 0.692454, acc 0.578125\n",
      "2017-06-30T10:09:17.421077: step 313, loss 0.583838, acc 0.734375\n",
      "2017-06-30T10:09:17.574702: step 314, loss 0.621161, acc 0.703125\n",
      "2017-06-30T10:09:17.715115: step 315, loss 0.624611, acc 0.65625\n",
      "2017-06-30T10:09:17.849520: step 316, loss 0.609571, acc 0.6875\n",
      "2017-06-30T10:09:17.983781: step 317, loss 0.627391, acc 0.640625\n",
      "2017-06-30T10:09:18.150025: step 318, loss 0.662083, acc 0.609375\n",
      "2017-06-30T10:09:18.317689: step 319, loss 0.602184, acc 0.671875\n",
      "2017-06-30T10:09:18.570628: step 320, loss 0.579449, acc 0.71875\n",
      "2017-06-30T10:09:18.705501: step 321, loss 0.64515, acc 0.65625\n",
      "2017-06-30T10:09:18.841319: step 322, loss 0.628381, acc 0.640625\n",
      "2017-06-30T10:09:18.971909: step 323, loss 0.679049, acc 0.625\n",
      "2017-06-30T10:09:19.132715: step 324, loss 0.669837, acc 0.59375\n",
      "2017-06-30T10:09:19.304522: step 325, loss 0.615088, acc 0.671875\n",
      "2017-06-30T10:09:19.446881: step 326, loss 0.669592, acc 0.59375\n",
      "2017-06-30T10:09:19.588107: step 327, loss 0.608666, acc 0.6875\n",
      "2017-06-30T10:09:19.756879: step 328, loss 0.716272, acc 0.59375\n",
      "2017-06-30T10:09:19.900874: step 329, loss 0.700771, acc 0.5625\n",
      "2017-06-30T10:09:20.037808: step 330, loss 0.648273, acc 0.640625\n",
      "2017-06-30T10:09:20.194407: step 331, loss 0.620776, acc 0.640625\n",
      "2017-06-30T10:09:20.360827: step 332, loss 0.677322, acc 0.59375\n",
      "2017-06-30T10:09:20.498833: step 333, loss 0.725494, acc 0.578125\n",
      "2017-06-30T10:09:20.632123: step 334, loss 0.674669, acc 0.625\n",
      "2017-06-30T10:09:20.795667: step 335, loss 0.663119, acc 0.65625\n",
      "2017-06-30T10:09:20.964438: step 336, loss 0.637945, acc 0.640625\n",
      "2017-06-30T10:09:21.135918: step 337, loss 0.676495, acc 0.609375\n",
      "2017-06-30T10:09:21.279288: step 338, loss 0.674463, acc 0.609375\n",
      "2017-06-30T10:09:21.426456: step 339, loss 0.624855, acc 0.671875\n",
      "2017-06-30T10:09:21.601951: step 340, loss 0.68152, acc 0.625\n",
      "2017-06-30T10:09:21.835253: step 341, loss 0.639196, acc 0.640625\n",
      "2017-06-30T10:09:22.006854: step 342, loss 0.619799, acc 0.671875\n",
      "2017-06-30T10:09:22.156970: step 343, loss 0.742866, acc 0.5\n",
      "2017-06-30T10:09:22.294329: step 344, loss 0.645502, acc 0.671875\n",
      "2017-06-30T10:09:22.431651: step 345, loss 0.599032, acc 0.71875\n",
      "2017-06-30T10:09:22.578529: step 346, loss 0.581041, acc 0.75\n",
      "2017-06-30T10:09:22.748919: step 347, loss 0.620622, acc 0.65625\n",
      "2017-06-30T10:09:22.887389: step 348, loss 0.71202, acc 0.546875\n",
      "2017-06-30T10:09:23.019217: step 349, loss 0.565821, acc 0.734375\n",
      "2017-06-30T10:09:23.159839: step 350, loss 0.523621, acc 0.796875\n",
      "2017-06-30T10:09:23.300924: step 351, loss 0.559585, acc 0.75\n",
      "2017-06-30T10:09:23.443760: step 352, loss 0.700709, acc 0.578125\n",
      "2017-06-30T10:09:23.592851: step 353, loss 0.623832, acc 0.6875\n",
      "2017-06-30T10:09:23.757626: step 354, loss 0.659638, acc 0.640625\n",
      "2017-06-30T10:09:23.917585: step 355, loss 0.569236, acc 0.734375\n",
      "2017-06-30T10:09:24.075567: step 356, loss 0.646833, acc 0.65625\n",
      "2017-06-30T10:09:24.228354: step 357, loss 0.655452, acc 0.609375\n",
      "2017-06-30T10:09:24.375514: step 358, loss 0.62188, acc 0.625\n",
      "2017-06-30T10:09:24.520936: step 359, loss 0.695971, acc 0.625\n",
      "2017-06-30T10:09:24.657309: step 360, loss 0.670745, acc 0.609375\n",
      "2017-06-30T10:09:24.825046: step 361, loss 0.615798, acc 0.640625\n",
      "2017-06-30T10:09:24.983119: step 362, loss 0.683785, acc 0.578125\n",
      "2017-06-30T10:09:25.143672: step 363, loss 0.733384, acc 0.53125\n",
      "2017-06-30T10:09:25.317511: step 364, loss 0.727914, acc 0.5625\n",
      "2017-06-30T10:09:25.484338: step 365, loss 0.683217, acc 0.59375\n",
      "2017-06-30T10:09:25.646603: step 366, loss 0.598292, acc 0.703125\n",
      "2017-06-30T10:09:25.814112: step 367, loss 0.620304, acc 0.6875\n",
      "2017-06-30T10:09:25.975409: step 368, loss 0.6661, acc 0.609375\n",
      "2017-06-30T10:09:26.145224: step 369, loss 0.657049, acc 0.640625\n",
      "2017-06-30T10:09:26.297517: step 370, loss 0.627371, acc 0.65625\n",
      "2017-06-30T10:09:26.459461: step 371, loss 0.607441, acc 0.703125\n",
      "2017-06-30T10:09:26.627291: step 372, loss 0.669293, acc 0.609375\n",
      "2017-06-30T10:09:26.793223: step 373, loss 0.582927, acc 0.734375\n",
      "2017-06-30T10:09:26.945587: step 374, loss 0.658474, acc 0.625\n",
      "2017-06-30T10:09:27.095990: step 375, loss 0.619962, acc 0.6875\n",
      "2017-06-30T10:09:27.265670: step 376, loss 0.652582, acc 0.640625\n",
      "2017-06-30T10:09:27.427406: step 377, loss 0.72674, acc 0.546875\n",
      "2017-06-30T10:09:27.589050: step 378, loss 0.664258, acc 0.625\n",
      "2017-06-30T10:09:27.757981: step 379, loss 0.662536, acc 0.59375\n",
      "2017-06-30T10:09:27.897685: step 380, loss 0.594793, acc 0.703125\n",
      "2017-06-30T10:09:28.040020: step 381, loss 0.676631, acc 0.609375\n",
      "2017-06-30T10:09:28.177459: step 382, loss 0.673504, acc 0.640625\n",
      "2017-06-30T10:09:28.340673: step 383, loss 0.633813, acc 0.65625\n",
      "2017-06-30T10:09:28.513902: step 384, loss 0.783857, acc 0.46875\n",
      "2017-06-30T10:09:28.658737: step 385, loss 0.616162, acc 0.6875\n",
      "2017-06-30T10:09:28.812657: step 386, loss 0.663578, acc 0.625\n",
      "2017-06-30T10:09:28.971448: step 387, loss 0.613815, acc 0.671875\n",
      "2017-06-30T10:09:29.111500: step 388, loss 0.66262, acc 0.609375\n",
      "2017-06-30T10:09:29.266155: step 389, loss 0.689853, acc 0.59375\n",
      "2017-06-30T10:09:29.434091: step 390, loss 0.619157, acc 0.671875\n",
      "2017-06-30T10:09:29.601502: step 391, loss 0.73956, acc 0.546875\n",
      "2017-06-30T10:09:29.756824: step 392, loss 0.679932, acc 0.59375\n",
      "2017-06-30T10:09:29.916820: step 393, loss 0.680097, acc 0.640625\n",
      "2017-06-30T10:09:30.082816: step 394, loss 0.61836, acc 0.65625\n",
      "2017-06-30T10:09:30.226865: step 395, loss 0.640809, acc 0.65625\n",
      "2017-06-30T10:09:30.397885: step 396, loss 0.665635, acc 0.609375\n",
      "2017-06-30T10:09:30.549698: step 397, loss 0.632076, acc 0.671875\n",
      "2017-06-30T10:09:30.686584: step 398, loss 0.603919, acc 0.703125\n",
      "2017-06-30T10:09:30.827011: step 399, loss 0.616541, acc 0.65625\n",
      "2017-06-30T10:09:30.990219: step 400, loss 0.711643, acc 0.578125\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:09:31.387792: step 400, loss 0.661234, acc 0.615385\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-400\n",
      "\n",
      "2017-06-30T10:09:32.303360: step 401, loss 0.641867, acc 0.609375\n",
      "2017-06-30T10:09:32.459413: step 402, loss 0.636747, acc 0.671875\n",
      "2017-06-30T10:09:32.602804: step 403, loss 0.631815, acc 0.65625\n",
      "2017-06-30T10:09:32.746347: step 404, loss 0.642222, acc 0.671875\n",
      "2017-06-30T10:09:32.894343: step 405, loss 0.679133, acc 0.625\n",
      "2017-06-30T10:09:33.046427: step 406, loss 0.557389, acc 0.75\n",
      "2017-06-30T10:09:33.191985: step 407, loss 0.73138, acc 0.5625\n",
      "2017-06-30T10:09:33.336041: step 408, loss 0.672306, acc 0.625\n",
      "2017-06-30T10:09:33.501241: step 409, loss 0.671026, acc 0.640625\n",
      "2017-06-30T10:09:33.654875: step 410, loss 0.668361, acc 0.625\n",
      "2017-06-30T10:09:33.883156: step 411, loss 0.650047, acc 0.640625\n",
      "2017-06-30T10:09:34.038681: step 412, loss 0.674815, acc 0.625\n",
      "2017-06-30T10:09:34.204811: step 413, loss 0.713004, acc 0.59375\n",
      "2017-06-30T10:09:34.363385: step 414, loss 0.616338, acc 0.703125\n",
      "2017-06-30T10:09:34.532296: step 415, loss 0.617958, acc 0.6875\n",
      "2017-06-30T10:09:34.697413: step 416, loss 0.751323, acc 0.5625\n",
      "2017-06-30T10:09:34.862280: step 417, loss 0.683028, acc 0.609375\n",
      "2017-06-30T10:09:35.017982: step 418, loss 0.590952, acc 0.71875\n",
      "2017-06-30T10:09:35.177665: step 419, loss 0.636539, acc 0.65625\n",
      "2017-06-30T10:09:35.343762: step 420, loss 0.74614, acc 0.5625\n",
      "2017-06-30T10:09:35.515055: step 421, loss 0.604154, acc 0.71875\n",
      "2017-06-30T10:09:35.685556: step 422, loss 0.679707, acc 0.578125\n",
      "2017-06-30T10:09:35.828006: step 423, loss 0.603062, acc 0.6875\n",
      "2017-06-30T10:09:35.961695: step 424, loss 0.68133, acc 0.625\n",
      "2017-06-30T10:09:36.105325: step 425, loss 0.651966, acc 0.609375\n",
      "2017-06-30T10:09:36.249449: step 426, loss 0.658095, acc 0.625\n",
      "2017-06-30T10:09:36.425687: step 427, loss 0.723243, acc 0.546875\n",
      "2017-06-30T10:09:36.659077: step 428, loss 0.59918, acc 0.734375\n",
      "2017-06-30T10:09:36.792281: step 429, loss 0.569098, acc 0.734375\n",
      "2017-06-30T10:09:36.932213: step 430, loss 0.618943, acc 0.671875\n",
      "2017-06-30T10:09:37.098237: step 431, loss 0.625847, acc 0.671875\n",
      "2017-06-30T10:09:37.263514: step 432, loss 0.657591, acc 0.640625\n",
      "2017-06-30T10:09:37.413936: step 433, loss 0.674292, acc 0.609375\n",
      "2017-06-30T10:09:37.554501: step 434, loss 0.718951, acc 0.578125\n",
      "2017-06-30T10:09:37.696797: step 435, loss 0.618428, acc 0.6875\n",
      "2017-06-30T10:09:37.836626: step 436, loss 0.592499, acc 0.703125\n",
      "2017-06-30T10:09:37.985562: step 437, loss 0.643512, acc 0.65625\n",
      "2017-06-30T10:09:38.141352: step 438, loss 0.600358, acc 0.6875\n",
      "2017-06-30T10:09:38.278178: step 439, loss 0.657095, acc 0.640625\n",
      "2017-06-30T10:09:38.417932: step 440, loss 0.651787, acc 0.65625\n",
      "2017-06-30T10:09:38.555803: step 441, loss 0.657365, acc 0.625\n",
      "2017-06-30T10:09:38.691375: step 442, loss 0.616836, acc 0.65625\n",
      "2017-06-30T10:09:38.828660: step 443, loss 0.736582, acc 0.546875\n",
      "2017-06-30T10:09:38.974978: step 444, loss 0.569012, acc 0.734375\n",
      "2017-06-30T10:09:39.118499: step 445, loss 0.647484, acc 0.640625\n",
      "2017-06-30T10:09:39.261353: step 446, loss 0.551728, acc 0.765625\n",
      "2017-06-30T10:09:39.395168: step 447, loss 0.61257, acc 0.65625\n",
      "2017-06-30T10:09:39.530179: step 448, loss 0.681961, acc 0.546875\n",
      "2017-06-30T10:09:39.673793: step 449, loss 0.728482, acc 0.53125\n",
      "2017-06-30T10:09:39.806577: step 450, loss 0.633239, acc 0.666667\n",
      "2017-06-30T10:09:39.944717: step 451, loss 0.681466, acc 0.609375\n",
      "2017-06-30T10:09:40.098694: step 452, loss 0.723548, acc 0.5625\n",
      "2017-06-30T10:09:40.239330: step 453, loss 0.618713, acc 0.640625\n",
      "2017-06-30T10:09:40.378323: step 454, loss 0.634324, acc 0.65625\n",
      "2017-06-30T10:09:40.527320: step 455, loss 0.597275, acc 0.71875\n",
      "2017-06-30T10:09:40.664760: step 456, loss 0.650132, acc 0.625\n",
      "2017-06-30T10:09:40.805838: step 457, loss 0.639011, acc 0.671875\n",
      "2017-06-30T10:09:40.942535: step 458, loss 0.584823, acc 0.734375\n",
      "2017-06-30T10:09:41.088445: step 459, loss 0.652617, acc 0.65625\n",
      "2017-06-30T10:09:41.231600: step 460, loss 0.65415, acc 0.65625\n",
      "2017-06-30T10:09:41.366426: step 461, loss 0.671138, acc 0.625\n",
      "2017-06-30T10:09:41.503346: step 462, loss 0.581957, acc 0.6875\n",
      "2017-06-30T10:09:41.640950: step 463, loss 0.544455, acc 0.75\n",
      "2017-06-30T10:09:41.783405: step 464, loss 0.532427, acc 0.796875\n",
      "2017-06-30T10:09:41.923402: step 465, loss 0.646471, acc 0.640625\n",
      "2017-06-30T10:09:42.067634: step 466, loss 0.599727, acc 0.703125\n",
      "2017-06-30T10:09:42.211641: step 467, loss 0.575446, acc 0.71875\n",
      "2017-06-30T10:09:42.353692: step 468, loss 0.601343, acc 0.71875\n",
      "2017-06-30T10:09:42.487437: step 469, loss 0.624661, acc 0.6875\n",
      "2017-06-30T10:09:42.621735: step 470, loss 0.556492, acc 0.765625\n",
      "2017-06-30T10:09:42.757530: step 471, loss 0.530006, acc 0.78125\n",
      "2017-06-30T10:09:42.890095: step 472, loss 0.560555, acc 0.703125\n",
      "2017-06-30T10:09:43.023342: step 473, loss 0.600815, acc 0.75\n",
      "2017-06-30T10:09:43.145387: step 474, loss 0.518491, acc 0.78125\n",
      "2017-06-30T10:09:43.269473: step 475, loss 0.638075, acc 0.6875\n",
      "2017-06-30T10:09:43.398265: step 476, loss 0.646498, acc 0.65625\n",
      "2017-06-30T10:09:43.532227: step 477, loss 0.579461, acc 0.71875\n",
      "2017-06-30T10:09:43.660858: step 478, loss 0.565325, acc 0.71875\n",
      "2017-06-30T10:09:43.798511: step 479, loss 0.562076, acc 0.71875\n",
      "2017-06-30T10:09:43.926769: step 480, loss 0.630278, acc 0.640625\n",
      "2017-06-30T10:09:44.058965: step 481, loss 0.609272, acc 0.6875\n",
      "2017-06-30T10:09:44.192693: step 482, loss 0.611352, acc 0.6875\n",
      "2017-06-30T10:09:44.325900: step 483, loss 0.60246, acc 0.703125\n",
      "2017-06-30T10:09:44.457541: step 484, loss 0.612124, acc 0.640625\n",
      "2017-06-30T10:09:44.594327: step 485, loss 0.637255, acc 0.65625\n",
      "2017-06-30T10:09:44.731061: step 486, loss 0.625475, acc 0.6875\n",
      "2017-06-30T10:09:44.866900: step 487, loss 0.651528, acc 0.625\n",
      "2017-06-30T10:09:45.007357: step 488, loss 0.549423, acc 0.765625\n",
      "2017-06-30T10:09:45.143389: step 489, loss 0.578491, acc 0.703125\n",
      "2017-06-30T10:09:45.283990: step 490, loss 0.569975, acc 0.71875\n",
      "2017-06-30T10:09:45.419664: step 491, loss 0.500049, acc 0.8125\n",
      "2017-06-30T10:09:45.562210: step 492, loss 0.686228, acc 0.59375\n",
      "2017-06-30T10:09:45.701098: step 493, loss 0.5225, acc 0.796875\n",
      "2017-06-30T10:09:45.856395: step 494, loss 0.553047, acc 0.78125\n",
      "2017-06-30T10:09:46.016626: step 495, loss 0.618873, acc 0.671875\n",
      "2017-06-30T10:09:46.172699: step 496, loss 0.629896, acc 0.640625\n",
      "2017-06-30T10:09:46.328839: step 497, loss 0.537168, acc 0.78125\n",
      "2017-06-30T10:09:46.493660: step 498, loss 0.591497, acc 0.6875\n",
      "2017-06-30T10:09:46.655863: step 499, loss 0.644531, acc 0.625\n",
      "2017-06-30T10:09:46.809465: step 500, loss 0.588935, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:09:47.202144: step 500, loss 0.645238, acc 0.634146\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-500\n",
      "\n",
      "2017-06-30T10:09:47.935679: step 501, loss 0.674546, acc 0.609375\n",
      "2017-06-30T10:09:48.094357: step 502, loss 0.677981, acc 0.609375\n",
      "2017-06-30T10:09:48.260518: step 503, loss 0.560774, acc 0.71875\n",
      "2017-06-30T10:09:48.425577: step 504, loss 0.614508, acc 0.640625\n",
      "2017-06-30T10:09:48.559818: step 505, loss 0.580657, acc 0.703125\n",
      "2017-06-30T10:09:48.690096: step 506, loss 0.584222, acc 0.703125\n",
      "2017-06-30T10:09:48.830083: step 507, loss 0.542399, acc 0.78125\n",
      "2017-06-30T10:09:48.961740: step 508, loss 0.649362, acc 0.625\n",
      "2017-06-30T10:09:49.108834: step 509, loss 0.611819, acc 0.65625\n",
      "2017-06-30T10:09:49.254299: step 510, loss 0.532748, acc 0.765625\n",
      "2017-06-30T10:09:49.395857: step 511, loss 0.534302, acc 0.765625\n",
      "2017-06-30T10:09:49.537761: step 512, loss 0.596059, acc 0.71875\n",
      "2017-06-30T10:09:49.676829: step 513, loss 0.605668, acc 0.6875\n",
      "2017-06-30T10:09:49.805616: step 514, loss 0.553678, acc 0.75\n",
      "2017-06-30T10:09:49.956508: step 515, loss 0.579361, acc 0.703125\n",
      "2017-06-30T10:09:50.099512: step 516, loss 0.654398, acc 0.640625\n",
      "2017-06-30T10:09:50.237136: step 517, loss 0.60886, acc 0.6875\n",
      "2017-06-30T10:09:50.372454: step 518, loss 0.59841, acc 0.703125\n",
      "2017-06-30T10:09:50.507774: step 519, loss 0.531092, acc 0.78125\n",
      "2017-06-30T10:09:50.646914: step 520, loss 0.652329, acc 0.640625\n",
      "2017-06-30T10:09:50.779175: step 521, loss 0.546167, acc 0.765625\n",
      "2017-06-30T10:09:50.919514: step 522, loss 0.646164, acc 0.625\n",
      "2017-06-30T10:09:51.061698: step 523, loss 0.572652, acc 0.734375\n",
      "2017-06-30T10:09:51.196023: step 524, loss 0.545414, acc 0.75\n",
      "2017-06-30T10:09:51.346875: step 525, loss 0.574106, acc 0.734375\n",
      "2017-06-30T10:09:51.492212: step 526, loss 0.524791, acc 0.796875\n",
      "2017-06-30T10:09:51.629163: step 527, loss 0.654039, acc 0.625\n",
      "2017-06-30T10:09:51.760332: step 528, loss 0.596318, acc 0.703125\n",
      "2017-06-30T10:09:51.894085: step 529, loss 0.584169, acc 0.703125\n",
      "2017-06-30T10:09:52.031887: step 530, loss 0.633513, acc 0.671875\n",
      "2017-06-30T10:09:52.164742: step 531, loss 0.606272, acc 0.6875\n",
      "2017-06-30T10:09:52.299895: step 532, loss 0.589023, acc 0.734375\n",
      "2017-06-30T10:09:52.436784: step 533, loss 0.544634, acc 0.765625\n",
      "2017-06-30T10:09:52.575335: step 534, loss 0.464975, acc 0.84375\n",
      "2017-06-30T10:09:52.706073: step 535, loss 0.635619, acc 0.625\n",
      "2017-06-30T10:09:52.838282: step 536, loss 0.62577, acc 0.6875\n",
      "2017-06-30T10:09:52.974076: step 537, loss 0.558655, acc 0.75\n",
      "2017-06-30T10:09:53.112707: step 538, loss 0.544556, acc 0.75\n",
      "2017-06-30T10:09:53.244797: step 539, loss 0.724555, acc 0.515625\n",
      "2017-06-30T10:09:53.406182: step 540, loss 0.674582, acc 0.59375\n",
      "2017-06-30T10:09:53.544585: step 541, loss 0.608223, acc 0.671875\n",
      "2017-06-30T10:09:53.683180: step 542, loss 0.553308, acc 0.75\n",
      "2017-06-30T10:09:53.850919: step 543, loss 0.563496, acc 0.75\n",
      "2017-06-30T10:09:54.003577: step 544, loss 0.547623, acc 0.75\n",
      "2017-06-30T10:09:54.137067: step 545, loss 0.618837, acc 0.6875\n",
      "2017-06-30T10:09:54.275216: step 546, loss 0.661571, acc 0.65625\n",
      "2017-06-30T10:09:54.412827: step 547, loss 0.553423, acc 0.75\n",
      "2017-06-30T10:09:54.555249: step 548, loss 0.620783, acc 0.71875\n",
      "2017-06-30T10:09:54.689704: step 549, loss 0.632458, acc 0.6875\n",
      "2017-06-30T10:09:54.819280: step 550, loss 0.643402, acc 0.6875\n",
      "2017-06-30T10:09:54.944131: step 551, loss 0.641662, acc 0.625\n",
      "2017-06-30T10:09:55.075758: step 552, loss 0.668427, acc 0.625\n",
      "2017-06-30T10:09:55.212094: step 553, loss 0.632444, acc 0.640625\n",
      "2017-06-30T10:09:55.353362: step 554, loss 0.587068, acc 0.6875\n",
      "2017-06-30T10:09:55.478922: step 555, loss 0.631121, acc 0.671875\n",
      "2017-06-30T10:09:55.608381: step 556, loss 0.592258, acc 0.734375\n",
      "2017-06-30T10:09:55.737460: step 557, loss 0.577534, acc 0.734375\n",
      "2017-06-30T10:09:55.873375: step 558, loss 0.560723, acc 0.734375\n",
      "2017-06-30T10:09:56.011653: step 559, loss 0.582952, acc 0.703125\n",
      "2017-06-30T10:09:56.140591: step 560, loss 0.583361, acc 0.703125\n",
      "2017-06-30T10:09:56.278005: step 561, loss 0.567799, acc 0.765625\n",
      "2017-06-30T10:09:56.417758: step 562, loss 0.551687, acc 0.734375\n",
      "2017-06-30T10:09:56.551967: step 563, loss 0.626957, acc 0.6875\n",
      "2017-06-30T10:09:56.683581: step 564, loss 0.644551, acc 0.65625\n",
      "2017-06-30T10:09:56.818215: step 565, loss 0.566351, acc 0.75\n",
      "2017-06-30T10:09:56.951096: step 566, loss 0.601474, acc 0.703125\n",
      "2017-06-30T10:09:57.079138: step 567, loss 0.58679, acc 0.734375\n",
      "2017-06-30T10:09:57.228398: step 568, loss 0.581214, acc 0.71875\n",
      "2017-06-30T10:09:57.363245: step 569, loss 0.620323, acc 0.65625\n",
      "2017-06-30T10:09:57.499834: step 570, loss 0.56494, acc 0.78125\n",
      "2017-06-30T10:09:57.629373: step 571, loss 0.628129, acc 0.6875\n",
      "2017-06-30T10:09:57.766119: step 572, loss 0.630721, acc 0.625\n",
      "2017-06-30T10:09:57.902945: step 573, loss 0.627709, acc 0.65625\n",
      "2017-06-30T10:09:58.034858: step 574, loss 0.569552, acc 0.765625\n",
      "2017-06-30T10:09:58.167784: step 575, loss 0.639316, acc 0.65625\n",
      "2017-06-30T10:09:58.297862: step 576, loss 0.514625, acc 0.796875\n",
      "2017-06-30T10:09:58.426460: step 577, loss 0.631822, acc 0.671875\n",
      "2017-06-30T10:09:58.554657: step 578, loss 0.541836, acc 0.734375\n",
      "2017-06-30T10:09:58.685356: step 579, loss 0.632442, acc 0.625\n",
      "2017-06-30T10:09:58.820131: step 580, loss 0.620097, acc 0.65625\n",
      "2017-06-30T10:09:58.949725: step 581, loss 0.649795, acc 0.640625\n",
      "2017-06-30T10:09:59.081104: step 582, loss 0.659503, acc 0.609375\n",
      "2017-06-30T10:09:59.209699: step 583, loss 0.598428, acc 0.671875\n",
      "2017-06-30T10:09:59.338925: step 584, loss 0.519101, acc 0.765625\n",
      "2017-06-30T10:09:59.466682: step 585, loss 0.511221, acc 0.796875\n",
      "2017-06-30T10:09:59.601714: step 586, loss 0.602328, acc 0.703125\n",
      "2017-06-30T10:09:59.742662: step 587, loss 0.571459, acc 0.71875\n",
      "2017-06-30T10:09:59.872970: step 588, loss 0.636714, acc 0.625\n",
      "2017-06-30T10:10:00.011623: step 589, loss 0.687349, acc 0.578125\n",
      "2017-06-30T10:10:00.145654: step 590, loss 0.711158, acc 0.609375\n",
      "2017-06-30T10:10:00.286284: step 591, loss 0.680366, acc 0.609375\n",
      "2017-06-30T10:10:00.416736: step 592, loss 0.560233, acc 0.734375\n",
      "2017-06-30T10:10:00.553417: step 593, loss 0.580907, acc 0.703125\n",
      "2017-06-30T10:10:00.682261: step 594, loss 0.468948, acc 0.859375\n",
      "2017-06-30T10:10:00.815805: step 595, loss 0.557914, acc 0.75\n",
      "2017-06-30T10:10:00.945414: step 596, loss 0.579215, acc 0.703125\n",
      "2017-06-30T10:10:01.073477: step 597, loss 0.605244, acc 0.6875\n",
      "2017-06-30T10:10:01.198181: step 598, loss 0.605446, acc 0.71875\n",
      "2017-06-30T10:10:01.323059: step 599, loss 0.638452, acc 0.65625\n",
      "2017-06-30T10:10:01.443985: step 600, loss 0.627204, acc 0.65\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:10:01.764944: step 600, loss 0.638108, acc 0.651032\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-600\n",
      "\n",
      "2017-06-30T10:10:02.485616: step 601, loss 0.648315, acc 0.625\n",
      "2017-06-30T10:10:02.604745: step 602, loss 0.561094, acc 0.765625\n",
      "2017-06-30T10:10:02.729520: step 603, loss 0.593759, acc 0.6875\n",
      "2017-06-30T10:10:02.854803: step 604, loss 0.63124, acc 0.625\n",
      "2017-06-30T10:10:02.982571: step 605, loss 0.554089, acc 0.75\n",
      "2017-06-30T10:10:03.109373: step 606, loss 0.506933, acc 0.796875\n",
      "2017-06-30T10:10:03.231399: step 607, loss 0.553488, acc 0.75\n",
      "2017-06-30T10:10:03.352573: step 608, loss 0.501831, acc 0.828125\n",
      "2017-06-30T10:10:03.478452: step 609, loss 0.557369, acc 0.734375\n",
      "2017-06-30T10:10:03.607716: step 610, loss 0.53111, acc 0.78125\n",
      "2017-06-30T10:10:03.740476: step 611, loss 0.538407, acc 0.765625\n",
      "2017-06-30T10:10:03.873596: step 612, loss 0.51405, acc 0.765625\n",
      "2017-06-30T10:10:04.002667: step 613, loss 0.542345, acc 0.78125\n",
      "2017-06-30T10:10:04.124945: step 614, loss 0.616137, acc 0.65625\n",
      "2017-06-30T10:10:04.251117: step 615, loss 0.611832, acc 0.671875\n",
      "2017-06-30T10:10:04.374879: step 616, loss 0.611116, acc 0.703125\n",
      "2017-06-30T10:10:04.498468: step 617, loss 0.621298, acc 0.640625\n",
      "2017-06-30T10:10:04.629949: step 618, loss 0.524311, acc 0.796875\n",
      "2017-06-30T10:10:04.756691: step 619, loss 0.591198, acc 0.703125\n",
      "2017-06-30T10:10:04.883554: step 620, loss 0.604732, acc 0.65625\n",
      "2017-06-30T10:10:05.011104: step 621, loss 0.602054, acc 0.6875\n",
      "2017-06-30T10:10:05.143411: step 622, loss 0.542823, acc 0.734375\n",
      "2017-06-30T10:10:05.270256: step 623, loss 0.484542, acc 0.828125\n",
      "2017-06-30T10:10:05.391015: step 624, loss 0.571336, acc 0.734375\n",
      "2017-06-30T10:10:05.519744: step 625, loss 0.485361, acc 0.8125\n",
      "2017-06-30T10:10:05.652664: step 626, loss 0.505977, acc 0.8125\n",
      "2017-06-30T10:10:05.778462: step 627, loss 0.511358, acc 0.796875\n",
      "2017-06-30T10:10:05.904894: step 628, loss 0.555092, acc 0.734375\n",
      "2017-06-30T10:10:06.034876: step 629, loss 0.638055, acc 0.671875\n",
      "2017-06-30T10:10:06.162665: step 630, loss 0.598426, acc 0.6875\n",
      "2017-06-30T10:10:06.290400: step 631, loss 0.672211, acc 0.609375\n",
      "2017-06-30T10:10:06.415595: step 632, loss 0.594216, acc 0.71875\n",
      "2017-06-30T10:10:06.541630: step 633, loss 0.55882, acc 0.734375\n",
      "2017-06-30T10:10:06.667744: step 634, loss 0.537382, acc 0.765625\n",
      "2017-06-30T10:10:06.794988: step 635, loss 0.579646, acc 0.703125\n",
      "2017-06-30T10:10:06.920306: step 636, loss 0.565519, acc 0.703125\n",
      "2017-06-30T10:10:07.044189: step 637, loss 0.627915, acc 0.65625\n",
      "2017-06-30T10:10:07.171176: step 638, loss 0.511065, acc 0.796875\n",
      "2017-06-30T10:10:07.298357: step 639, loss 0.52104, acc 0.78125\n",
      "2017-06-30T10:10:07.426877: step 640, loss 0.501713, acc 0.8125\n",
      "2017-06-30T10:10:07.551548: step 641, loss 0.594718, acc 0.6875\n",
      "2017-06-30T10:10:07.676011: step 642, loss 0.591818, acc 0.6875\n",
      "2017-06-30T10:10:07.804873: step 643, loss 0.613708, acc 0.6875\n",
      "2017-06-30T10:10:07.929165: step 644, loss 0.605079, acc 0.6875\n",
      "2017-06-30T10:10:08.057139: step 645, loss 0.625245, acc 0.640625\n",
      "2017-06-30T10:10:08.180438: step 646, loss 0.55403, acc 0.703125\n",
      "2017-06-30T10:10:08.306429: step 647, loss 0.537123, acc 0.78125\n",
      "2017-06-30T10:10:08.426706: step 648, loss 0.609219, acc 0.6875\n",
      "2017-06-30T10:10:08.554247: step 649, loss 0.551047, acc 0.765625\n",
      "2017-06-30T10:10:08.678015: step 650, loss 0.52775, acc 0.78125\n",
      "2017-06-30T10:10:08.808476: step 651, loss 0.62635, acc 0.671875\n",
      "2017-06-30T10:10:08.936842: step 652, loss 0.549166, acc 0.75\n",
      "2017-06-30T10:10:09.067251: step 653, loss 0.539176, acc 0.765625\n",
      "2017-06-30T10:10:09.188719: step 654, loss 0.587394, acc 0.71875\n",
      "2017-06-30T10:10:09.317193: step 655, loss 0.636773, acc 0.65625\n",
      "2017-06-30T10:10:09.441993: step 656, loss 0.523869, acc 0.796875\n",
      "2017-06-30T10:10:09.569172: step 657, loss 0.569891, acc 0.734375\n",
      "2017-06-30T10:10:09.691030: step 658, loss 0.605218, acc 0.703125\n",
      "2017-06-30T10:10:09.818470: step 659, loss 0.441045, acc 0.859375\n",
      "2017-06-30T10:10:09.944865: step 660, loss 0.502726, acc 0.765625\n",
      "2017-06-30T10:10:10.064589: step 661, loss 0.567425, acc 0.71875\n",
      "2017-06-30T10:10:10.191930: step 662, loss 0.54589, acc 0.765625\n",
      "2017-06-30T10:10:10.316925: step 663, loss 0.538852, acc 0.765625\n",
      "2017-06-30T10:10:10.444388: step 664, loss 0.603532, acc 0.703125\n",
      "2017-06-30T10:10:10.569227: step 665, loss 0.648396, acc 0.625\n",
      "2017-06-30T10:10:10.693917: step 666, loss 0.547889, acc 0.734375\n",
      "2017-06-30T10:10:10.820946: step 667, loss 0.598386, acc 0.703125\n",
      "2017-06-30T10:10:10.946952: step 668, loss 0.545663, acc 0.765625\n",
      "2017-06-30T10:10:11.074441: step 669, loss 0.555466, acc 0.71875\n",
      "2017-06-30T10:10:11.198390: step 670, loss 0.534505, acc 0.78125\n",
      "2017-06-30T10:10:11.326318: step 671, loss 0.533054, acc 0.765625\n",
      "2017-06-30T10:10:11.455266: step 672, loss 0.567835, acc 0.71875\n",
      "2017-06-30T10:10:11.579812: step 673, loss 0.579132, acc 0.734375\n",
      "2017-06-30T10:10:11.698166: step 674, loss 0.58899, acc 0.71875\n",
      "2017-06-30T10:10:11.823480: step 675, loss 0.519708, acc 0.734375\n",
      "2017-06-30T10:10:11.948104: step 676, loss 0.578593, acc 0.734375\n",
      "2017-06-30T10:10:12.077521: step 677, loss 0.618659, acc 0.671875\n",
      "2017-06-30T10:10:12.201992: step 678, loss 0.546316, acc 0.765625\n",
      "2017-06-30T10:10:12.324838: step 679, loss 0.551612, acc 0.734375\n",
      "2017-06-30T10:10:12.449410: step 680, loss 0.514713, acc 0.75\n",
      "2017-06-30T10:10:12.574226: step 681, loss 0.604174, acc 0.6875\n",
      "2017-06-30T10:10:12.701299: step 682, loss 0.494339, acc 0.828125\n",
      "2017-06-30T10:10:12.823083: step 683, loss 0.603721, acc 0.703125\n",
      "2017-06-30T10:10:12.946077: step 684, loss 0.66827, acc 0.59375\n",
      "2017-06-30T10:10:13.068021: step 685, loss 0.593217, acc 0.703125\n",
      "2017-06-30T10:10:13.187616: step 686, loss 0.631834, acc 0.625\n",
      "2017-06-30T10:10:13.313454: step 687, loss 0.552552, acc 0.71875\n",
      "2017-06-30T10:10:13.435694: step 688, loss 0.593644, acc 0.6875\n",
      "2017-06-30T10:10:13.559218: step 689, loss 0.550641, acc 0.75\n",
      "2017-06-30T10:10:13.682829: step 690, loss 0.58272, acc 0.703125\n",
      "2017-06-30T10:10:13.810769: step 691, loss 0.48491, acc 0.8125\n",
      "2017-06-30T10:10:13.934351: step 692, loss 0.628762, acc 0.625\n",
      "2017-06-30T10:10:14.061306: step 693, loss 0.550353, acc 0.75\n",
      "2017-06-30T10:10:14.183914: step 694, loss 0.547545, acc 0.78125\n",
      "2017-06-30T10:10:14.311296: step 695, loss 0.579314, acc 0.734375\n",
      "2017-06-30T10:10:14.434873: step 696, loss 0.536424, acc 0.75\n",
      "2017-06-30T10:10:14.562260: step 697, loss 0.494428, acc 0.796875\n",
      "2017-06-30T10:10:14.687106: step 698, loss 0.542044, acc 0.75\n",
      "2017-06-30T10:10:14.810477: step 699, loss 0.620747, acc 0.6875\n",
      "2017-06-30T10:10:14.933135: step 700, loss 0.579859, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:10:15.241063: step 700, loss 0.628749, acc 0.655722\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-700\n",
      "\n",
      "2017-06-30T10:10:16.068511: step 701, loss 0.548762, acc 0.734375\n",
      "2017-06-30T10:10:16.188171: step 702, loss 0.646775, acc 0.625\n",
      "2017-06-30T10:10:16.310858: step 703, loss 0.647466, acc 0.640625\n",
      "2017-06-30T10:10:16.430828: step 704, loss 0.670955, acc 0.640625\n",
      "2017-06-30T10:10:16.555099: step 705, loss 0.568583, acc 0.6875\n",
      "2017-06-30T10:10:16.680168: step 706, loss 0.540488, acc 0.765625\n",
      "2017-06-30T10:10:16.804984: step 707, loss 0.607919, acc 0.65625\n",
      "2017-06-30T10:10:16.930681: step 708, loss 0.515543, acc 0.796875\n",
      "2017-06-30T10:10:17.059282: step 709, loss 0.67444, acc 0.625\n",
      "2017-06-30T10:10:17.182621: step 710, loss 0.543386, acc 0.765625\n",
      "2017-06-30T10:10:17.313130: step 711, loss 0.560062, acc 0.75\n",
      "2017-06-30T10:10:17.436048: step 712, loss 0.537088, acc 0.78125\n",
      "2017-06-30T10:10:17.557714: step 713, loss 0.599948, acc 0.703125\n",
      "2017-06-30T10:10:17.677762: step 714, loss 0.446236, acc 0.875\n",
      "2017-06-30T10:10:17.802220: step 715, loss 0.543672, acc 0.734375\n",
      "2017-06-30T10:10:17.922724: step 716, loss 0.526084, acc 0.765625\n",
      "2017-06-30T10:10:18.043013: step 717, loss 0.555532, acc 0.734375\n",
      "2017-06-30T10:10:18.164085: step 718, loss 0.554667, acc 0.734375\n",
      "2017-06-30T10:10:18.290498: step 719, loss 0.54706, acc 0.765625\n",
      "2017-06-30T10:10:18.414839: step 720, loss 0.567112, acc 0.703125\n",
      "2017-06-30T10:10:18.538264: step 721, loss 0.611268, acc 0.6875\n",
      "2017-06-30T10:10:18.658680: step 722, loss 0.505896, acc 0.796875\n",
      "2017-06-30T10:10:18.783000: step 723, loss 0.601938, acc 0.6875\n",
      "2017-06-30T10:10:18.903289: step 724, loss 0.630176, acc 0.640625\n",
      "2017-06-30T10:10:19.025356: step 725, loss 0.53914, acc 0.796875\n",
      "2017-06-30T10:10:19.151497: step 726, loss 0.540986, acc 0.78125\n",
      "2017-06-30T10:10:19.273390: step 727, loss 0.522057, acc 0.765625\n",
      "2017-06-30T10:10:19.393338: step 728, loss 0.541876, acc 0.75\n",
      "2017-06-30T10:10:19.516150: step 729, loss 0.512887, acc 0.8125\n",
      "2017-06-30T10:10:19.637335: step 730, loss 0.565891, acc 0.71875\n",
      "2017-06-30T10:10:19.762893: step 731, loss 0.546068, acc 0.734375\n",
      "2017-06-30T10:10:19.886058: step 732, loss 0.662638, acc 0.59375\n",
      "2017-06-30T10:10:20.012488: step 733, loss 0.604041, acc 0.671875\n",
      "2017-06-30T10:10:20.137277: step 734, loss 0.516179, acc 0.78125\n",
      "2017-06-30T10:10:20.264208: step 735, loss 0.502789, acc 0.828125\n",
      "2017-06-30T10:10:20.387361: step 736, loss 0.6017, acc 0.65625\n",
      "2017-06-30T10:10:20.514098: step 737, loss 0.532905, acc 0.734375\n",
      "2017-06-30T10:10:20.640545: step 738, loss 0.559524, acc 0.71875\n",
      "2017-06-30T10:10:20.763944: step 739, loss 0.498576, acc 0.796875\n",
      "2017-06-30T10:10:20.886034: step 740, loss 0.585639, acc 0.71875\n",
      "2017-06-30T10:10:21.003580: step 741, loss 0.606294, acc 0.703125\n",
      "2017-06-30T10:10:21.126770: step 742, loss 0.578695, acc 0.71875\n",
      "2017-06-30T10:10:21.250664: step 743, loss 0.577331, acc 0.703125\n",
      "2017-06-30T10:10:21.374783: step 744, loss 0.568709, acc 0.75\n",
      "2017-06-30T10:10:21.504350: step 745, loss 0.57518, acc 0.703125\n",
      "2017-06-30T10:10:21.628309: step 746, loss 0.569675, acc 0.75\n",
      "2017-06-30T10:10:21.751194: step 747, loss 0.600185, acc 0.6875\n",
      "2017-06-30T10:10:21.872301: step 748, loss 0.572471, acc 0.71875\n",
      "2017-06-30T10:10:21.998819: step 749, loss 0.50407, acc 0.8125\n",
      "2017-06-30T10:10:22.119726: step 750, loss 0.558757, acc 0.75\n",
      "2017-06-30T10:10:22.241457: step 751, loss 0.561572, acc 0.71875\n",
      "2017-06-30T10:10:22.370702: step 752, loss 0.560955, acc 0.734375\n",
      "2017-06-30T10:10:22.501396: step 753, loss 0.547457, acc 0.75\n",
      "2017-06-30T10:10:22.629504: step 754, loss 0.577876, acc 0.6875\n",
      "2017-06-30T10:10:22.757148: step 755, loss 0.525531, acc 0.796875\n",
      "2017-06-30T10:10:22.883223: step 756, loss 0.514587, acc 0.8125\n",
      "2017-06-30T10:10:23.006958: step 757, loss 0.549411, acc 0.75\n",
      "2017-06-30T10:10:23.136199: step 758, loss 0.520273, acc 0.78125\n",
      "2017-06-30T10:10:23.271873: step 759, loss 0.554842, acc 0.71875\n",
      "2017-06-30T10:10:23.395196: step 760, loss 0.575868, acc 0.71875\n",
      "2017-06-30T10:10:23.524308: step 761, loss 0.620735, acc 0.65625\n",
      "2017-06-30T10:10:23.653198: step 762, loss 0.442467, acc 0.84375\n",
      "2017-06-30T10:10:23.783001: step 763, loss 0.532738, acc 0.75\n",
      "2017-06-30T10:10:23.915902: step 764, loss 0.565121, acc 0.71875\n",
      "2017-06-30T10:10:24.046584: step 765, loss 0.564293, acc 0.734375\n",
      "2017-06-30T10:10:24.176802: step 766, loss 0.47945, acc 0.78125\n",
      "2017-06-30T10:10:24.308402: step 767, loss 0.524477, acc 0.796875\n",
      "2017-06-30T10:10:24.436382: step 768, loss 0.523376, acc 0.78125\n",
      "2017-06-30T10:10:24.567485: step 769, loss 0.505019, acc 0.796875\n",
      "2017-06-30T10:10:24.699065: step 770, loss 0.517786, acc 0.75\n",
      "2017-06-30T10:10:24.831286: step 771, loss 0.532392, acc 0.734375\n",
      "2017-06-30T10:10:24.963727: step 772, loss 0.505692, acc 0.796875\n",
      "2017-06-30T10:10:25.089444: step 773, loss 0.514664, acc 0.796875\n",
      "2017-06-30T10:10:25.215945: step 774, loss 0.472195, acc 0.84375\n",
      "2017-06-30T10:10:25.357110: step 775, loss 0.569639, acc 0.734375\n",
      "2017-06-30T10:10:25.517232: step 776, loss 0.432695, acc 0.890625\n",
      "2017-06-30T10:10:25.656081: step 777, loss 0.520056, acc 0.765625\n",
      "2017-06-30T10:10:25.821757: step 778, loss 0.523016, acc 0.78125\n",
      "2017-06-30T10:10:25.988020: step 779, loss 0.510649, acc 0.78125\n",
      "2017-06-30T10:10:26.130006: step 780, loss 0.51325, acc 0.78125\n",
      "2017-06-30T10:10:26.270628: step 781, loss 0.559113, acc 0.75\n",
      "2017-06-30T10:10:26.404576: step 782, loss 0.550245, acc 0.75\n",
      "2017-06-30T10:10:26.547182: step 783, loss 0.487154, acc 0.84375\n",
      "2017-06-30T10:10:26.684396: step 784, loss 0.571772, acc 0.71875\n",
      "2017-06-30T10:10:26.811826: step 785, loss 0.533246, acc 0.75\n",
      "2017-06-30T10:10:26.938123: step 786, loss 0.484897, acc 0.796875\n",
      "2017-06-30T10:10:27.068839: step 787, loss 0.568188, acc 0.75\n",
      "2017-06-30T10:10:27.197450: step 788, loss 0.441139, acc 0.859375\n",
      "2017-06-30T10:10:27.320709: step 789, loss 0.472631, acc 0.84375\n",
      "2017-06-30T10:10:27.447353: step 790, loss 0.585034, acc 0.6875\n",
      "2017-06-30T10:10:27.574595: step 791, loss 0.489588, acc 0.828125\n",
      "2017-06-30T10:10:27.699640: step 792, loss 0.528682, acc 0.796875\n",
      "2017-06-30T10:10:27.830614: step 793, loss 0.472341, acc 0.84375\n",
      "2017-06-30T10:10:27.968316: step 794, loss 0.524779, acc 0.78125\n",
      "2017-06-30T10:10:28.100147: step 795, loss 0.45567, acc 0.828125\n",
      "2017-06-30T10:10:28.245414: step 796, loss 0.556313, acc 0.75\n",
      "2017-06-30T10:10:28.381478: step 797, loss 0.484535, acc 0.84375\n",
      "2017-06-30T10:10:28.531049: step 798, loss 0.576914, acc 0.765625\n",
      "2017-06-30T10:10:28.671522: step 799, loss 0.566637, acc 0.71875\n",
      "2017-06-30T10:10:28.806359: step 800, loss 0.490158, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:10:29.233413: step 800, loss 0.616196, acc 0.672608\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-800\n",
      "\n",
      "2017-06-30T10:10:29.923099: step 801, loss 0.493242, acc 0.8125\n",
      "2017-06-30T10:10:30.048411: step 802, loss 0.575718, acc 0.71875\n",
      "2017-06-30T10:10:30.188689: step 803, loss 0.517513, acc 0.796875\n",
      "2017-06-30T10:10:30.356976: step 804, loss 0.488255, acc 0.8125\n",
      "2017-06-30T10:10:30.600515: step 805, loss 0.486279, acc 0.84375\n",
      "2017-06-30T10:10:30.741523: step 806, loss 0.549213, acc 0.765625\n",
      "2017-06-30T10:10:30.876668: step 807, loss 0.520031, acc 0.78125\n",
      "2017-06-30T10:10:31.006051: step 808, loss 0.555648, acc 0.765625\n",
      "2017-06-30T10:10:31.134365: step 809, loss 0.504421, acc 0.84375\n",
      "2017-06-30T10:10:31.264396: step 810, loss 0.519082, acc 0.78125\n",
      "2017-06-30T10:10:31.391012: step 811, loss 0.471008, acc 0.828125\n",
      "2017-06-30T10:10:31.522013: step 812, loss 0.495687, acc 0.796875\n",
      "2017-06-30T10:10:31.650727: step 813, loss 0.447443, acc 0.859375\n",
      "2017-06-30T10:10:31.782085: step 814, loss 0.502778, acc 0.765625\n",
      "2017-06-30T10:10:31.907712: step 815, loss 0.591895, acc 0.703125\n",
      "2017-06-30T10:10:32.041184: step 816, loss 0.575042, acc 0.734375\n",
      "2017-06-30T10:10:32.166080: step 817, loss 0.485566, acc 0.828125\n",
      "2017-06-30T10:10:32.303395: step 818, loss 0.553379, acc 0.734375\n",
      "2017-06-30T10:10:32.442116: step 819, loss 0.582852, acc 0.703125\n",
      "2017-06-30T10:10:32.588701: step 820, loss 0.544395, acc 0.765625\n",
      "2017-06-30T10:10:32.724545: step 821, loss 0.515043, acc 0.75\n",
      "2017-06-30T10:10:32.852669: step 822, loss 0.498736, acc 0.796875\n",
      "2017-06-30T10:10:32.978453: step 823, loss 0.515971, acc 0.8125\n",
      "2017-06-30T10:10:33.109550: step 824, loss 0.491277, acc 0.796875\n",
      "2017-06-30T10:10:33.243537: step 825, loss 0.526626, acc 0.765625\n",
      "2017-06-30T10:10:33.368248: step 826, loss 0.461249, acc 0.875\n",
      "2017-06-30T10:10:33.498686: step 827, loss 0.547612, acc 0.75\n",
      "2017-06-30T10:10:33.633435: step 828, loss 0.527812, acc 0.765625\n",
      "2017-06-30T10:10:33.758213: step 829, loss 0.468459, acc 0.828125\n",
      "2017-06-30T10:10:33.884642: step 830, loss 0.521924, acc 0.78125\n",
      "2017-06-30T10:10:34.045858: step 831, loss 0.522571, acc 0.78125\n",
      "2017-06-30T10:10:34.173870: step 832, loss 0.539487, acc 0.78125\n",
      "2017-06-30T10:10:34.301954: step 833, loss 0.515714, acc 0.796875\n",
      "2017-06-30T10:10:34.425569: step 834, loss 0.579826, acc 0.75\n",
      "2017-06-30T10:10:34.551759: step 835, loss 0.474794, acc 0.8125\n",
      "2017-06-30T10:10:34.677417: step 836, loss 0.527422, acc 0.78125\n",
      "2017-06-30T10:10:34.799476: step 837, loss 0.474799, acc 0.8125\n",
      "2017-06-30T10:10:34.924221: step 838, loss 0.496454, acc 0.796875\n",
      "2017-06-30T10:10:35.060318: step 839, loss 0.460807, acc 0.859375\n",
      "2017-06-30T10:10:35.185174: step 840, loss 0.479593, acc 0.84375\n",
      "2017-06-30T10:10:35.308814: step 841, loss 0.515987, acc 0.78125\n",
      "2017-06-30T10:10:35.435978: step 842, loss 0.559151, acc 0.75\n",
      "2017-06-30T10:10:35.560030: step 843, loss 0.531387, acc 0.765625\n",
      "2017-06-30T10:10:35.681037: step 844, loss 0.569299, acc 0.75\n",
      "2017-06-30T10:10:35.801172: step 845, loss 0.549666, acc 0.71875\n",
      "2017-06-30T10:10:35.922725: step 846, loss 0.524473, acc 0.78125\n",
      "2017-06-30T10:10:36.044957: step 847, loss 0.498047, acc 0.828125\n",
      "2017-06-30T10:10:36.176083: step 848, loss 0.540629, acc 0.765625\n",
      "2017-06-30T10:10:36.299491: step 849, loss 0.46383, acc 0.84375\n",
      "2017-06-30T10:10:36.428119: step 850, loss 0.604753, acc 0.6875\n",
      "2017-06-30T10:10:36.562682: step 851, loss 0.471397, acc 0.84375\n",
      "2017-06-30T10:10:36.698715: step 852, loss 0.50258, acc 0.8125\n",
      "2017-06-30T10:10:36.831622: step 853, loss 0.591214, acc 0.71875\n",
      "2017-06-30T10:10:36.961578: step 854, loss 0.568163, acc 0.71875\n",
      "2017-06-30T10:10:37.088791: step 855, loss 0.508633, acc 0.78125\n",
      "2017-06-30T10:10:37.213705: step 856, loss 0.545079, acc 0.71875\n",
      "2017-06-30T10:10:37.341674: step 857, loss 0.459192, acc 0.828125\n",
      "2017-06-30T10:10:37.471348: step 858, loss 0.566747, acc 0.734375\n",
      "2017-06-30T10:10:37.596106: step 859, loss 0.506157, acc 0.78125\n",
      "2017-06-30T10:10:37.727170: step 860, loss 0.509105, acc 0.796875\n",
      "2017-06-30T10:10:37.852830: step 861, loss 0.503562, acc 0.796875\n",
      "2017-06-30T10:10:37.977315: step 862, loss 0.533404, acc 0.78125\n",
      "2017-06-30T10:10:38.101629: step 863, loss 0.575883, acc 0.71875\n",
      "2017-06-30T10:10:38.228886: step 864, loss 0.589961, acc 0.71875\n",
      "2017-06-30T10:10:38.355416: step 865, loss 0.575366, acc 0.703125\n",
      "2017-06-30T10:10:38.480911: step 866, loss 0.509218, acc 0.828125\n",
      "2017-06-30T10:10:38.614142: step 867, loss 0.544922, acc 0.75\n",
      "2017-06-30T10:10:38.743799: step 868, loss 0.563957, acc 0.734375\n",
      "2017-06-30T10:10:38.871233: step 869, loss 0.539547, acc 0.765625\n",
      "2017-06-30T10:10:39.001455: step 870, loss 0.617516, acc 0.65625\n",
      "2017-06-30T10:10:39.134711: step 871, loss 0.550438, acc 0.765625\n",
      "2017-06-30T10:10:39.260938: step 872, loss 0.520703, acc 0.765625\n",
      "2017-06-30T10:10:39.394089: step 873, loss 0.469994, acc 0.859375\n",
      "2017-06-30T10:10:39.520869: step 874, loss 0.533832, acc 0.75\n",
      "2017-06-30T10:10:39.648978: step 875, loss 0.565728, acc 0.703125\n",
      "2017-06-30T10:10:39.775919: step 876, loss 0.505002, acc 0.828125\n",
      "2017-06-30T10:10:39.932935: step 877, loss 0.528489, acc 0.78125\n",
      "2017-06-30T10:10:40.082744: step 878, loss 0.507901, acc 0.796875\n",
      "2017-06-30T10:10:40.230143: step 879, loss 0.549053, acc 0.765625\n",
      "2017-06-30T10:10:40.382669: step 880, loss 0.514806, acc 0.78125\n",
      "2017-06-30T10:10:40.530577: step 881, loss 0.527639, acc 0.78125\n",
      "2017-06-30T10:10:40.678022: step 882, loss 0.559707, acc 0.703125\n",
      "2017-06-30T10:10:40.821907: step 883, loss 0.602896, acc 0.703125\n",
      "2017-06-30T10:10:40.978386: step 884, loss 0.514473, acc 0.765625\n",
      "2017-06-30T10:10:41.128859: step 885, loss 0.582923, acc 0.71875\n",
      "2017-06-30T10:10:41.278004: step 886, loss 0.554243, acc 0.75\n",
      "2017-06-30T10:10:41.426711: step 887, loss 0.539207, acc 0.75\n",
      "2017-06-30T10:10:41.576387: step 888, loss 0.567647, acc 0.734375\n",
      "2017-06-30T10:10:41.721341: step 889, loss 0.508804, acc 0.78125\n",
      "2017-06-30T10:10:41.861188: step 890, loss 0.514328, acc 0.796875\n",
      "2017-06-30T10:10:42.007410: step 891, loss 0.59644, acc 0.703125\n",
      "2017-06-30T10:10:42.158231: step 892, loss 0.587151, acc 0.734375\n",
      "2017-06-30T10:10:42.312137: step 893, loss 0.472131, acc 0.859375\n",
      "2017-06-30T10:10:42.471620: step 894, loss 0.565955, acc 0.765625\n",
      "2017-06-30T10:10:42.630369: step 895, loss 0.537803, acc 0.78125\n",
      "2017-06-30T10:10:42.774405: step 896, loss 0.417462, acc 0.90625\n",
      "2017-06-30T10:10:42.912849: step 897, loss 0.544684, acc 0.78125\n",
      "2017-06-30T10:10:43.043308: step 898, loss 0.44905, acc 0.84375\n",
      "2017-06-30T10:10:43.174643: step 899, loss 0.546236, acc 0.71875\n",
      "2017-06-30T10:10:43.297601: step 900, loss 0.582554, acc 0.7\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:10:43.620539: step 900, loss 0.609483, acc 0.682927\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-900\n",
      "\n",
      "2017-06-30T10:10:44.442672: step 901, loss 0.47945, acc 0.8125\n",
      "2017-06-30T10:10:44.565320: step 902, loss 0.518938, acc 0.78125\n",
      "2017-06-30T10:10:44.687187: step 903, loss 0.447727, acc 0.875\n",
      "2017-06-30T10:10:44.819706: step 904, loss 0.570372, acc 0.71875\n",
      "2017-06-30T10:10:44.946785: step 905, loss 0.483478, acc 0.828125\n",
      "2017-06-30T10:10:45.073438: step 906, loss 0.510171, acc 0.765625\n",
      "2017-06-30T10:10:45.217898: step 907, loss 0.511495, acc 0.796875\n",
      "2017-06-30T10:10:45.359191: step 908, loss 0.51772, acc 0.78125\n",
      "2017-06-30T10:10:45.482530: step 909, loss 0.489063, acc 0.828125\n",
      "2017-06-30T10:10:45.611763: step 910, loss 0.514574, acc 0.796875\n",
      "2017-06-30T10:10:45.740759: step 911, loss 0.517592, acc 0.78125\n",
      "2017-06-30T10:10:45.869010: step 912, loss 0.512875, acc 0.75\n",
      "2017-06-30T10:10:45.993929: step 913, loss 0.555329, acc 0.734375\n",
      "2017-06-30T10:10:46.119271: step 914, loss 0.543931, acc 0.734375\n",
      "2017-06-30T10:10:46.250448: step 915, loss 0.511444, acc 0.828125\n",
      "2017-06-30T10:10:46.377580: step 916, loss 0.395797, acc 0.9375\n",
      "2017-06-30T10:10:46.500101: step 917, loss 0.545565, acc 0.734375\n",
      "2017-06-30T10:10:46.627462: step 918, loss 0.49102, acc 0.8125\n",
      "2017-06-30T10:10:46.750600: step 919, loss 0.497926, acc 0.765625\n",
      "2017-06-30T10:10:46.877743: step 920, loss 0.455524, acc 0.828125\n",
      "2017-06-30T10:10:47.006450: step 921, loss 0.547009, acc 0.734375\n",
      "2017-06-30T10:10:47.134546: step 922, loss 0.453443, acc 0.84375\n",
      "2017-06-30T10:10:47.260988: step 923, loss 0.557031, acc 0.734375\n",
      "2017-06-30T10:10:47.386466: step 924, loss 0.492433, acc 0.8125\n",
      "2017-06-30T10:10:47.513190: step 925, loss 0.456444, acc 0.859375\n",
      "2017-06-30T10:10:47.641876: step 926, loss 0.571504, acc 0.75\n",
      "2017-06-30T10:10:47.765484: step 927, loss 0.472173, acc 0.8125\n",
      "2017-06-30T10:10:47.889729: step 928, loss 0.422313, acc 0.875\n",
      "2017-06-30T10:10:48.009308: step 929, loss 0.441177, acc 0.859375\n",
      "2017-06-30T10:10:48.131488: step 930, loss 0.455346, acc 0.859375\n",
      "2017-06-30T10:10:48.260233: step 931, loss 0.520068, acc 0.78125\n",
      "2017-06-30T10:10:48.387759: step 932, loss 0.534663, acc 0.78125\n",
      "2017-06-30T10:10:48.513660: step 933, loss 0.423102, acc 0.921875\n",
      "2017-06-30T10:10:48.640904: step 934, loss 0.492723, acc 0.796875\n",
      "2017-06-30T10:10:48.766862: step 935, loss 0.539555, acc 0.765625\n",
      "2017-06-30T10:10:48.896268: step 936, loss 0.456927, acc 0.859375\n",
      "2017-06-30T10:10:49.020490: step 937, loss 0.489522, acc 0.8125\n",
      "2017-06-30T10:10:49.140743: step 938, loss 0.507573, acc 0.75\n",
      "2017-06-30T10:10:49.264771: step 939, loss 0.444595, acc 0.84375\n",
      "2017-06-30T10:10:49.386932: step 940, loss 0.515818, acc 0.78125\n",
      "2017-06-30T10:10:49.511952: step 941, loss 0.533976, acc 0.75\n",
      "2017-06-30T10:10:49.635819: step 942, loss 0.462074, acc 0.859375\n",
      "2017-06-30T10:10:49.765974: step 943, loss 0.569458, acc 0.734375\n",
      "2017-06-30T10:10:49.905335: step 944, loss 0.490727, acc 0.8125\n",
      "2017-06-30T10:10:50.040147: step 945, loss 0.49777, acc 0.796875\n",
      "2017-06-30T10:10:50.169557: step 946, loss 0.517339, acc 0.78125\n",
      "2017-06-30T10:10:50.296825: step 947, loss 0.50643, acc 0.78125\n",
      "2017-06-30T10:10:50.427278: step 948, loss 0.531587, acc 0.75\n",
      "2017-06-30T10:10:50.557048: step 949, loss 0.473552, acc 0.8125\n",
      "2017-06-30T10:10:50.682354: step 950, loss 0.524477, acc 0.796875\n",
      "2017-06-30T10:10:50.809832: step 951, loss 0.48651, acc 0.828125\n",
      "2017-06-30T10:10:50.928387: step 952, loss 0.469437, acc 0.84375\n",
      "2017-06-30T10:10:51.054417: step 953, loss 0.569284, acc 0.71875\n",
      "2017-06-30T10:10:51.179808: step 954, loss 0.540749, acc 0.734375\n",
      "2017-06-30T10:10:51.305477: step 955, loss 0.390345, acc 0.9375\n",
      "2017-06-30T10:10:51.426666: step 956, loss 0.537506, acc 0.796875\n",
      "2017-06-30T10:10:51.553621: step 957, loss 0.555132, acc 0.734375\n",
      "2017-06-30T10:10:51.672701: step 958, loss 0.517896, acc 0.78125\n",
      "2017-06-30T10:10:51.796012: step 959, loss 0.514032, acc 0.78125\n",
      "2017-06-30T10:10:51.920822: step 960, loss 0.552656, acc 0.75\n",
      "2017-06-30T10:10:52.043886: step 961, loss 0.503952, acc 0.8125\n",
      "2017-06-30T10:10:52.166373: step 962, loss 0.44679, acc 0.859375\n",
      "2017-06-30T10:10:52.296465: step 963, loss 0.491558, acc 0.828125\n",
      "2017-06-30T10:10:52.420671: step 964, loss 0.489355, acc 0.8125\n",
      "2017-06-30T10:10:52.547086: step 965, loss 0.586265, acc 0.71875\n",
      "2017-06-30T10:10:52.669602: step 966, loss 0.508729, acc 0.765625\n",
      "2017-06-30T10:10:52.797459: step 967, loss 0.459726, acc 0.859375\n",
      "2017-06-30T10:10:52.920916: step 968, loss 0.531731, acc 0.78125\n",
      "2017-06-30T10:10:53.041528: step 969, loss 0.468545, acc 0.828125\n",
      "2017-06-30T10:10:53.167536: step 970, loss 0.501775, acc 0.8125\n",
      "2017-06-30T10:10:53.292690: step 971, loss 0.551741, acc 0.765625\n",
      "2017-06-30T10:10:53.417951: step 972, loss 0.468975, acc 0.828125\n",
      "2017-06-30T10:10:53.543252: step 973, loss 0.457533, acc 0.859375\n",
      "2017-06-30T10:10:53.662188: step 974, loss 0.524169, acc 0.75\n",
      "2017-06-30T10:10:53.787633: step 975, loss 0.584768, acc 0.703125\n",
      "2017-06-30T10:10:53.909565: step 976, loss 0.45875, acc 0.828125\n",
      "2017-06-30T10:10:54.036753: step 977, loss 0.465751, acc 0.84375\n",
      "2017-06-30T10:10:54.161531: step 978, loss 0.405069, acc 0.90625\n",
      "2017-06-30T10:10:54.286367: step 979, loss 0.443784, acc 0.875\n",
      "2017-06-30T10:10:54.410827: step 980, loss 0.458137, acc 0.84375\n",
      "2017-06-30T10:10:54.537100: step 981, loss 0.428502, acc 0.890625\n",
      "2017-06-30T10:10:54.665484: step 982, loss 0.461759, acc 0.828125\n",
      "2017-06-30T10:10:54.788852: step 983, loss 0.524655, acc 0.765625\n",
      "2017-06-30T10:10:54.909563: step 984, loss 0.449274, acc 0.84375\n",
      "2017-06-30T10:10:55.033593: step 985, loss 0.470808, acc 0.828125\n",
      "2017-06-30T10:10:55.157965: step 986, loss 0.472265, acc 0.828125\n",
      "2017-06-30T10:10:55.282155: step 987, loss 0.440014, acc 0.875\n",
      "2017-06-30T10:10:55.402722: step 988, loss 0.430834, acc 0.875\n",
      "2017-06-30T10:10:55.527725: step 989, loss 0.55108, acc 0.765625\n",
      "2017-06-30T10:10:55.655259: step 990, loss 0.520619, acc 0.75\n",
      "2017-06-30T10:10:55.778588: step 991, loss 0.4956, acc 0.8125\n",
      "2017-06-30T10:10:55.903817: step 992, loss 0.535698, acc 0.765625\n",
      "2017-06-30T10:10:56.022967: step 993, loss 0.430227, acc 0.90625\n",
      "2017-06-30T10:10:56.144041: step 994, loss 0.50869, acc 0.796875\n",
      "2017-06-30T10:10:56.268612: step 995, loss 0.516317, acc 0.796875\n",
      "2017-06-30T10:10:56.389741: step 996, loss 0.478041, acc 0.84375\n",
      "2017-06-30T10:10:56.515243: step 997, loss 0.507791, acc 0.78125\n",
      "2017-06-30T10:10:56.642248: step 998, loss 0.516193, acc 0.796875\n",
      "2017-06-30T10:10:56.769421: step 999, loss 0.484916, acc 0.796875\n",
      "2017-06-30T10:10:56.893727: step 1000, loss 0.468706, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:10:57.196066: step 1000, loss 0.602843, acc 0.684803\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1000\n",
      "\n",
      "2017-06-30T10:10:58.025921: step 1001, loss 0.480931, acc 0.828125\n",
      "2017-06-30T10:10:58.142639: step 1002, loss 0.483372, acc 0.8125\n",
      "2017-06-30T10:10:58.265060: step 1003, loss 0.491828, acc 0.828125\n",
      "2017-06-30T10:10:58.384835: step 1004, loss 0.52034, acc 0.78125\n",
      "2017-06-30T10:10:58.503104: step 1005, loss 0.544573, acc 0.75\n",
      "2017-06-30T10:10:58.624970: step 1006, loss 0.428988, acc 0.859375\n",
      "2017-06-30T10:10:58.746814: step 1007, loss 0.505282, acc 0.765625\n",
      "2017-06-30T10:10:58.865567: step 1008, loss 0.569376, acc 0.734375\n",
      "2017-06-30T10:10:58.989391: step 1009, loss 0.506234, acc 0.8125\n",
      "2017-06-30T10:10:59.112314: step 1010, loss 0.49056, acc 0.828125\n",
      "2017-06-30T10:10:59.235226: step 1011, loss 0.499692, acc 0.8125\n",
      "2017-06-30T10:10:59.355029: step 1012, loss 0.482632, acc 0.84375\n",
      "2017-06-30T10:10:59.482450: step 1013, loss 0.526645, acc 0.78125\n",
      "2017-06-30T10:10:59.603505: step 1014, loss 0.463534, acc 0.84375\n",
      "2017-06-30T10:10:59.724936: step 1015, loss 0.533905, acc 0.765625\n",
      "2017-06-30T10:10:59.848541: step 1016, loss 0.480519, acc 0.828125\n",
      "2017-06-30T10:10:59.969257: step 1017, loss 0.509136, acc 0.78125\n",
      "2017-06-30T10:11:00.091240: step 1018, loss 0.477923, acc 0.828125\n",
      "2017-06-30T10:11:00.210978: step 1019, loss 0.47678, acc 0.828125\n",
      "2017-06-30T10:11:00.330802: step 1020, loss 0.590247, acc 0.71875\n",
      "2017-06-30T10:11:00.455052: step 1021, loss 0.468597, acc 0.84375\n",
      "2017-06-30T10:11:00.580219: step 1022, loss 0.457202, acc 0.859375\n",
      "2017-06-30T10:11:00.702701: step 1023, loss 0.535165, acc 0.75\n",
      "2017-06-30T10:11:00.831684: step 1024, loss 0.482355, acc 0.828125\n",
      "2017-06-30T10:11:00.952336: step 1025, loss 0.561644, acc 0.71875\n",
      "2017-06-30T10:11:01.076033: step 1026, loss 0.505548, acc 0.8125\n",
      "2017-06-30T10:11:01.202118: step 1027, loss 0.485537, acc 0.8125\n",
      "2017-06-30T10:11:01.324362: step 1028, loss 0.488239, acc 0.8125\n",
      "2017-06-30T10:11:01.447794: step 1029, loss 0.522793, acc 0.765625\n",
      "2017-06-30T10:11:01.568474: step 1030, loss 0.541248, acc 0.71875\n",
      "2017-06-30T10:11:01.698039: step 1031, loss 0.6149, acc 0.671875\n",
      "2017-06-30T10:11:01.822259: step 1032, loss 0.525227, acc 0.78125\n",
      "2017-06-30T10:11:01.944379: step 1033, loss 0.512031, acc 0.78125\n",
      "2017-06-30T10:11:02.066559: step 1034, loss 0.519424, acc 0.796875\n",
      "2017-06-30T10:11:02.190582: step 1035, loss 0.514303, acc 0.796875\n",
      "2017-06-30T10:11:02.310789: step 1036, loss 0.450103, acc 0.859375\n",
      "2017-06-30T10:11:02.431305: step 1037, loss 0.507503, acc 0.796875\n",
      "2017-06-30T10:11:02.552891: step 1038, loss 0.413412, acc 0.90625\n",
      "2017-06-30T10:11:02.674891: step 1039, loss 0.529373, acc 0.8125\n",
      "2017-06-30T10:11:02.798192: step 1040, loss 0.562693, acc 0.75\n",
      "2017-06-30T10:11:02.924001: step 1041, loss 0.446053, acc 0.875\n",
      "2017-06-30T10:11:03.047743: step 1042, loss 0.403709, acc 0.921875\n",
      "2017-06-30T10:11:03.170660: step 1043, loss 0.50744, acc 0.78125\n",
      "2017-06-30T10:11:03.290219: step 1044, loss 0.558996, acc 0.703125\n",
      "2017-06-30T10:11:03.406832: step 1045, loss 0.49737, acc 0.828125\n",
      "2017-06-30T10:11:03.523942: step 1046, loss 0.568541, acc 0.6875\n",
      "2017-06-30T10:11:03.649687: step 1047, loss 0.522721, acc 0.78125\n",
      "2017-06-30T10:11:03.768234: step 1048, loss 0.504124, acc 0.796875\n",
      "2017-06-30T10:11:03.892244: step 1049, loss 0.500066, acc 0.828125\n",
      "2017-06-30T10:11:04.012834: step 1050, loss 0.452202, acc 0.866667\n",
      "2017-06-30T10:11:04.132571: step 1051, loss 0.456514, acc 0.828125\n",
      "2017-06-30T10:11:04.255845: step 1052, loss 0.396515, acc 0.921875\n",
      "2017-06-30T10:11:04.378757: step 1053, loss 0.453447, acc 0.859375\n",
      "2017-06-30T10:11:04.497201: step 1054, loss 0.460322, acc 0.84375\n",
      "2017-06-30T10:11:04.618378: step 1055, loss 0.523225, acc 0.78125\n",
      "2017-06-30T10:11:04.740437: step 1056, loss 0.512146, acc 0.796875\n",
      "2017-06-30T10:11:04.861373: step 1057, loss 0.514549, acc 0.765625\n",
      "2017-06-30T10:11:04.985462: step 1058, loss 0.517387, acc 0.734375\n",
      "2017-06-30T10:11:05.107504: step 1059, loss 0.457778, acc 0.859375\n",
      "2017-06-30T10:11:05.232592: step 1060, loss 0.478358, acc 0.828125\n",
      "2017-06-30T10:11:05.355455: step 1061, loss 0.489797, acc 0.828125\n",
      "2017-06-30T10:11:05.471503: step 1062, loss 0.499253, acc 0.796875\n",
      "2017-06-30T10:11:05.592635: step 1063, loss 0.449191, acc 0.84375\n",
      "2017-06-30T10:11:05.720878: step 1064, loss 0.494371, acc 0.78125\n",
      "2017-06-30T10:11:05.848129: step 1065, loss 0.450447, acc 0.859375\n",
      "2017-06-30T10:11:05.980786: step 1066, loss 0.483169, acc 0.828125\n",
      "2017-06-30T10:11:06.103328: step 1067, loss 0.508369, acc 0.78125\n",
      "2017-06-30T10:11:06.225026: step 1068, loss 0.460575, acc 0.859375\n",
      "2017-06-30T10:11:06.353913: step 1069, loss 0.477054, acc 0.828125\n",
      "2017-06-30T10:11:06.487559: step 1070, loss 0.509619, acc 0.78125\n",
      "2017-06-30T10:11:06.609584: step 1071, loss 0.44704, acc 0.859375\n",
      "2017-06-30T10:11:06.733783: step 1072, loss 0.486832, acc 0.796875\n",
      "2017-06-30T10:11:06.860186: step 1073, loss 0.473711, acc 0.828125\n",
      "2017-06-30T10:11:06.988241: step 1074, loss 0.494524, acc 0.796875\n",
      "2017-06-30T10:11:07.112736: step 1075, loss 0.466717, acc 0.828125\n",
      "2017-06-30T10:11:07.237586: step 1076, loss 0.448554, acc 0.84375\n",
      "2017-06-30T10:11:07.362946: step 1077, loss 0.43283, acc 0.875\n",
      "2017-06-30T10:11:07.482659: step 1078, loss 0.461879, acc 0.8125\n",
      "2017-06-30T10:11:07.614207: step 1079, loss 0.497612, acc 0.8125\n",
      "2017-06-30T10:11:07.740552: step 1080, loss 0.424086, acc 0.875\n",
      "2017-06-30T10:11:07.867010: step 1081, loss 0.480168, acc 0.828125\n",
      "2017-06-30T10:11:07.992298: step 1082, loss 0.493591, acc 0.8125\n",
      "2017-06-30T10:11:08.117630: step 1083, loss 0.423535, acc 0.875\n",
      "2017-06-30T10:11:08.236699: step 1084, loss 0.468148, acc 0.84375\n",
      "2017-06-30T10:11:08.361442: step 1085, loss 0.527944, acc 0.75\n",
      "2017-06-30T10:11:08.483234: step 1086, loss 0.505322, acc 0.8125\n",
      "2017-06-30T10:11:08.609533: step 1087, loss 0.477103, acc 0.84375\n",
      "2017-06-30T10:11:08.730875: step 1088, loss 0.46829, acc 0.828125\n",
      "2017-06-30T10:11:08.853351: step 1089, loss 0.498554, acc 0.8125\n",
      "2017-06-30T10:11:08.978409: step 1090, loss 0.456606, acc 0.859375\n",
      "2017-06-30T10:11:09.106925: step 1091, loss 0.506422, acc 0.8125\n",
      "2017-06-30T10:11:09.229136: step 1092, loss 0.544912, acc 0.75\n",
      "2017-06-30T10:11:09.348248: step 1093, loss 0.428388, acc 0.890625\n",
      "2017-06-30T10:11:09.465482: step 1094, loss 0.490291, acc 0.828125\n",
      "2017-06-30T10:11:09.585863: step 1095, loss 0.514624, acc 0.8125\n",
      "2017-06-30T10:11:09.706024: step 1096, loss 0.467008, acc 0.84375\n",
      "2017-06-30T10:11:09.828908: step 1097, loss 0.440159, acc 0.859375\n",
      "2017-06-30T10:11:09.956195: step 1098, loss 0.444633, acc 0.84375\n",
      "2017-06-30T10:11:10.078553: step 1099, loss 0.535174, acc 0.75\n",
      "2017-06-30T10:11:10.205736: step 1100, loss 0.439278, acc 0.875\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:11:10.504023: step 1100, loss 0.589289, acc 0.706379\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1100\n",
      "\n",
      "2017-06-30T10:11:11.128073: step 1101, loss 0.518674, acc 0.78125\n",
      "2017-06-30T10:11:11.244880: step 1102, loss 0.531819, acc 0.75\n",
      "2017-06-30T10:11:11.363691: step 1103, loss 0.510824, acc 0.8125\n",
      "2017-06-30T10:11:11.483398: step 1104, loss 0.473888, acc 0.828125\n",
      "2017-06-30T10:11:11.602983: step 1105, loss 0.45968, acc 0.859375\n",
      "2017-06-30T10:11:11.723558: step 1106, loss 0.411788, acc 0.890625\n",
      "2017-06-30T10:11:11.849772: step 1107, loss 0.600174, acc 0.703125\n",
      "2017-06-30T10:11:11.969051: step 1108, loss 0.498032, acc 0.796875\n",
      "2017-06-30T10:11:12.092254: step 1109, loss 0.416062, acc 0.875\n",
      "2017-06-30T10:11:12.218205: step 1110, loss 0.478254, acc 0.84375\n",
      "2017-06-30T10:11:12.342769: step 1111, loss 0.400135, acc 0.953125\n",
      "2017-06-30T10:11:12.469039: step 1112, loss 0.458226, acc 0.859375\n",
      "2017-06-30T10:11:12.589945: step 1113, loss 0.408361, acc 0.890625\n",
      "2017-06-30T10:11:12.713005: step 1114, loss 0.453756, acc 0.84375\n",
      "2017-06-30T10:11:12.853137: step 1115, loss 0.427713, acc 0.890625\n",
      "2017-06-30T10:11:12.980428: step 1116, loss 0.461366, acc 0.84375\n",
      "2017-06-30T10:11:13.114301: step 1117, loss 0.431686, acc 0.890625\n",
      "2017-06-30T10:11:13.241892: step 1118, loss 0.510027, acc 0.8125\n",
      "2017-06-30T10:11:13.361227: step 1119, loss 0.423677, acc 0.890625\n",
      "2017-06-30T10:11:13.484889: step 1120, loss 0.449546, acc 0.875\n",
      "2017-06-30T10:11:13.607576: step 1121, loss 0.4885, acc 0.828125\n",
      "2017-06-30T10:11:13.729714: step 1122, loss 0.473547, acc 0.828125\n",
      "2017-06-30T10:11:13.854571: step 1123, loss 0.523764, acc 0.765625\n",
      "2017-06-30T10:11:13.979677: step 1124, loss 0.451987, acc 0.859375\n",
      "2017-06-30T10:11:14.104528: step 1125, loss 0.39037, acc 0.921875\n",
      "2017-06-30T10:11:14.228427: step 1126, loss 0.470386, acc 0.859375\n",
      "2017-06-30T10:11:14.351079: step 1127, loss 0.504421, acc 0.796875\n",
      "2017-06-30T10:11:14.476646: step 1128, loss 0.493463, acc 0.8125\n",
      "2017-06-30T10:11:14.599899: step 1129, loss 0.533936, acc 0.75\n",
      "2017-06-30T10:11:14.720503: step 1130, loss 0.513551, acc 0.796875\n",
      "2017-06-30T10:11:14.845953: step 1131, loss 0.416857, acc 0.90625\n",
      "2017-06-30T10:11:14.964516: step 1132, loss 0.492796, acc 0.78125\n",
      "2017-06-30T10:11:15.086607: step 1133, loss 0.43916, acc 0.859375\n",
      "2017-06-30T10:11:15.208936: step 1134, loss 0.440598, acc 0.859375\n",
      "2017-06-30T10:11:15.330734: step 1135, loss 0.482416, acc 0.78125\n",
      "2017-06-30T10:11:15.451262: step 1136, loss 0.456505, acc 0.859375\n",
      "2017-06-30T10:11:15.575195: step 1137, loss 0.495821, acc 0.828125\n",
      "2017-06-30T10:11:15.696409: step 1138, loss 0.476481, acc 0.828125\n",
      "2017-06-30T10:11:15.817868: step 1139, loss 0.473736, acc 0.8125\n",
      "2017-06-30T10:11:15.938526: step 1140, loss 0.494049, acc 0.828125\n",
      "2017-06-30T10:11:16.061361: step 1141, loss 0.429932, acc 0.890625\n",
      "2017-06-30T10:11:16.184324: step 1142, loss 0.483433, acc 0.8125\n",
      "2017-06-30T10:11:16.304489: step 1143, loss 0.43943, acc 0.875\n",
      "2017-06-30T10:11:16.426243: step 1144, loss 0.485563, acc 0.84375\n",
      "2017-06-30T10:11:16.548109: step 1145, loss 0.472242, acc 0.859375\n",
      "2017-06-30T10:11:16.666968: step 1146, loss 0.475279, acc 0.8125\n",
      "2017-06-30T10:11:16.786964: step 1147, loss 0.510111, acc 0.796875\n",
      "2017-06-30T10:11:16.905565: step 1148, loss 0.446677, acc 0.890625\n",
      "2017-06-30T10:11:17.028425: step 1149, loss 0.453067, acc 0.859375\n",
      "2017-06-30T10:11:17.151731: step 1150, loss 0.438852, acc 0.875\n",
      "2017-06-30T10:11:17.270447: step 1151, loss 0.490128, acc 0.796875\n",
      "2017-06-30T10:11:17.389312: step 1152, loss 0.481347, acc 0.84375\n",
      "2017-06-30T10:11:17.508518: step 1153, loss 0.515303, acc 0.796875\n",
      "2017-06-30T10:11:17.630652: step 1154, loss 0.441307, acc 0.859375\n",
      "2017-06-30T10:11:17.751158: step 1155, loss 0.524799, acc 0.765625\n",
      "2017-06-30T10:11:17.872878: step 1156, loss 0.486628, acc 0.828125\n",
      "2017-06-30T10:11:17.996535: step 1157, loss 0.504606, acc 0.8125\n",
      "2017-06-30T10:11:18.123319: step 1158, loss 0.455519, acc 0.875\n",
      "2017-06-30T10:11:18.248349: step 1159, loss 0.422003, acc 0.90625\n",
      "2017-06-30T10:11:18.373949: step 1160, loss 0.561794, acc 0.75\n",
      "2017-06-30T10:11:18.496253: step 1161, loss 0.483702, acc 0.8125\n",
      "2017-06-30T10:11:18.618566: step 1162, loss 0.517297, acc 0.78125\n",
      "2017-06-30T10:11:18.747593: step 1163, loss 0.439888, acc 0.890625\n",
      "2017-06-30T10:11:18.870601: step 1164, loss 0.494736, acc 0.8125\n",
      "2017-06-30T10:11:18.994799: step 1165, loss 0.424515, acc 0.890625\n",
      "2017-06-30T10:11:19.138414: step 1166, loss 0.431897, acc 0.890625\n",
      "2017-06-30T10:11:19.278885: step 1167, loss 0.457153, acc 0.859375\n",
      "2017-06-30T10:11:19.413565: step 1168, loss 0.470849, acc 0.84375\n",
      "2017-06-30T10:11:19.552286: step 1169, loss 0.427299, acc 0.90625\n",
      "2017-06-30T10:11:19.673227: step 1170, loss 0.43928, acc 0.859375\n",
      "2017-06-30T10:11:19.797448: step 1171, loss 0.503656, acc 0.8125\n",
      "2017-06-30T10:11:19.920777: step 1172, loss 0.447507, acc 0.859375\n",
      "2017-06-30T10:11:20.068443: step 1173, loss 0.512742, acc 0.78125\n",
      "2017-06-30T10:11:20.203487: step 1174, loss 0.444813, acc 0.859375\n",
      "2017-06-30T10:11:20.331542: step 1175, loss 0.469094, acc 0.84375\n",
      "2017-06-30T10:11:20.462266: step 1176, loss 0.449644, acc 0.875\n",
      "2017-06-30T10:11:20.606467: step 1177, loss 0.456989, acc 0.859375\n",
      "2017-06-30T10:11:20.736230: step 1178, loss 0.491617, acc 0.828125\n",
      "2017-06-30T10:11:20.859948: step 1179, loss 0.454832, acc 0.859375\n",
      "2017-06-30T10:11:20.992480: step 1180, loss 0.458817, acc 0.828125\n",
      "2017-06-30T10:11:21.110943: step 1181, loss 0.514723, acc 0.796875\n",
      "2017-06-30T10:11:21.232869: step 1182, loss 0.536684, acc 0.734375\n",
      "2017-06-30T10:11:21.355745: step 1183, loss 0.450313, acc 0.859375\n",
      "2017-06-30T10:11:21.478586: step 1184, loss 0.519521, acc 0.765625\n",
      "2017-06-30T10:11:21.605227: step 1185, loss 0.500093, acc 0.8125\n",
      "2017-06-30T10:11:21.728299: step 1186, loss 0.437743, acc 0.875\n",
      "2017-06-30T10:11:21.847760: step 1187, loss 0.433332, acc 0.890625\n",
      "2017-06-30T10:11:21.968109: step 1188, loss 0.460374, acc 0.859375\n",
      "2017-06-30T10:11:22.094677: step 1189, loss 0.47044, acc 0.828125\n",
      "2017-06-30T10:11:22.220015: step 1190, loss 0.408496, acc 0.90625\n",
      "2017-06-30T10:11:22.342336: step 1191, loss 0.519507, acc 0.765625\n",
      "2017-06-30T10:11:22.462721: step 1192, loss 0.463221, acc 0.84375\n",
      "2017-06-30T10:11:22.589073: step 1193, loss 0.492153, acc 0.78125\n",
      "2017-06-30T10:11:22.715178: step 1194, loss 0.466331, acc 0.859375\n",
      "2017-06-30T10:11:22.837414: step 1195, loss 0.421267, acc 0.90625\n",
      "2017-06-30T10:11:22.963842: step 1196, loss 0.531372, acc 0.765625\n",
      "2017-06-30T10:11:23.085581: step 1197, loss 0.461717, acc 0.828125\n",
      "2017-06-30T10:11:23.211878: step 1198, loss 0.43526, acc 0.890625\n",
      "2017-06-30T10:11:23.333775: step 1199, loss 0.448725, acc 0.859375\n",
      "2017-06-30T10:11:23.448852: step 1200, loss 0.521946, acc 0.766667\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:11:23.753954: step 1200, loss 0.586913, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1200\n",
      "\n",
      "2017-06-30T10:11:24.508476: step 1201, loss 0.412238, acc 0.90625\n",
      "2017-06-30T10:11:24.629302: step 1202, loss 0.442121, acc 0.890625\n",
      "2017-06-30T10:11:24.765970: step 1203, loss 0.479461, acc 0.828125\n",
      "2017-06-30T10:11:24.895740: step 1204, loss 0.426819, acc 0.90625\n",
      "2017-06-30T10:11:25.017056: step 1205, loss 0.501039, acc 0.796875\n",
      "2017-06-30T10:11:25.138493: step 1206, loss 0.451808, acc 0.84375\n",
      "2017-06-30T10:11:25.275310: step 1207, loss 0.49091, acc 0.796875\n",
      "2017-06-30T10:11:25.396213: step 1208, loss 0.393542, acc 0.90625\n",
      "2017-06-30T10:11:25.519236: step 1209, loss 0.418159, acc 0.90625\n",
      "2017-06-30T10:11:25.647670: step 1210, loss 0.465096, acc 0.859375\n",
      "2017-06-30T10:11:25.786348: step 1211, loss 0.376215, acc 0.953125\n",
      "2017-06-30T10:11:25.916755: step 1212, loss 0.401386, acc 0.90625\n",
      "2017-06-30T10:11:26.057266: step 1213, loss 0.44465, acc 0.875\n",
      "2017-06-30T10:11:26.181052: step 1214, loss 0.413021, acc 0.90625\n",
      "2017-06-30T10:11:26.300448: step 1215, loss 0.463538, acc 0.859375\n",
      "2017-06-30T10:11:26.423476: step 1216, loss 0.469061, acc 0.84375\n",
      "2017-06-30T10:11:26.546510: step 1217, loss 0.41998, acc 0.890625\n",
      "2017-06-30T10:11:26.671926: step 1218, loss 0.360411, acc 0.96875\n",
      "2017-06-30T10:11:26.794153: step 1219, loss 0.416067, acc 0.875\n",
      "2017-06-30T10:11:26.925875: step 1220, loss 0.41517, acc 0.90625\n",
      "2017-06-30T10:11:27.053569: step 1221, loss 0.420723, acc 0.90625\n",
      "2017-06-30T10:11:27.178679: step 1222, loss 0.423984, acc 0.859375\n",
      "2017-06-30T10:11:27.308401: step 1223, loss 0.412017, acc 0.890625\n",
      "2017-06-30T10:11:27.429709: step 1224, loss 0.541493, acc 0.765625\n",
      "2017-06-30T10:11:27.555175: step 1225, loss 0.468396, acc 0.84375\n",
      "2017-06-30T10:11:27.671837: step 1226, loss 0.421106, acc 0.859375\n",
      "2017-06-30T10:11:27.801506: step 1227, loss 0.449015, acc 0.859375\n",
      "2017-06-30T10:11:27.929967: step 1228, loss 0.426034, acc 0.875\n",
      "2017-06-30T10:11:28.050389: step 1229, loss 0.435654, acc 0.859375\n",
      "2017-06-30T10:11:28.176316: step 1230, loss 0.458452, acc 0.859375\n",
      "2017-06-30T10:11:28.298377: step 1231, loss 0.405395, acc 0.921875\n",
      "2017-06-30T10:11:28.433899: step 1232, loss 0.504912, acc 0.796875\n",
      "2017-06-30T10:11:28.555705: step 1233, loss 0.410259, acc 0.890625\n",
      "2017-06-30T10:11:28.685505: step 1234, loss 0.378411, acc 0.953125\n",
      "2017-06-30T10:11:28.815284: step 1235, loss 0.504862, acc 0.78125\n",
      "2017-06-30T10:11:28.940117: step 1236, loss 0.388158, acc 0.9375\n",
      "2017-06-30T10:11:29.064444: step 1237, loss 0.462023, acc 0.828125\n",
      "2017-06-30T10:11:29.191122: step 1238, loss 0.410321, acc 0.90625\n",
      "2017-06-30T10:11:29.325621: step 1239, loss 0.479893, acc 0.8125\n",
      "2017-06-30T10:11:29.451343: step 1240, loss 0.467707, acc 0.84375\n",
      "2017-06-30T10:11:29.575986: step 1241, loss 0.455224, acc 0.84375\n",
      "2017-06-30T10:11:29.699625: step 1242, loss 0.393178, acc 0.921875\n",
      "2017-06-30T10:11:29.830216: step 1243, loss 0.500012, acc 0.78125\n",
      "2017-06-30T10:11:29.953164: step 1244, loss 0.463759, acc 0.859375\n",
      "2017-06-30T10:11:30.080785: step 1245, loss 0.450576, acc 0.859375\n",
      "2017-06-30T10:11:30.218619: step 1246, loss 0.497715, acc 0.8125\n",
      "2017-06-30T10:11:30.352847: step 1247, loss 0.387051, acc 0.921875\n",
      "2017-06-30T10:11:30.483613: step 1248, loss 0.494045, acc 0.8125\n",
      "2017-06-30T10:11:30.608815: step 1249, loss 0.422497, acc 0.890625\n",
      "2017-06-30T10:11:30.730564: step 1250, loss 0.454642, acc 0.859375\n",
      "2017-06-30T10:11:30.854544: step 1251, loss 0.409487, acc 0.875\n",
      "2017-06-30T10:11:30.973145: step 1252, loss 0.524097, acc 0.8125\n",
      "2017-06-30T10:11:31.093988: step 1253, loss 0.405838, acc 0.875\n",
      "2017-06-30T10:11:31.219333: step 1254, loss 0.49321, acc 0.78125\n",
      "2017-06-30T10:11:31.344507: step 1255, loss 0.439101, acc 0.859375\n",
      "2017-06-30T10:11:31.469696: step 1256, loss 0.434989, acc 0.875\n",
      "2017-06-30T10:11:31.592862: step 1257, loss 0.425203, acc 0.875\n",
      "2017-06-30T10:11:31.711162: step 1258, loss 0.462983, acc 0.859375\n",
      "2017-06-30T10:11:31.828283: step 1259, loss 0.445399, acc 0.875\n",
      "2017-06-30T10:11:31.947233: step 1260, loss 0.406823, acc 0.90625\n",
      "2017-06-30T10:11:32.069439: step 1261, loss 0.441984, acc 0.84375\n",
      "2017-06-30T10:11:32.190619: step 1262, loss 0.411307, acc 0.921875\n",
      "2017-06-30T10:11:32.311312: step 1263, loss 0.501356, acc 0.796875\n",
      "2017-06-30T10:11:32.433942: step 1264, loss 0.394857, acc 0.890625\n",
      "2017-06-30T10:11:32.556720: step 1265, loss 0.504007, acc 0.796875\n",
      "2017-06-30T10:11:32.679558: step 1266, loss 0.477187, acc 0.828125\n",
      "2017-06-30T10:11:32.800766: step 1267, loss 0.47583, acc 0.84375\n",
      "2017-06-30T10:11:32.922585: step 1268, loss 0.378945, acc 0.953125\n",
      "2017-06-30T10:11:33.048392: step 1269, loss 0.469394, acc 0.84375\n",
      "2017-06-30T10:11:33.172307: step 1270, loss 0.45951, acc 0.84375\n",
      "2017-06-30T10:11:33.302070: step 1271, loss 0.454142, acc 0.828125\n",
      "2017-06-30T10:11:33.425398: step 1272, loss 0.443574, acc 0.84375\n",
      "2017-06-30T10:11:33.546057: step 1273, loss 0.446572, acc 0.859375\n",
      "2017-06-30T10:11:33.666141: step 1274, loss 0.44951, acc 0.859375\n",
      "2017-06-30T10:11:33.789105: step 1275, loss 0.451824, acc 0.859375\n",
      "2017-06-30T10:11:33.912560: step 1276, loss 0.500414, acc 0.796875\n",
      "2017-06-30T10:11:34.039165: step 1277, loss 0.440544, acc 0.859375\n",
      "2017-06-30T10:11:34.171120: step 1278, loss 0.508947, acc 0.796875\n",
      "2017-06-30T10:11:34.298516: step 1279, loss 0.442191, acc 0.875\n",
      "2017-06-30T10:11:34.420275: step 1280, loss 0.521033, acc 0.78125\n",
      "2017-06-30T10:11:34.551132: step 1281, loss 0.512484, acc 0.78125\n",
      "2017-06-30T10:11:34.682900: step 1282, loss 0.439563, acc 0.890625\n",
      "2017-06-30T10:11:34.811022: step 1283, loss 0.441931, acc 0.84375\n",
      "2017-06-30T10:11:34.935193: step 1284, loss 0.499708, acc 0.8125\n",
      "2017-06-30T10:11:35.059244: step 1285, loss 0.425038, acc 0.875\n",
      "2017-06-30T10:11:35.182210: step 1286, loss 0.448065, acc 0.859375\n",
      "2017-06-30T10:11:35.303956: step 1287, loss 0.406272, acc 0.9375\n",
      "2017-06-30T10:11:35.422676: step 1288, loss 0.398843, acc 0.921875\n",
      "2017-06-30T10:11:35.542115: step 1289, loss 0.49359, acc 0.796875\n",
      "2017-06-30T10:11:35.660496: step 1290, loss 0.445446, acc 0.875\n",
      "2017-06-30T10:11:35.782216: step 1291, loss 0.464641, acc 0.828125\n",
      "2017-06-30T10:11:35.904389: step 1292, loss 0.493326, acc 0.8125\n",
      "2017-06-30T10:11:36.028284: step 1293, loss 0.465174, acc 0.84375\n",
      "2017-06-30T10:11:36.149373: step 1294, loss 0.429853, acc 0.875\n",
      "2017-06-30T10:11:36.270654: step 1295, loss 0.396208, acc 0.921875\n",
      "2017-06-30T10:11:36.395561: step 1296, loss 0.423217, acc 0.890625\n",
      "2017-06-30T10:11:36.525867: step 1297, loss 0.438447, acc 0.890625\n",
      "2017-06-30T10:11:36.656354: step 1298, loss 0.373438, acc 0.9375\n",
      "2017-06-30T10:11:36.784746: step 1299, loss 0.502571, acc 0.828125\n",
      "2017-06-30T10:11:36.907815: step 1300, loss 0.39957, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:11:37.209573: step 1300, loss 0.58327, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1300\n",
      "\n",
      "2017-06-30T10:11:38.034834: step 1301, loss 0.472037, acc 0.828125\n",
      "2017-06-30T10:11:38.150305: step 1302, loss 0.414517, acc 0.890625\n",
      "2017-06-30T10:11:38.272100: step 1303, loss 0.408529, acc 0.890625\n",
      "2017-06-30T10:11:38.393496: step 1304, loss 0.493879, acc 0.828125\n",
      "2017-06-30T10:11:38.515432: step 1305, loss 0.43605, acc 0.875\n",
      "2017-06-30T10:11:38.639402: step 1306, loss 0.470015, acc 0.828125\n",
      "2017-06-30T10:11:38.763163: step 1307, loss 0.494498, acc 0.828125\n",
      "2017-06-30T10:11:38.886656: step 1308, loss 0.451226, acc 0.875\n",
      "2017-06-30T10:11:39.019334: step 1309, loss 0.474291, acc 0.859375\n",
      "2017-06-30T10:11:39.144926: step 1310, loss 0.43125, acc 0.875\n",
      "2017-06-30T10:11:39.266827: step 1311, loss 0.467237, acc 0.84375\n",
      "2017-06-30T10:11:39.393908: step 1312, loss 0.438186, acc 0.875\n",
      "2017-06-30T10:11:39.526153: step 1313, loss 0.454321, acc 0.875\n",
      "2017-06-30T10:11:39.647112: step 1314, loss 0.44966, acc 0.859375\n",
      "2017-06-30T10:11:39.768263: step 1315, loss 0.404397, acc 0.90625\n",
      "2017-06-30T10:11:39.888992: step 1316, loss 0.456628, acc 0.859375\n",
      "2017-06-30T10:11:40.019037: step 1317, loss 0.46379, acc 0.84375\n",
      "2017-06-30T10:11:40.139901: step 1318, loss 0.406061, acc 0.890625\n",
      "2017-06-30T10:11:40.266475: step 1319, loss 0.416651, acc 0.890625\n",
      "2017-06-30T10:11:40.385808: step 1320, loss 0.468099, acc 0.84375\n",
      "2017-06-30T10:11:40.507191: step 1321, loss 0.439429, acc 0.84375\n",
      "2017-06-30T10:11:40.634177: step 1322, loss 0.396044, acc 0.9375\n",
      "2017-06-30T10:11:40.769499: step 1323, loss 0.412515, acc 0.90625\n",
      "2017-06-30T10:11:40.896082: step 1324, loss 0.501585, acc 0.828125\n",
      "2017-06-30T10:11:41.021771: step 1325, loss 0.502557, acc 0.8125\n",
      "2017-06-30T10:11:41.149188: step 1326, loss 0.408953, acc 0.90625\n",
      "2017-06-30T10:11:41.275104: step 1327, loss 0.486787, acc 0.8125\n",
      "2017-06-30T10:11:41.394371: step 1328, loss 0.517982, acc 0.78125\n",
      "2017-06-30T10:11:41.517864: step 1329, loss 0.562702, acc 0.734375\n",
      "2017-06-30T10:11:41.642229: step 1330, loss 0.488777, acc 0.796875\n",
      "2017-06-30T10:11:41.770091: step 1331, loss 0.492492, acc 0.796875\n",
      "2017-06-30T10:11:41.896222: step 1332, loss 0.427038, acc 0.875\n",
      "2017-06-30T10:11:42.019464: step 1333, loss 0.451622, acc 0.84375\n",
      "2017-06-30T10:11:42.139977: step 1334, loss 0.437492, acc 0.859375\n",
      "2017-06-30T10:11:42.263164: step 1335, loss 0.438976, acc 0.859375\n",
      "2017-06-30T10:11:42.385133: step 1336, loss 0.403781, acc 0.9375\n",
      "2017-06-30T10:11:42.507172: step 1337, loss 0.435871, acc 0.875\n",
      "2017-06-30T10:11:42.630743: step 1338, loss 0.449869, acc 0.84375\n",
      "2017-06-30T10:11:42.752661: step 1339, loss 0.440592, acc 0.859375\n",
      "2017-06-30T10:11:42.878951: step 1340, loss 0.427484, acc 0.890625\n",
      "2017-06-30T10:11:43.014138: step 1341, loss 0.422041, acc 0.890625\n",
      "2017-06-30T10:11:43.145843: step 1342, loss 0.457331, acc 0.859375\n",
      "2017-06-30T10:11:43.269336: step 1343, loss 0.459583, acc 0.859375\n",
      "2017-06-30T10:11:43.393652: step 1344, loss 0.384872, acc 0.921875\n",
      "2017-06-30T10:11:43.518249: step 1345, loss 0.444864, acc 0.875\n",
      "2017-06-30T10:11:43.637812: step 1346, loss 0.378207, acc 0.9375\n",
      "2017-06-30T10:11:43.758274: step 1347, loss 0.415501, acc 0.875\n",
      "2017-06-30T10:11:43.880857: step 1348, loss 0.497997, acc 0.8125\n",
      "2017-06-30T10:11:44.008000: step 1349, loss 0.461872, acc 0.84375\n",
      "2017-06-30T10:11:44.126172: step 1350, loss 0.457018, acc 0.85\n",
      "2017-06-30T10:11:44.252015: step 1351, loss 0.409236, acc 0.90625\n",
      "2017-06-30T10:11:44.389052: step 1352, loss 0.396441, acc 0.921875\n",
      "2017-06-30T10:11:44.515512: step 1353, loss 0.436302, acc 0.859375\n",
      "2017-06-30T10:11:44.643478: step 1354, loss 0.433182, acc 0.875\n",
      "2017-06-30T10:11:44.768793: step 1355, loss 0.400483, acc 0.90625\n",
      "2017-06-30T10:11:44.896497: step 1356, loss 0.354503, acc 0.96875\n",
      "2017-06-30T10:11:45.019572: step 1357, loss 0.455887, acc 0.828125\n",
      "2017-06-30T10:11:45.143025: step 1358, loss 0.424252, acc 0.90625\n",
      "2017-06-30T10:11:45.267186: step 1359, loss 0.401215, acc 0.921875\n",
      "2017-06-30T10:11:45.391841: step 1360, loss 0.463777, acc 0.84375\n",
      "2017-06-30T10:11:45.513330: step 1361, loss 0.468107, acc 0.84375\n",
      "2017-06-30T10:11:45.634999: step 1362, loss 0.443855, acc 0.859375\n",
      "2017-06-30T10:11:45.771760: step 1363, loss 0.382673, acc 0.921875\n",
      "2017-06-30T10:11:45.901986: step 1364, loss 0.436981, acc 0.875\n",
      "2017-06-30T10:11:46.025196: step 1365, loss 0.410135, acc 0.90625\n",
      "2017-06-30T10:11:46.149804: step 1366, loss 0.420897, acc 0.890625\n",
      "2017-06-30T10:11:46.273435: step 1367, loss 0.384474, acc 0.9375\n",
      "2017-06-30T10:11:46.395679: step 1368, loss 0.420074, acc 0.890625\n",
      "2017-06-30T10:11:46.516999: step 1369, loss 0.48288, acc 0.828125\n",
      "2017-06-30T10:11:46.636193: step 1370, loss 0.412412, acc 0.890625\n",
      "2017-06-30T10:11:46.759382: step 1371, loss 0.416954, acc 0.90625\n",
      "2017-06-30T10:11:46.877998: step 1372, loss 0.381903, acc 0.9375\n",
      "2017-06-30T10:11:46.998670: step 1373, loss 0.453145, acc 0.84375\n",
      "2017-06-30T10:11:47.119385: step 1374, loss 0.470168, acc 0.84375\n",
      "2017-06-30T10:11:47.244302: step 1375, loss 0.458525, acc 0.875\n",
      "2017-06-30T10:11:47.365090: step 1376, loss 0.397521, acc 0.921875\n",
      "2017-06-30T10:11:47.491948: step 1377, loss 0.418598, acc 0.890625\n",
      "2017-06-30T10:11:47.613184: step 1378, loss 0.515467, acc 0.78125\n",
      "2017-06-30T10:11:47.730587: step 1379, loss 0.416194, acc 0.890625\n",
      "2017-06-30T10:11:47.855308: step 1380, loss 0.334486, acc 1\n",
      "2017-06-30T10:11:47.979107: step 1381, loss 0.464065, acc 0.84375\n",
      "2017-06-30T10:11:48.100093: step 1382, loss 0.425098, acc 0.890625\n",
      "2017-06-30T10:11:48.224318: step 1383, loss 0.44237, acc 0.84375\n",
      "2017-06-30T10:11:48.349287: step 1384, loss 0.410717, acc 0.921875\n",
      "2017-06-30T10:11:48.470825: step 1385, loss 0.449172, acc 0.859375\n",
      "2017-06-30T10:11:48.590535: step 1386, loss 0.405322, acc 0.9375\n",
      "2017-06-30T10:11:48.715137: step 1387, loss 0.453018, acc 0.859375\n",
      "2017-06-30T10:11:48.837802: step 1388, loss 0.430684, acc 0.875\n",
      "2017-06-30T10:11:48.965011: step 1389, loss 0.396176, acc 0.9375\n",
      "2017-06-30T10:11:49.106380: step 1390, loss 0.450208, acc 0.828125\n",
      "2017-06-30T10:11:49.231140: step 1391, loss 0.453152, acc 0.875\n",
      "2017-06-30T10:11:49.374905: step 1392, loss 0.423953, acc 0.859375\n",
      "2017-06-30T10:11:49.499823: step 1393, loss 0.498713, acc 0.796875\n",
      "2017-06-30T10:11:49.625071: step 1394, loss 0.41269, acc 0.890625\n",
      "2017-06-30T10:11:49.766169: step 1395, loss 0.449308, acc 0.84375\n",
      "2017-06-30T10:11:49.890214: step 1396, loss 0.518854, acc 0.796875\n",
      "2017-06-30T10:11:50.012276: step 1397, loss 0.411023, acc 0.9375\n",
      "2017-06-30T10:11:50.137467: step 1398, loss 0.448324, acc 0.859375\n",
      "2017-06-30T10:11:50.258696: step 1399, loss 0.450902, acc 0.84375\n",
      "2017-06-30T10:11:50.381786: step 1400, loss 0.441716, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:11:50.684243: step 1400, loss 0.585512, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1400\n",
      "\n",
      "2017-06-30T10:11:51.340867: step 1401, loss 0.456953, acc 0.828125\n",
      "2017-06-30T10:11:51.458368: step 1402, loss 0.505715, acc 0.796875\n",
      "2017-06-30T10:11:51.584739: step 1403, loss 0.423969, acc 0.890625\n",
      "2017-06-30T10:11:51.702732: step 1404, loss 0.410892, acc 0.90625\n",
      "2017-06-30T10:11:51.830738: step 1405, loss 0.408952, acc 0.921875\n",
      "2017-06-30T10:11:51.950081: step 1406, loss 0.425223, acc 0.90625\n",
      "2017-06-30T10:11:52.071904: step 1407, loss 0.473627, acc 0.8125\n",
      "2017-06-30T10:11:52.190665: step 1408, loss 0.454859, acc 0.859375\n",
      "2017-06-30T10:11:52.312864: step 1409, loss 0.48439, acc 0.796875\n",
      "2017-06-30T10:11:52.434398: step 1410, loss 0.424761, acc 0.875\n",
      "2017-06-30T10:11:52.561105: step 1411, loss 0.357729, acc 0.953125\n",
      "2017-06-30T10:11:52.679082: step 1412, loss 0.418133, acc 0.890625\n",
      "2017-06-30T10:11:52.802894: step 1413, loss 0.424112, acc 0.890625\n",
      "2017-06-30T10:11:52.925361: step 1414, loss 0.458794, acc 0.859375\n",
      "2017-06-30T10:11:53.048646: step 1415, loss 0.3981, acc 0.875\n",
      "2017-06-30T10:11:53.166336: step 1416, loss 0.399443, acc 0.9375\n",
      "2017-06-30T10:11:53.286981: step 1417, loss 0.456973, acc 0.859375\n",
      "2017-06-30T10:11:53.408723: step 1418, loss 0.416943, acc 0.875\n",
      "2017-06-30T10:11:53.532758: step 1419, loss 0.400248, acc 0.90625\n",
      "2017-06-30T10:11:53.657272: step 1420, loss 0.450163, acc 0.875\n",
      "2017-06-30T10:11:53.777778: step 1421, loss 0.45069, acc 0.859375\n",
      "2017-06-30T10:11:53.898183: step 1422, loss 0.418487, acc 0.875\n",
      "2017-06-30T10:11:54.019963: step 1423, loss 0.384372, acc 0.90625\n",
      "2017-06-30T10:11:54.140991: step 1424, loss 0.418626, acc 0.890625\n",
      "2017-06-30T10:11:54.260490: step 1425, loss 0.438119, acc 0.859375\n",
      "2017-06-30T10:11:54.384752: step 1426, loss 0.383852, acc 0.9375\n",
      "2017-06-30T10:11:54.508814: step 1427, loss 0.375615, acc 0.9375\n",
      "2017-06-30T10:11:54.633201: step 1428, loss 0.381663, acc 0.921875\n",
      "2017-06-30T10:11:54.758035: step 1429, loss 0.399762, acc 0.90625\n",
      "2017-06-30T10:11:54.879992: step 1430, loss 0.389494, acc 0.9375\n",
      "2017-06-30T10:11:55.003681: step 1431, loss 0.431204, acc 0.875\n",
      "2017-06-30T10:11:55.121000: step 1432, loss 0.443356, acc 0.890625\n",
      "2017-06-30T10:11:55.243960: step 1433, loss 0.377575, acc 0.9375\n",
      "2017-06-30T10:11:55.363101: step 1434, loss 0.385527, acc 0.9375\n",
      "2017-06-30T10:11:55.486215: step 1435, loss 0.387123, acc 0.921875\n",
      "2017-06-30T10:11:55.607797: step 1436, loss 0.413703, acc 0.90625\n",
      "2017-06-30T10:11:55.732352: step 1437, loss 0.419753, acc 0.859375\n",
      "2017-06-30T10:11:55.852819: step 1438, loss 0.396915, acc 0.921875\n",
      "2017-06-30T10:11:55.970904: step 1439, loss 0.495612, acc 0.796875\n",
      "2017-06-30T10:11:56.091539: step 1440, loss 0.419749, acc 0.890625\n",
      "2017-06-30T10:11:56.212453: step 1441, loss 0.425335, acc 0.890625\n",
      "2017-06-30T10:11:56.331806: step 1442, loss 0.473724, acc 0.828125\n",
      "2017-06-30T10:11:56.451697: step 1443, loss 0.498665, acc 0.828125\n",
      "2017-06-30T10:11:56.571140: step 1444, loss 0.467257, acc 0.84375\n",
      "2017-06-30T10:11:56.695326: step 1445, loss 0.445563, acc 0.859375\n",
      "2017-06-30T10:11:56.816520: step 1446, loss 0.350881, acc 0.953125\n",
      "2017-06-30T10:11:56.937987: step 1447, loss 0.416263, acc 0.890625\n",
      "2017-06-30T10:11:57.054009: step 1448, loss 0.474351, acc 0.828125\n",
      "2017-06-30T10:11:57.176773: step 1449, loss 0.446438, acc 0.84375\n",
      "2017-06-30T10:11:57.301854: step 1450, loss 0.395635, acc 0.90625\n",
      "2017-06-30T10:11:57.425316: step 1451, loss 0.458002, acc 0.828125\n",
      "2017-06-30T10:11:57.544441: step 1452, loss 0.479761, acc 0.828125\n",
      "2017-06-30T10:11:57.659592: step 1453, loss 0.414207, acc 0.890625\n",
      "2017-06-30T10:11:57.779327: step 1454, loss 0.41642, acc 0.859375\n",
      "2017-06-30T10:11:57.904034: step 1455, loss 0.406539, acc 0.890625\n",
      "2017-06-30T10:11:58.023656: step 1456, loss 0.492326, acc 0.8125\n",
      "2017-06-30T10:11:58.146758: step 1457, loss 0.41913, acc 0.890625\n",
      "2017-06-30T10:11:58.267652: step 1458, loss 0.444777, acc 0.875\n",
      "2017-06-30T10:11:58.392715: step 1459, loss 0.492284, acc 0.78125\n",
      "2017-06-30T10:11:58.515531: step 1460, loss 0.440045, acc 0.859375\n",
      "2017-06-30T10:11:58.633997: step 1461, loss 0.406648, acc 0.921875\n",
      "2017-06-30T10:11:58.752648: step 1462, loss 0.411698, acc 0.890625\n",
      "2017-06-30T10:11:58.872321: step 1463, loss 0.494845, acc 0.8125\n",
      "2017-06-30T10:11:58.991821: step 1464, loss 0.433654, acc 0.890625\n",
      "2017-06-30T10:11:59.113816: step 1465, loss 0.44269, acc 0.875\n",
      "2017-06-30T10:11:59.234272: step 1466, loss 0.404124, acc 0.890625\n",
      "2017-06-30T10:11:59.360501: step 1467, loss 0.42047, acc 0.890625\n",
      "2017-06-30T10:11:59.476905: step 1468, loss 0.473512, acc 0.828125\n",
      "2017-06-30T10:11:59.603609: step 1469, loss 0.472745, acc 0.84375\n",
      "2017-06-30T10:11:59.731256: step 1470, loss 0.441893, acc 0.859375\n",
      "2017-06-30T10:11:59.850980: step 1471, loss 0.465884, acc 0.84375\n",
      "2017-06-30T10:11:59.972506: step 1472, loss 0.437862, acc 0.890625\n",
      "2017-06-30T10:12:00.102912: step 1473, loss 0.44181, acc 0.859375\n",
      "2017-06-30T10:12:00.226176: step 1474, loss 0.43586, acc 0.875\n",
      "2017-06-30T10:12:00.351968: step 1475, loss 0.403964, acc 0.921875\n",
      "2017-06-30T10:12:00.471322: step 1476, loss 0.422355, acc 0.90625\n",
      "2017-06-30T10:12:00.595801: step 1477, loss 0.448646, acc 0.875\n",
      "2017-06-30T10:12:00.722368: step 1478, loss 0.418024, acc 0.890625\n",
      "2017-06-30T10:12:00.856185: step 1479, loss 0.482754, acc 0.8125\n",
      "2017-06-30T10:12:01.002290: step 1480, loss 0.449873, acc 0.875\n",
      "2017-06-30T10:12:01.141666: step 1481, loss 0.418058, acc 0.890625\n",
      "2017-06-30T10:12:01.266878: step 1482, loss 0.408043, acc 0.921875\n",
      "2017-06-30T10:12:01.402355: step 1483, loss 0.434845, acc 0.890625\n",
      "2017-06-30T10:12:01.529272: step 1484, loss 0.448607, acc 0.859375\n",
      "2017-06-30T10:12:01.655124: step 1485, loss 0.444252, acc 0.859375\n",
      "2017-06-30T10:12:01.774969: step 1486, loss 0.359779, acc 0.96875\n",
      "2017-06-30T10:12:01.898690: step 1487, loss 0.412532, acc 0.921875\n",
      "2017-06-30T10:12:02.022975: step 1488, loss 0.379939, acc 0.921875\n",
      "2017-06-30T10:12:02.145113: step 1489, loss 0.409645, acc 0.921875\n",
      "2017-06-30T10:12:02.269327: step 1490, loss 0.439708, acc 0.875\n",
      "2017-06-30T10:12:02.401213: step 1491, loss 0.35642, acc 0.96875\n",
      "2017-06-30T10:12:02.534747: step 1492, loss 0.470281, acc 0.828125\n",
      "2017-06-30T10:12:02.674740: step 1493, loss 0.466718, acc 0.859375\n",
      "2017-06-30T10:12:02.802843: step 1494, loss 0.416116, acc 0.890625\n",
      "2017-06-30T10:12:02.935404: step 1495, loss 0.446083, acc 0.890625\n",
      "2017-06-30T10:12:03.062051: step 1496, loss 0.48423, acc 0.8125\n",
      "2017-06-30T10:12:03.188723: step 1497, loss 0.48915, acc 0.8125\n",
      "2017-06-30T10:12:03.307334: step 1498, loss 0.414482, acc 0.90625\n",
      "2017-06-30T10:12:03.430754: step 1499, loss 0.477693, acc 0.8125\n",
      "2017-06-30T10:12:03.548000: step 1500, loss 0.423268, acc 0.883333\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:12:03.853282: step 1500, loss 0.582456, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1500\n",
      "\n",
      "2017-06-30T10:12:04.618699: step 1501, loss 0.415204, acc 0.890625\n",
      "2017-06-30T10:12:04.735550: step 1502, loss 0.43742, acc 0.875\n",
      "2017-06-30T10:12:04.863400: step 1503, loss 0.356963, acc 0.96875\n",
      "2017-06-30T10:12:04.982987: step 1504, loss 0.389647, acc 0.953125\n",
      "2017-06-30T10:12:05.106480: step 1505, loss 0.452263, acc 0.859375\n",
      "2017-06-30T10:12:05.231460: step 1506, loss 0.407081, acc 0.90625\n",
      "2017-06-30T10:12:05.349524: step 1507, loss 0.355163, acc 0.96875\n",
      "2017-06-30T10:12:05.480054: step 1508, loss 0.466302, acc 0.828125\n",
      "2017-06-30T10:12:05.627918: step 1509, loss 0.377189, acc 0.9375\n",
      "2017-06-30T10:12:05.813185: step 1510, loss 0.401743, acc 0.921875\n",
      "2017-06-30T10:12:05.955364: step 1511, loss 0.427347, acc 0.875\n",
      "2017-06-30T10:12:06.087883: step 1512, loss 0.465835, acc 0.8125\n",
      "2017-06-30T10:12:06.211703: step 1513, loss 0.384208, acc 0.9375\n",
      "2017-06-30T10:12:06.336963: step 1514, loss 0.392363, acc 0.921875\n",
      "2017-06-30T10:12:06.461698: step 1515, loss 0.455829, acc 0.859375\n",
      "2017-06-30T10:12:06.596456: step 1516, loss 0.378143, acc 0.9375\n",
      "2017-06-30T10:12:06.720482: step 1517, loss 0.365951, acc 0.9375\n",
      "2017-06-30T10:12:06.845078: step 1518, loss 0.358823, acc 0.96875\n",
      "2017-06-30T10:12:06.975390: step 1519, loss 0.417444, acc 0.90625\n",
      "2017-06-30T10:12:07.103399: step 1520, loss 0.428062, acc 0.890625\n",
      "2017-06-30T10:12:07.239021: step 1521, loss 0.40643, acc 0.890625\n",
      "2017-06-30T10:12:07.392755: step 1522, loss 0.399719, acc 0.90625\n",
      "2017-06-30T10:12:07.525902: step 1523, loss 0.392393, acc 0.921875\n",
      "2017-06-30T10:12:07.665447: step 1524, loss 0.468705, acc 0.828125\n",
      "2017-06-30T10:12:07.789125: step 1525, loss 0.377258, acc 0.9375\n",
      "2017-06-30T10:12:07.917751: step 1526, loss 0.375795, acc 0.9375\n",
      "2017-06-30T10:12:08.053272: step 1527, loss 0.389178, acc 0.921875\n",
      "2017-06-30T10:12:08.181760: step 1528, loss 0.405749, acc 0.90625\n",
      "2017-06-30T10:12:08.309913: step 1529, loss 0.459099, acc 0.859375\n",
      "2017-06-30T10:12:08.437405: step 1530, loss 0.37608, acc 0.9375\n",
      "2017-06-30T10:12:08.564300: step 1531, loss 0.480628, acc 0.8125\n",
      "2017-06-30T10:12:08.685991: step 1532, loss 0.37523, acc 0.9375\n",
      "2017-06-30T10:12:08.814306: step 1533, loss 0.448201, acc 0.890625\n",
      "2017-06-30T10:12:08.949859: step 1534, loss 0.45233, acc 0.84375\n",
      "2017-06-30T10:12:09.075676: step 1535, loss 0.386344, acc 0.9375\n",
      "2017-06-30T10:12:09.204081: step 1536, loss 0.473677, acc 0.84375\n",
      "2017-06-30T10:12:09.339539: step 1537, loss 0.379833, acc 0.953125\n",
      "2017-06-30T10:12:09.469620: step 1538, loss 0.417328, acc 0.890625\n",
      "2017-06-30T10:12:09.603181: step 1539, loss 0.376269, acc 0.953125\n",
      "2017-06-30T10:12:09.732970: step 1540, loss 0.42339, acc 0.890625\n",
      "2017-06-30T10:12:09.861205: step 1541, loss 0.370158, acc 0.9375\n",
      "2017-06-30T10:12:09.989883: step 1542, loss 0.425046, acc 0.90625\n",
      "2017-06-30T10:12:10.122514: step 1543, loss 0.422283, acc 0.890625\n",
      "2017-06-30T10:12:10.253183: step 1544, loss 0.419209, acc 0.890625\n",
      "2017-06-30T10:12:10.382089: step 1545, loss 0.374287, acc 0.9375\n",
      "2017-06-30T10:12:10.508535: step 1546, loss 0.420469, acc 0.890625\n",
      "2017-06-30T10:12:10.640724: step 1547, loss 0.496005, acc 0.828125\n",
      "2017-06-30T10:12:10.787505: step 1548, loss 0.399778, acc 0.921875\n",
      "2017-06-30T10:12:10.928620: step 1549, loss 0.404794, acc 0.890625\n",
      "2017-06-30T10:12:11.059465: step 1550, loss 0.429641, acc 0.875\n",
      "2017-06-30T10:12:11.188437: step 1551, loss 0.420514, acc 0.875\n",
      "2017-06-30T10:12:11.314960: step 1552, loss 0.435604, acc 0.859375\n",
      "2017-06-30T10:12:11.446448: step 1553, loss 0.487546, acc 0.828125\n",
      "2017-06-30T10:12:11.573177: step 1554, loss 0.388899, acc 0.9375\n",
      "2017-06-30T10:12:11.704238: step 1555, loss 0.410004, acc 0.875\n",
      "2017-06-30T10:12:11.835786: step 1556, loss 0.391935, acc 0.90625\n",
      "2017-06-30T10:12:11.967414: step 1557, loss 0.440085, acc 0.875\n",
      "2017-06-30T10:12:12.096049: step 1558, loss 0.459913, acc 0.859375\n",
      "2017-06-30T10:12:12.238761: step 1559, loss 0.41737, acc 0.90625\n",
      "2017-06-30T10:12:12.365756: step 1560, loss 0.382607, acc 0.953125\n",
      "2017-06-30T10:12:12.494123: step 1561, loss 0.407418, acc 0.90625\n",
      "2017-06-30T10:12:12.617933: step 1562, loss 0.407922, acc 0.90625\n",
      "2017-06-30T10:12:12.752075: step 1563, loss 0.393984, acc 0.90625\n",
      "2017-06-30T10:12:12.882727: step 1564, loss 0.410558, acc 0.890625\n",
      "2017-06-30T10:12:13.014387: step 1565, loss 0.380422, acc 0.921875\n",
      "2017-06-30T10:12:13.142845: step 1566, loss 0.384981, acc 0.9375\n",
      "2017-06-30T10:12:13.277633: step 1567, loss 0.380361, acc 0.9375\n",
      "2017-06-30T10:12:13.403024: step 1568, loss 0.409265, acc 0.890625\n",
      "2017-06-30T10:12:13.536454: step 1569, loss 0.431111, acc 0.890625\n",
      "2017-06-30T10:12:13.680272: step 1570, loss 0.45499, acc 0.84375\n",
      "2017-06-30T10:12:13.805820: step 1571, loss 0.498412, acc 0.796875\n",
      "2017-06-30T10:12:13.933354: step 1572, loss 0.393524, acc 0.90625\n",
      "2017-06-30T10:12:14.063532: step 1573, loss 0.454795, acc 0.84375\n",
      "2017-06-30T10:12:14.184522: step 1574, loss 0.426732, acc 0.890625\n",
      "2017-06-30T10:12:14.307754: step 1575, loss 0.383928, acc 0.90625\n",
      "2017-06-30T10:12:14.431937: step 1576, loss 0.436841, acc 0.875\n",
      "2017-06-30T10:12:14.551519: step 1577, loss 0.377785, acc 0.9375\n",
      "2017-06-30T10:12:14.669075: step 1578, loss 0.47276, acc 0.828125\n",
      "2017-06-30T10:12:14.798298: step 1579, loss 0.426388, acc 0.890625\n",
      "2017-06-30T10:12:14.920807: step 1580, loss 0.417506, acc 0.90625\n",
      "2017-06-30T10:12:15.042925: step 1581, loss 0.421185, acc 0.890625\n",
      "2017-06-30T10:12:15.167336: step 1582, loss 0.397453, acc 0.890625\n",
      "2017-06-30T10:12:15.287027: step 1583, loss 0.408621, acc 0.890625\n",
      "2017-06-30T10:12:15.454369: step 1584, loss 0.488422, acc 0.828125\n",
      "2017-06-30T10:12:15.598600: step 1585, loss 0.40188, acc 0.90625\n",
      "2017-06-30T10:12:15.748256: step 1586, loss 0.417652, acc 0.90625\n",
      "2017-06-30T10:12:15.875979: step 1587, loss 0.400942, acc 0.921875\n",
      "2017-06-30T10:12:16.013482: step 1588, loss 0.391195, acc 0.921875\n",
      "2017-06-30T10:12:16.148448: step 1589, loss 0.405691, acc 0.90625\n",
      "2017-06-30T10:12:16.294835: step 1590, loss 0.402675, acc 0.90625\n",
      "2017-06-30T10:12:16.437062: step 1591, loss 0.41304, acc 0.890625\n",
      "2017-06-30T10:12:16.573964: step 1592, loss 0.384249, acc 0.921875\n",
      "2017-06-30T10:12:16.712316: step 1593, loss 0.516762, acc 0.78125\n",
      "2017-06-30T10:12:16.847197: step 1594, loss 0.422902, acc 0.875\n",
      "2017-06-30T10:12:16.981498: step 1595, loss 0.423097, acc 0.890625\n",
      "2017-06-30T10:12:17.108557: step 1596, loss 0.46626, acc 0.828125\n",
      "2017-06-30T10:12:17.254666: step 1597, loss 0.350982, acc 0.953125\n",
      "2017-06-30T10:12:17.392951: step 1598, loss 0.386907, acc 0.921875\n",
      "2017-06-30T10:12:17.526389: step 1599, loss 0.435858, acc 0.875\n",
      "2017-06-30T10:12:17.655062: step 1600, loss 0.459767, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:12:17.956115: step 1600, loss 0.576203, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1600\n",
      "\n",
      "2017-06-30T10:12:18.879872: step 1601, loss 0.428552, acc 0.875\n",
      "2017-06-30T10:12:19.011700: step 1602, loss 0.440749, acc 0.859375\n",
      "2017-06-30T10:12:19.139549: step 1603, loss 0.370357, acc 0.953125\n",
      "2017-06-30T10:12:19.278668: step 1604, loss 0.45675, acc 0.859375\n",
      "2017-06-30T10:12:19.409770: step 1605, loss 0.418081, acc 0.890625\n",
      "2017-06-30T10:12:19.544474: step 1606, loss 0.403965, acc 0.890625\n",
      "2017-06-30T10:12:19.685080: step 1607, loss 0.398612, acc 0.890625\n",
      "2017-06-30T10:12:19.811537: step 1608, loss 0.449421, acc 0.859375\n",
      "2017-06-30T10:12:19.936918: step 1609, loss 0.38071, acc 0.921875\n",
      "2017-06-30T10:12:20.073107: step 1610, loss 0.414301, acc 0.890625\n",
      "2017-06-30T10:12:20.211757: step 1611, loss 0.404884, acc 0.921875\n",
      "2017-06-30T10:12:20.341106: step 1612, loss 0.395694, acc 0.921875\n",
      "2017-06-30T10:12:20.475465: step 1613, loss 0.465379, acc 0.84375\n",
      "2017-06-30T10:12:20.610636: step 1614, loss 0.441661, acc 0.890625\n",
      "2017-06-30T10:12:20.751204: step 1615, loss 0.392846, acc 0.921875\n",
      "2017-06-30T10:12:20.893562: step 1616, loss 0.391118, acc 0.953125\n",
      "2017-06-30T10:12:21.021223: step 1617, loss 0.461337, acc 0.828125\n",
      "2017-06-30T10:12:21.157447: step 1618, loss 0.423143, acc 0.890625\n",
      "2017-06-30T10:12:21.295078: step 1619, loss 0.425706, acc 0.84375\n",
      "2017-06-30T10:12:21.430704: step 1620, loss 0.40874, acc 0.875\n",
      "2017-06-30T10:12:21.569163: step 1621, loss 0.383529, acc 0.9375\n",
      "2017-06-30T10:12:21.706759: step 1622, loss 0.446214, acc 0.84375\n",
      "2017-06-30T10:12:21.837643: step 1623, loss 0.421409, acc 0.875\n",
      "2017-06-30T10:12:21.960700: step 1624, loss 0.430064, acc 0.90625\n",
      "2017-06-30T10:12:22.084521: step 1625, loss 0.439545, acc 0.859375\n",
      "2017-06-30T10:12:22.204977: step 1626, loss 0.37975, acc 0.9375\n",
      "2017-06-30T10:12:22.329575: step 1627, loss 0.399726, acc 0.90625\n",
      "2017-06-30T10:12:22.449250: step 1628, loss 0.440578, acc 0.859375\n",
      "2017-06-30T10:12:22.569870: step 1629, loss 0.454993, acc 0.859375\n",
      "2017-06-30T10:12:22.688929: step 1630, loss 0.393262, acc 0.90625\n",
      "2017-06-30T10:12:22.827369: step 1631, loss 0.421192, acc 0.875\n",
      "2017-06-30T10:12:22.963235: step 1632, loss 0.372933, acc 0.953125\n",
      "2017-06-30T10:12:23.085395: step 1633, loss 0.391918, acc 0.921875\n",
      "2017-06-30T10:12:23.214357: step 1634, loss 0.456296, acc 0.828125\n",
      "2017-06-30T10:12:23.333302: step 1635, loss 0.375027, acc 0.953125\n",
      "2017-06-30T10:12:23.451137: step 1636, loss 0.498546, acc 0.8125\n",
      "2017-06-30T10:12:23.571740: step 1637, loss 0.441542, acc 0.875\n",
      "2017-06-30T10:12:23.691660: step 1638, loss 0.395644, acc 0.90625\n",
      "2017-06-30T10:12:23.816900: step 1639, loss 0.42078, acc 0.890625\n",
      "2017-06-30T10:12:23.938851: step 1640, loss 0.394303, acc 0.90625\n",
      "2017-06-30T10:12:24.061081: step 1641, loss 0.403305, acc 0.890625\n",
      "2017-06-30T10:12:24.180448: step 1642, loss 0.410178, acc 0.890625\n",
      "2017-06-30T10:12:24.301011: step 1643, loss 0.451572, acc 0.828125\n",
      "2017-06-30T10:12:24.421693: step 1644, loss 0.412765, acc 0.875\n",
      "2017-06-30T10:12:24.547500: step 1645, loss 0.441545, acc 0.859375\n",
      "2017-06-30T10:12:24.665814: step 1646, loss 0.443208, acc 0.859375\n",
      "2017-06-30T10:12:24.787034: step 1647, loss 0.4402, acc 0.875\n",
      "2017-06-30T10:12:24.911905: step 1648, loss 0.347637, acc 0.984375\n",
      "2017-06-30T10:12:25.033293: step 1649, loss 0.429547, acc 0.875\n",
      "2017-06-30T10:12:25.151608: step 1650, loss 0.462075, acc 0.833333\n",
      "2017-06-30T10:12:25.271972: step 1651, loss 0.362027, acc 0.96875\n",
      "2017-06-30T10:12:25.392490: step 1652, loss 0.414263, acc 0.875\n",
      "2017-06-30T10:12:25.514016: step 1653, loss 0.401375, acc 0.921875\n",
      "2017-06-30T10:12:25.633708: step 1654, loss 0.414741, acc 0.890625\n",
      "2017-06-30T10:12:25.774868: step 1655, loss 0.481739, acc 0.828125\n",
      "2017-06-30T10:12:25.914715: step 1656, loss 0.440088, acc 0.875\n",
      "2017-06-30T10:12:26.046877: step 1657, loss 0.393461, acc 0.921875\n",
      "2017-06-30T10:12:26.180083: step 1658, loss 0.388321, acc 0.921875\n",
      "2017-06-30T10:12:26.319278: step 1659, loss 0.364553, acc 0.953125\n",
      "2017-06-30T10:12:26.448416: step 1660, loss 0.39397, acc 0.921875\n",
      "2017-06-30T10:12:26.572468: step 1661, loss 0.405567, acc 0.921875\n",
      "2017-06-30T10:12:26.691023: step 1662, loss 0.456482, acc 0.84375\n",
      "2017-06-30T10:12:26.813779: step 1663, loss 0.418529, acc 0.890625\n",
      "2017-06-30T10:12:26.936055: step 1664, loss 0.416941, acc 0.90625\n",
      "2017-06-30T10:12:27.063567: step 1665, loss 0.416792, acc 0.90625\n",
      "2017-06-30T10:12:27.195646: step 1666, loss 0.433124, acc 0.875\n",
      "2017-06-30T10:12:27.330507: step 1667, loss 0.395344, acc 0.921875\n",
      "2017-06-30T10:12:27.460783: step 1668, loss 0.439523, acc 0.859375\n",
      "2017-06-30T10:12:27.587768: step 1669, loss 0.380353, acc 0.921875\n",
      "2017-06-30T10:12:27.708058: step 1670, loss 0.415162, acc 0.90625\n",
      "2017-06-30T10:12:27.833208: step 1671, loss 0.392756, acc 0.90625\n",
      "2017-06-30T10:12:27.950762: step 1672, loss 0.392746, acc 0.9375\n",
      "2017-06-30T10:12:28.091672: step 1673, loss 0.404925, acc 0.90625\n",
      "2017-06-30T10:12:28.213639: step 1674, loss 0.375653, acc 0.9375\n",
      "2017-06-30T10:12:28.331046: step 1675, loss 0.410481, acc 0.90625\n",
      "2017-06-30T10:12:28.449473: step 1676, loss 0.386545, acc 0.921875\n",
      "2017-06-30T10:12:28.569347: step 1677, loss 0.375539, acc 0.953125\n",
      "2017-06-30T10:12:28.695965: step 1678, loss 0.385561, acc 0.9375\n",
      "2017-06-30T10:12:28.815029: step 1679, loss 0.444653, acc 0.828125\n",
      "2017-06-30T10:12:28.930992: step 1680, loss 0.384831, acc 0.9375\n",
      "2017-06-30T10:12:29.049621: step 1681, loss 0.385371, acc 0.921875\n",
      "2017-06-30T10:12:29.172534: step 1682, loss 0.447886, acc 0.859375\n",
      "2017-06-30T10:12:29.296638: step 1683, loss 0.418262, acc 0.90625\n",
      "2017-06-30T10:12:29.417638: step 1684, loss 0.392688, acc 0.921875\n",
      "2017-06-30T10:12:29.542648: step 1685, loss 0.398194, acc 0.90625\n",
      "2017-06-30T10:12:29.667731: step 1686, loss 0.382495, acc 0.9375\n",
      "2017-06-30T10:12:29.793865: step 1687, loss 0.380662, acc 0.9375\n",
      "2017-06-30T10:12:29.914067: step 1688, loss 0.361537, acc 0.984375\n",
      "2017-06-30T10:12:30.035707: step 1689, loss 0.34389, acc 0.984375\n",
      "2017-06-30T10:12:30.159448: step 1690, loss 0.378269, acc 0.921875\n",
      "2017-06-30T10:12:30.279968: step 1691, loss 0.380569, acc 0.9375\n",
      "2017-06-30T10:12:30.396969: step 1692, loss 0.410076, acc 0.890625\n",
      "2017-06-30T10:12:30.513728: step 1693, loss 0.389631, acc 0.921875\n",
      "2017-06-30T10:12:30.635203: step 1694, loss 0.393895, acc 0.890625\n",
      "2017-06-30T10:12:30.763808: step 1695, loss 0.349033, acc 0.96875\n",
      "2017-06-30T10:12:30.890512: step 1696, loss 0.457143, acc 0.84375\n",
      "2017-06-30T10:12:31.017599: step 1697, loss 0.381614, acc 0.921875\n",
      "2017-06-30T10:12:31.142288: step 1698, loss 0.36266, acc 0.9375\n",
      "2017-06-30T10:12:31.266509: step 1699, loss 0.422599, acc 0.875\n",
      "2017-06-30T10:12:31.392835: step 1700, loss 0.451978, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:12:31.680909: step 1700, loss 0.57339, acc 0.726079\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1700\n",
      "\n",
      "2017-06-30T10:12:32.383400: step 1701, loss 0.407238, acc 0.90625\n",
      "2017-06-30T10:12:32.501263: step 1702, loss 0.419921, acc 0.890625\n",
      "2017-06-30T10:12:32.620777: step 1703, loss 0.415242, acc 0.890625\n",
      "2017-06-30T10:12:32.740700: step 1704, loss 0.425858, acc 0.890625\n",
      "2017-06-30T10:12:32.863564: step 1705, loss 0.402446, acc 0.90625\n",
      "2017-06-30T10:12:32.984355: step 1706, loss 0.383923, acc 0.9375\n",
      "2017-06-30T10:12:33.107802: step 1707, loss 0.464878, acc 0.828125\n",
      "2017-06-30T10:12:33.238001: step 1708, loss 0.356523, acc 0.953125\n",
      "2017-06-30T10:12:33.360147: step 1709, loss 0.39326, acc 0.921875\n",
      "2017-06-30T10:12:33.484045: step 1710, loss 0.413963, acc 0.90625\n",
      "2017-06-30T10:12:33.608256: step 1711, loss 0.50034, acc 0.796875\n",
      "2017-06-30T10:12:33.731245: step 1712, loss 0.392806, acc 0.921875\n",
      "2017-06-30T10:12:33.855718: step 1713, loss 0.435692, acc 0.859375\n",
      "2017-06-30T10:12:33.977501: step 1714, loss 0.466206, acc 0.84375\n",
      "2017-06-30T10:12:34.103822: step 1715, loss 0.354836, acc 0.96875\n",
      "2017-06-30T10:12:34.222075: step 1716, loss 0.41361, acc 0.90625\n",
      "2017-06-30T10:12:34.343862: step 1717, loss 0.432112, acc 0.875\n",
      "2017-06-30T10:12:34.475776: step 1718, loss 0.387094, acc 0.9375\n",
      "2017-06-30T10:12:34.613308: step 1719, loss 0.396659, acc 0.9375\n",
      "2017-06-30T10:12:34.752914: step 1720, loss 0.406053, acc 0.890625\n",
      "2017-06-30T10:12:34.871337: step 1721, loss 0.391841, acc 0.921875\n",
      "2017-06-30T10:12:34.990474: step 1722, loss 0.400733, acc 0.90625\n",
      "2017-06-30T10:12:35.125045: step 1723, loss 0.401239, acc 0.90625\n",
      "2017-06-30T10:12:35.259451: step 1724, loss 0.44358, acc 0.875\n",
      "2017-06-30T10:12:35.393432: step 1725, loss 0.404246, acc 0.890625\n",
      "2017-06-30T10:12:35.520661: step 1726, loss 0.415021, acc 0.90625\n",
      "2017-06-30T10:12:35.640447: step 1727, loss 0.389119, acc 0.921875\n",
      "2017-06-30T10:12:35.764244: step 1728, loss 0.435522, acc 0.875\n",
      "2017-06-30T10:12:35.893521: step 1729, loss 0.409053, acc 0.890625\n",
      "2017-06-30T10:12:36.028960: step 1730, loss 0.350571, acc 0.96875\n",
      "2017-06-30T10:12:36.154008: step 1731, loss 0.360855, acc 0.953125\n",
      "2017-06-30T10:12:36.271491: step 1732, loss 0.362247, acc 0.96875\n",
      "2017-06-30T10:12:36.396713: step 1733, loss 0.398173, acc 0.90625\n",
      "2017-06-30T10:12:36.529062: step 1734, loss 0.413388, acc 0.890625\n",
      "2017-06-30T10:12:36.667372: step 1735, loss 0.451921, acc 0.828125\n",
      "2017-06-30T10:12:36.794675: step 1736, loss 0.383115, acc 0.953125\n",
      "2017-06-30T10:12:36.914401: step 1737, loss 0.451259, acc 0.875\n",
      "2017-06-30T10:12:37.046045: step 1738, loss 0.387089, acc 0.9375\n",
      "2017-06-30T10:12:37.178451: step 1739, loss 0.34711, acc 0.96875\n",
      "2017-06-30T10:12:37.297094: step 1740, loss 0.369508, acc 0.953125\n",
      "2017-06-30T10:12:37.432881: step 1741, loss 0.424362, acc 0.890625\n",
      "2017-06-30T10:12:37.555291: step 1742, loss 0.365052, acc 0.9375\n",
      "2017-06-30T10:12:37.680784: step 1743, loss 0.380329, acc 0.9375\n",
      "2017-06-30T10:12:37.799727: step 1744, loss 0.411356, acc 0.890625\n",
      "2017-06-30T10:12:37.915151: step 1745, loss 0.411572, acc 0.90625\n",
      "2017-06-30T10:12:38.037986: step 1746, loss 0.41383, acc 0.890625\n",
      "2017-06-30T10:12:38.161867: step 1747, loss 0.378738, acc 0.953125\n",
      "2017-06-30T10:12:38.282452: step 1748, loss 0.395145, acc 0.921875\n",
      "2017-06-30T10:12:38.405242: step 1749, loss 0.458699, acc 0.84375\n",
      "2017-06-30T10:12:38.536142: step 1750, loss 0.406525, acc 0.90625\n",
      "2017-06-30T10:12:38.655391: step 1751, loss 0.40401, acc 0.921875\n",
      "2017-06-30T10:12:38.776323: step 1752, loss 0.396104, acc 0.921875\n",
      "2017-06-30T10:12:38.899843: step 1753, loss 0.398714, acc 0.90625\n",
      "2017-06-30T10:12:39.027230: step 1754, loss 0.359198, acc 0.96875\n",
      "2017-06-30T10:12:39.148360: step 1755, loss 0.39539, acc 0.9375\n",
      "2017-06-30T10:12:39.268398: step 1756, loss 0.398019, acc 0.921875\n",
      "2017-06-30T10:12:39.395228: step 1757, loss 0.388297, acc 0.953125\n",
      "2017-06-30T10:12:39.518544: step 1758, loss 0.386845, acc 0.921875\n",
      "2017-06-30T10:12:39.645760: step 1759, loss 0.398257, acc 0.90625\n",
      "2017-06-30T10:12:39.767373: step 1760, loss 0.40096, acc 0.921875\n",
      "2017-06-30T10:12:39.890605: step 1761, loss 0.38213, acc 0.9375\n",
      "2017-06-30T10:12:40.012341: step 1762, loss 0.39152, acc 0.921875\n",
      "2017-06-30T10:12:40.137894: step 1763, loss 0.418237, acc 0.890625\n",
      "2017-06-30T10:12:40.275757: step 1764, loss 0.453141, acc 0.84375\n",
      "2017-06-30T10:12:40.401152: step 1765, loss 0.372923, acc 0.953125\n",
      "2017-06-30T10:12:40.522176: step 1766, loss 0.418892, acc 0.875\n",
      "2017-06-30T10:12:40.648453: step 1767, loss 0.39355, acc 0.90625\n",
      "2017-06-30T10:12:40.770465: step 1768, loss 0.383253, acc 0.921875\n",
      "2017-06-30T10:12:40.896524: step 1769, loss 0.380591, acc 0.9375\n",
      "2017-06-30T10:12:41.027323: step 1770, loss 0.345481, acc 0.984375\n",
      "2017-06-30T10:12:41.149183: step 1771, loss 0.40679, acc 0.890625\n",
      "2017-06-30T10:12:41.270446: step 1772, loss 0.39602, acc 0.90625\n",
      "2017-06-30T10:12:41.408236: step 1773, loss 0.395802, acc 0.90625\n",
      "2017-06-30T10:12:41.537132: step 1774, loss 0.461394, acc 0.828125\n",
      "2017-06-30T10:12:41.654913: step 1775, loss 0.364038, acc 0.9375\n",
      "2017-06-30T10:12:41.782992: step 1776, loss 0.441629, acc 0.859375\n",
      "2017-06-30T10:12:41.903770: step 1777, loss 0.385138, acc 0.9375\n",
      "2017-06-30T10:12:42.020371: step 1778, loss 0.386406, acc 0.921875\n",
      "2017-06-30T10:12:42.138583: step 1779, loss 0.358796, acc 0.953125\n",
      "2017-06-30T10:12:42.257059: step 1780, loss 0.408965, acc 0.890625\n",
      "2017-06-30T10:12:42.385651: step 1781, loss 0.380892, acc 0.9375\n",
      "2017-06-30T10:12:42.510308: step 1782, loss 0.427618, acc 0.84375\n",
      "2017-06-30T10:12:42.640195: step 1783, loss 0.417047, acc 0.890625\n",
      "2017-06-30T10:12:42.768108: step 1784, loss 0.393468, acc 0.921875\n",
      "2017-06-30T10:12:42.888472: step 1785, loss 0.360387, acc 0.96875\n",
      "2017-06-30T10:12:43.012797: step 1786, loss 0.334879, acc 0.984375\n",
      "2017-06-30T10:12:43.140674: step 1787, loss 0.441962, acc 0.890625\n",
      "2017-06-30T10:12:43.258317: step 1788, loss 0.429774, acc 0.875\n",
      "2017-06-30T10:12:43.376733: step 1789, loss 0.485179, acc 0.796875\n",
      "2017-06-30T10:12:43.503208: step 1790, loss 0.370187, acc 0.953125\n",
      "2017-06-30T10:12:43.626183: step 1791, loss 0.396081, acc 0.90625\n",
      "2017-06-30T10:12:43.761929: step 1792, loss 0.364852, acc 0.953125\n",
      "2017-06-30T10:12:43.889576: step 1793, loss 0.406646, acc 0.921875\n",
      "2017-06-30T10:12:44.018129: step 1794, loss 0.397459, acc 0.921875\n",
      "2017-06-30T10:12:44.141187: step 1795, loss 0.404318, acc 0.921875\n",
      "2017-06-30T10:12:44.257945: step 1796, loss 0.409332, acc 0.890625\n",
      "2017-06-30T10:12:44.382287: step 1797, loss 0.355725, acc 0.96875\n",
      "2017-06-30T10:12:44.506088: step 1798, loss 0.44494, acc 0.890625\n",
      "2017-06-30T10:12:44.626819: step 1799, loss 0.421205, acc 0.890625\n",
      "2017-06-30T10:12:44.738499: step 1800, loss 0.43629, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:12:45.025954: step 1800, loss 0.566562, acc 0.736398\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1800\n",
      "\n",
      "2017-06-30T10:12:45.821258: step 1801, loss 0.413157, acc 0.84375\n",
      "2017-06-30T10:12:45.952034: step 1802, loss 0.324298, acc 1\n",
      "2017-06-30T10:12:46.088304: step 1803, loss 0.39478, acc 0.921875\n",
      "2017-06-30T10:12:46.214240: step 1804, loss 0.393682, acc 0.9375\n",
      "2017-06-30T10:12:46.328609: step 1805, loss 0.395978, acc 0.90625\n",
      "2017-06-30T10:12:46.439099: step 1806, loss 0.412124, acc 0.90625\n",
      "2017-06-30T10:12:46.561098: step 1807, loss 0.359159, acc 0.96875\n",
      "2017-06-30T10:12:46.693050: step 1808, loss 0.46323, acc 0.84375\n",
      "2017-06-30T10:12:46.843340: step 1809, loss 0.389148, acc 0.90625\n",
      "2017-06-30T10:12:46.966966: step 1810, loss 0.417545, acc 0.890625\n",
      "2017-06-30T10:12:47.084801: step 1811, loss 0.373774, acc 0.953125\n",
      "2017-06-30T10:12:47.200639: step 1812, loss 0.367621, acc 0.953125\n",
      "2017-06-30T10:12:47.333577: step 1813, loss 0.367162, acc 0.953125\n",
      "2017-06-30T10:12:47.462453: step 1814, loss 0.365926, acc 0.9375\n",
      "2017-06-30T10:12:47.582902: step 1815, loss 0.385857, acc 0.90625\n",
      "2017-06-30T10:12:47.696456: step 1816, loss 0.38491, acc 0.921875\n",
      "2017-06-30T10:12:47.814161: step 1817, loss 0.390387, acc 0.921875\n",
      "2017-06-30T10:12:47.933773: step 1818, loss 0.37306, acc 0.9375\n",
      "2017-06-30T10:12:48.052525: step 1819, loss 0.385493, acc 0.9375\n",
      "2017-06-30T10:12:48.174967: step 1820, loss 0.428789, acc 0.890625\n",
      "2017-06-30T10:12:48.297855: step 1821, loss 0.354755, acc 0.953125\n",
      "2017-06-30T10:12:48.417085: step 1822, loss 0.353417, acc 0.96875\n",
      "2017-06-30T10:12:48.535131: step 1823, loss 0.387118, acc 0.921875\n",
      "2017-06-30T10:12:48.654525: step 1824, loss 0.358704, acc 0.96875\n",
      "2017-06-30T10:12:48.772914: step 1825, loss 0.422399, acc 0.875\n",
      "2017-06-30T10:12:48.888273: step 1826, loss 0.361066, acc 0.953125\n",
      "2017-06-30T10:12:49.012143: step 1827, loss 0.387912, acc 0.921875\n",
      "2017-06-30T10:12:49.142586: step 1828, loss 0.372403, acc 0.9375\n",
      "2017-06-30T10:12:49.278738: step 1829, loss 0.393881, acc 0.921875\n",
      "2017-06-30T10:12:49.404198: step 1830, loss 0.372433, acc 0.9375\n",
      "2017-06-30T10:12:49.518952: step 1831, loss 0.397545, acc 0.90625\n",
      "2017-06-30T10:12:49.633202: step 1832, loss 0.414162, acc 0.890625\n",
      "2017-06-30T10:12:49.758642: step 1833, loss 0.40925, acc 0.90625\n",
      "2017-06-30T10:12:49.897244: step 1834, loss 0.436062, acc 0.875\n",
      "2017-06-30T10:12:50.027092: step 1835, loss 0.371839, acc 0.953125\n",
      "2017-06-30T10:12:50.153324: step 1836, loss 0.401142, acc 0.90625\n",
      "2017-06-30T10:12:50.269343: step 1837, loss 0.400985, acc 0.921875\n",
      "2017-06-30T10:12:50.390693: step 1838, loss 0.44757, acc 0.859375\n",
      "2017-06-30T10:12:50.527586: step 1839, loss 0.38809, acc 0.9375\n",
      "2017-06-30T10:12:50.656690: step 1840, loss 0.36744, acc 0.953125\n",
      "2017-06-30T10:12:50.790681: step 1841, loss 0.403391, acc 0.90625\n",
      "2017-06-30T10:12:50.919388: step 1842, loss 0.386537, acc 0.9375\n",
      "2017-06-30T10:12:51.045386: step 1843, loss 0.366351, acc 0.9375\n",
      "2017-06-30T10:12:51.177642: step 1844, loss 0.383404, acc 0.921875\n",
      "2017-06-30T10:12:51.309346: step 1845, loss 0.392679, acc 0.921875\n",
      "2017-06-30T10:12:51.440632: step 1846, loss 0.363504, acc 0.953125\n",
      "2017-06-30T10:12:51.571948: step 1847, loss 0.380398, acc 0.9375\n",
      "2017-06-30T10:12:51.710310: step 1848, loss 0.395475, acc 0.90625\n",
      "2017-06-30T10:12:51.838917: step 1849, loss 0.40095, acc 0.90625\n",
      "2017-06-30T10:12:51.972525: step 1850, loss 0.390455, acc 0.921875\n",
      "2017-06-30T10:12:52.102362: step 1851, loss 0.445334, acc 0.859375\n",
      "2017-06-30T10:12:52.244632: step 1852, loss 0.403836, acc 0.90625\n",
      "2017-06-30T10:12:52.375719: step 1853, loss 0.389537, acc 0.921875\n",
      "2017-06-30T10:12:52.496883: step 1854, loss 0.378036, acc 0.9375\n",
      "2017-06-30T10:12:52.619251: step 1855, loss 0.416661, acc 0.859375\n",
      "2017-06-30T10:12:52.748342: step 1856, loss 0.374762, acc 0.9375\n",
      "2017-06-30T10:12:52.869271: step 1857, loss 0.402035, acc 0.921875\n",
      "2017-06-30T10:12:52.993150: step 1858, loss 0.347171, acc 0.96875\n",
      "2017-06-30T10:12:53.118377: step 1859, loss 0.400344, acc 0.90625\n",
      "2017-06-30T10:12:53.238741: step 1860, loss 0.362372, acc 0.953125\n",
      "2017-06-30T10:12:53.362603: step 1861, loss 0.387723, acc 0.921875\n",
      "2017-06-30T10:12:53.482229: step 1862, loss 0.39717, acc 0.921875\n",
      "2017-06-30T10:12:53.606817: step 1863, loss 0.399011, acc 0.890625\n",
      "2017-06-30T10:12:53.742510: step 1864, loss 0.414083, acc 0.90625\n",
      "2017-06-30T10:12:53.878577: step 1865, loss 0.395084, acc 0.90625\n",
      "2017-06-30T10:12:54.005310: step 1866, loss 0.386407, acc 0.921875\n",
      "2017-06-30T10:12:54.123536: step 1867, loss 0.374634, acc 0.9375\n",
      "2017-06-30T10:12:54.242478: step 1868, loss 0.385307, acc 0.9375\n",
      "2017-06-30T10:12:54.365825: step 1869, loss 0.361722, acc 0.953125\n",
      "2017-06-30T10:12:54.505253: step 1870, loss 0.386849, acc 0.921875\n",
      "2017-06-30T10:12:54.641011: step 1871, loss 0.3745, acc 0.9375\n",
      "2017-06-30T10:12:54.774182: step 1872, loss 0.392125, acc 0.9375\n",
      "2017-06-30T10:12:54.891600: step 1873, loss 0.373868, acc 0.9375\n",
      "2017-06-30T10:12:55.013482: step 1874, loss 0.397739, acc 0.90625\n",
      "2017-06-30T10:12:55.150939: step 1875, loss 0.393519, acc 0.9375\n",
      "2017-06-30T10:12:55.282273: step 1876, loss 0.410307, acc 0.890625\n",
      "2017-06-30T10:12:55.411021: step 1877, loss 0.440885, acc 0.875\n",
      "2017-06-30T10:12:55.544285: step 1878, loss 0.448134, acc 0.84375\n",
      "2017-06-30T10:12:55.664563: step 1879, loss 0.384862, acc 0.921875\n",
      "2017-06-30T10:12:55.782408: step 1880, loss 0.394344, acc 0.90625\n",
      "2017-06-30T10:12:55.906383: step 1881, loss 0.354024, acc 0.96875\n",
      "2017-06-30T10:12:56.047737: step 1882, loss 0.388802, acc 0.9375\n",
      "2017-06-30T10:12:56.173490: step 1883, loss 0.3626, acc 0.953125\n",
      "2017-06-30T10:12:56.298691: step 1884, loss 0.387347, acc 0.9375\n",
      "2017-06-30T10:12:56.418694: step 1885, loss 0.418672, acc 0.890625\n",
      "2017-06-30T10:12:56.539661: step 1886, loss 0.386724, acc 0.9375\n",
      "2017-06-30T10:12:56.677153: step 1887, loss 0.391183, acc 0.921875\n",
      "2017-06-30T10:12:56.804451: step 1888, loss 0.385085, acc 0.9375\n",
      "2017-06-30T10:12:56.941530: step 1889, loss 0.358357, acc 0.96875\n",
      "2017-06-30T10:12:57.063464: step 1890, loss 0.358333, acc 0.953125\n",
      "2017-06-30T10:12:57.186392: step 1891, loss 0.404021, acc 0.90625\n",
      "2017-06-30T10:12:57.308284: step 1892, loss 0.3965, acc 0.921875\n",
      "2017-06-30T10:12:57.427656: step 1893, loss 0.35865, acc 0.953125\n",
      "2017-06-30T10:12:57.543357: step 1894, loss 0.417169, acc 0.90625\n",
      "2017-06-30T10:12:57.665232: step 1895, loss 0.362159, acc 0.9375\n",
      "2017-06-30T10:12:57.792855: step 1896, loss 0.369114, acc 0.9375\n",
      "2017-06-30T10:12:57.926166: step 1897, loss 0.379323, acc 0.921875\n",
      "2017-06-30T10:12:58.046131: step 1898, loss 0.367239, acc 0.953125\n",
      "2017-06-30T10:12:58.180660: step 1899, loss 0.396822, acc 0.90625\n",
      "2017-06-30T10:12:58.296939: step 1900, loss 0.375431, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:12:58.592628: step 1900, loss 0.567136, acc 0.736398\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-1900\n",
      "\n",
      "2017-06-30T10:12:59.450777: step 1901, loss 0.370572, acc 0.9375\n",
      "2017-06-30T10:12:59.573559: step 1902, loss 0.353599, acc 0.96875\n",
      "2017-06-30T10:12:59.703247: step 1903, loss 0.368059, acc 0.953125\n",
      "2017-06-30T10:12:59.829175: step 1904, loss 0.381031, acc 0.9375\n",
      "2017-06-30T10:12:59.948767: step 1905, loss 0.370363, acc 0.96875\n",
      "2017-06-30T10:13:00.073403: step 1906, loss 0.380531, acc 0.9375\n",
      "2017-06-30T10:13:00.221789: step 1907, loss 0.419086, acc 0.875\n",
      "2017-06-30T10:13:00.365349: step 1908, loss 0.417181, acc 0.875\n",
      "2017-06-30T10:13:00.493960: step 1909, loss 0.381921, acc 0.9375\n",
      "2017-06-30T10:13:00.628328: step 1910, loss 0.389764, acc 0.921875\n",
      "2017-06-30T10:13:00.773017: step 1911, loss 0.369301, acc 0.9375\n",
      "2017-06-30T10:13:00.905124: step 1912, loss 0.409321, acc 0.890625\n",
      "2017-06-30T10:13:01.022398: step 1913, loss 0.3824, acc 0.921875\n",
      "2017-06-30T10:13:01.142667: step 1914, loss 0.347761, acc 0.96875\n",
      "2017-06-30T10:13:01.278657: step 1915, loss 0.404545, acc 0.90625\n",
      "2017-06-30T10:13:01.451148: step 1916, loss 0.374036, acc 0.9375\n",
      "2017-06-30T10:13:01.600097: step 1917, loss 0.38138, acc 0.9375\n",
      "2017-06-30T10:13:01.722427: step 1918, loss 0.382237, acc 0.953125\n",
      "2017-06-30T10:13:01.850858: step 1919, loss 0.38958, acc 0.921875\n",
      "2017-06-30T10:13:01.980689: step 1920, loss 0.369897, acc 0.9375\n",
      "2017-06-30T10:13:02.098328: step 1921, loss 0.401215, acc 0.90625\n",
      "2017-06-30T10:13:02.218726: step 1922, loss 0.405175, acc 0.90625\n",
      "2017-06-30T10:13:02.347652: step 1923, loss 0.411955, acc 0.875\n",
      "2017-06-30T10:13:02.482075: step 1924, loss 0.359988, acc 0.953125\n",
      "2017-06-30T10:13:02.611905: step 1925, loss 0.433349, acc 0.890625\n",
      "2017-06-30T10:13:02.740561: step 1926, loss 0.431593, acc 0.875\n",
      "2017-06-30T10:13:02.875584: step 1927, loss 0.36421, acc 0.953125\n",
      "2017-06-30T10:13:03.007104: step 1928, loss 0.411934, acc 0.890625\n",
      "2017-06-30T10:13:03.138012: step 1929, loss 0.340601, acc 0.96875\n",
      "2017-06-30T10:13:03.275414: step 1930, loss 0.42779, acc 0.875\n",
      "2017-06-30T10:13:03.403063: step 1931, loss 0.395732, acc 0.90625\n",
      "2017-06-30T10:13:03.520342: step 1932, loss 0.395522, acc 0.90625\n",
      "2017-06-30T10:13:03.642169: step 1933, loss 0.407832, acc 0.90625\n",
      "2017-06-30T10:13:03.766979: step 1934, loss 0.370704, acc 0.9375\n",
      "2017-06-30T10:13:03.900079: step 1935, loss 0.378699, acc 0.921875\n",
      "2017-06-30T10:13:04.069743: step 1936, loss 0.405276, acc 0.890625\n",
      "2017-06-30T10:13:04.226851: step 1937, loss 0.392135, acc 0.921875\n",
      "2017-06-30T10:13:04.357139: step 1938, loss 0.367209, acc 0.953125\n",
      "2017-06-30T10:13:04.483417: step 1939, loss 0.361049, acc 0.953125\n",
      "2017-06-30T10:13:04.610129: step 1940, loss 0.356948, acc 0.96875\n",
      "2017-06-30T10:13:04.735426: step 1941, loss 0.390835, acc 0.921875\n",
      "2017-06-30T10:13:04.867882: step 1942, loss 0.364943, acc 0.953125\n",
      "2017-06-30T10:13:04.994409: step 1943, loss 0.409374, acc 0.890625\n",
      "2017-06-30T10:13:05.118711: step 1944, loss 0.393408, acc 0.90625\n",
      "2017-06-30T10:13:05.246401: step 1945, loss 0.331407, acc 1\n",
      "2017-06-30T10:13:05.369977: step 1946, loss 0.433289, acc 0.875\n",
      "2017-06-30T10:13:05.504493: step 1947, loss 0.415722, acc 0.890625\n",
      "2017-06-30T10:13:05.644090: step 1948, loss 0.374385, acc 0.9375\n",
      "2017-06-30T10:13:05.777222: step 1949, loss 0.417067, acc 0.90625\n",
      "2017-06-30T10:13:05.905463: step 1950, loss 0.406631, acc 0.916667\n",
      "2017-06-30T10:13:06.039928: step 1951, loss 0.395256, acc 0.90625\n",
      "2017-06-30T10:13:06.161307: step 1952, loss 0.394557, acc 0.90625\n",
      "2017-06-30T10:13:06.298200: step 1953, loss 0.36256, acc 0.953125\n",
      "2017-06-30T10:13:06.433482: step 1954, loss 0.36518, acc 0.953125\n",
      "2017-06-30T10:13:06.565466: step 1955, loss 0.39498, acc 0.90625\n",
      "2017-06-30T10:13:06.699491: step 1956, loss 0.34508, acc 0.984375\n",
      "2017-06-30T10:13:06.829139: step 1957, loss 0.346526, acc 0.96875\n",
      "2017-06-30T10:13:06.964802: step 1958, loss 0.375245, acc 0.9375\n",
      "2017-06-30T10:13:07.096059: step 1959, loss 0.381556, acc 0.953125\n",
      "2017-06-30T10:13:07.230173: step 1960, loss 0.378049, acc 0.921875\n",
      "2017-06-30T10:13:07.367761: step 1961, loss 0.391981, acc 0.921875\n",
      "2017-06-30T10:13:07.507278: step 1962, loss 0.37625, acc 0.953125\n",
      "2017-06-30T10:13:07.641983: step 1963, loss 0.368224, acc 0.953125\n",
      "2017-06-30T10:13:07.771892: step 1964, loss 0.357256, acc 0.96875\n",
      "2017-06-30T10:13:07.909726: step 1965, loss 0.424799, acc 0.875\n",
      "2017-06-30T10:13:08.043440: step 1966, loss 0.373571, acc 0.921875\n",
      "2017-06-30T10:13:08.174747: step 1967, loss 0.346676, acc 0.96875\n",
      "2017-06-30T10:13:08.298240: step 1968, loss 0.374762, acc 0.9375\n",
      "2017-06-30T10:13:08.421074: step 1969, loss 0.382103, acc 0.9375\n",
      "2017-06-30T10:13:08.539486: step 1970, loss 0.418261, acc 0.890625\n",
      "2017-06-30T10:13:08.658367: step 1971, loss 0.375816, acc 0.9375\n",
      "2017-06-30T10:13:08.782208: step 1972, loss 0.360021, acc 0.953125\n",
      "2017-06-30T10:13:08.914879: step 1973, loss 0.419204, acc 0.890625\n",
      "2017-06-30T10:13:09.048054: step 1974, loss 0.368163, acc 0.9375\n",
      "2017-06-30T10:13:09.172283: step 1975, loss 0.406965, acc 0.921875\n",
      "2017-06-30T10:13:09.289885: step 1976, loss 0.382087, acc 0.9375\n",
      "2017-06-30T10:13:09.415551: step 1977, loss 0.401685, acc 0.90625\n",
      "2017-06-30T10:13:09.553222: step 1978, loss 0.380463, acc 0.9375\n",
      "2017-06-30T10:13:09.684855: step 1979, loss 0.357864, acc 0.96875\n",
      "2017-06-30T10:13:09.809967: step 1980, loss 0.362235, acc 0.953125\n",
      "2017-06-30T10:13:09.931392: step 1981, loss 0.379252, acc 0.921875\n",
      "2017-06-30T10:13:10.052779: step 1982, loss 0.372029, acc 0.921875\n",
      "2017-06-30T10:13:10.199958: step 1983, loss 0.391595, acc 0.921875\n",
      "2017-06-30T10:13:10.330776: step 1984, loss 0.359164, acc 0.953125\n",
      "2017-06-30T10:13:10.455429: step 1985, loss 0.372441, acc 0.9375\n",
      "2017-06-30T10:13:10.588825: step 1986, loss 0.403707, acc 0.90625\n",
      "2017-06-30T10:13:10.727918: step 1987, loss 0.369796, acc 0.953125\n",
      "2017-06-30T10:13:10.848303: step 1988, loss 0.34816, acc 0.96875\n",
      "2017-06-30T10:13:10.971263: step 1989, loss 0.379178, acc 0.9375\n",
      "2017-06-30T10:13:11.097072: step 1990, loss 0.350441, acc 0.953125\n",
      "2017-06-30T10:13:11.228602: step 1991, loss 0.433388, acc 0.890625\n",
      "2017-06-30T10:13:11.366978: step 1992, loss 0.367057, acc 0.953125\n",
      "2017-06-30T10:13:11.487339: step 1993, loss 0.357054, acc 0.953125\n",
      "2017-06-30T10:13:11.605953: step 1994, loss 0.386355, acc 0.921875\n",
      "2017-06-30T10:13:11.727496: step 1995, loss 0.405391, acc 0.90625\n",
      "2017-06-30T10:13:11.850514: step 1996, loss 0.334315, acc 0.96875\n",
      "2017-06-30T10:13:11.981966: step 1997, loss 0.364586, acc 0.9375\n",
      "2017-06-30T10:13:12.117162: step 1998, loss 0.366098, acc 0.96875\n",
      "2017-06-30T10:13:12.247003: step 1999, loss 0.388823, acc 0.921875\n",
      "2017-06-30T10:13:12.374953: step 2000, loss 0.457514, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:13:12.679292: step 2000, loss 0.567322, acc 0.730769\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2000\n",
      "\n",
      "2017-06-30T10:13:13.360762: step 2001, loss 0.383469, acc 0.90625\n",
      "2017-06-30T10:13:13.478229: step 2002, loss 0.393838, acc 0.90625\n",
      "2017-06-30T10:13:13.595789: step 2003, loss 0.365168, acc 0.953125\n",
      "2017-06-30T10:13:13.718510: step 2004, loss 0.332667, acc 0.984375\n",
      "2017-06-30T10:13:13.842175: step 2005, loss 0.365026, acc 0.9375\n",
      "2017-06-30T10:13:13.965132: step 2006, loss 0.380008, acc 0.921875\n",
      "2017-06-30T10:13:14.086589: step 2007, loss 0.391745, acc 0.921875\n",
      "2017-06-30T10:13:14.209113: step 2008, loss 0.327189, acc 0.984375\n",
      "2017-06-30T10:13:14.332846: step 2009, loss 0.399562, acc 0.921875\n",
      "2017-06-30T10:13:14.450086: step 2010, loss 0.32413, acc 0.984375\n",
      "2017-06-30T10:13:14.570299: step 2011, loss 0.372525, acc 0.921875\n",
      "2017-06-30T10:13:14.691014: step 2012, loss 0.420713, acc 0.890625\n",
      "2017-06-30T10:13:14.814583: step 2013, loss 0.352148, acc 0.96875\n",
      "2017-06-30T10:13:14.937024: step 2014, loss 0.369353, acc 0.9375\n",
      "2017-06-30T10:13:15.059023: step 2015, loss 0.409704, acc 0.921875\n",
      "2017-06-30T10:13:15.177863: step 2016, loss 0.357789, acc 0.953125\n",
      "2017-06-30T10:13:15.296158: step 2017, loss 0.412437, acc 0.875\n",
      "2017-06-30T10:13:15.416307: step 2018, loss 0.318016, acc 1\n",
      "2017-06-30T10:13:15.542502: step 2019, loss 0.390969, acc 0.921875\n",
      "2017-06-30T10:13:15.666458: step 2020, loss 0.349514, acc 0.96875\n",
      "2017-06-30T10:13:15.788060: step 2021, loss 0.353273, acc 0.953125\n",
      "2017-06-30T10:13:15.912154: step 2022, loss 0.390208, acc 0.9375\n",
      "2017-06-30T10:13:16.029506: step 2023, loss 0.364502, acc 0.953125\n",
      "2017-06-30T10:13:16.149477: step 2024, loss 0.325292, acc 1\n",
      "2017-06-30T10:13:16.271565: step 2025, loss 0.402995, acc 0.90625\n",
      "2017-06-30T10:13:16.394952: step 2026, loss 0.374291, acc 0.9375\n",
      "2017-06-30T10:13:16.520613: step 2027, loss 0.388768, acc 0.921875\n",
      "2017-06-30T10:13:16.640065: step 2028, loss 0.361504, acc 0.953125\n",
      "2017-06-30T10:13:16.758405: step 2029, loss 0.397626, acc 0.921875\n",
      "2017-06-30T10:13:16.882791: step 2030, loss 0.345488, acc 0.984375\n",
      "2017-06-30T10:13:17.028212: step 2031, loss 0.374818, acc 0.9375\n",
      "2017-06-30T10:13:17.192727: step 2032, loss 0.366961, acc 0.96875\n",
      "2017-06-30T10:13:17.316127: step 2033, loss 0.396567, acc 0.921875\n",
      "2017-06-30T10:13:17.435822: step 2034, loss 0.373584, acc 0.953125\n",
      "2017-06-30T10:13:17.555243: step 2035, loss 0.393854, acc 0.90625\n",
      "2017-06-30T10:13:17.677347: step 2036, loss 0.361419, acc 0.953125\n",
      "2017-06-30T10:13:17.799967: step 2037, loss 0.375199, acc 0.953125\n",
      "2017-06-30T10:13:17.922693: step 2038, loss 0.333869, acc 0.984375\n",
      "2017-06-30T10:13:18.041557: step 2039, loss 0.370019, acc 0.9375\n",
      "2017-06-30T10:13:18.163911: step 2040, loss 0.364773, acc 0.9375\n",
      "2017-06-30T10:13:18.325022: step 2041, loss 0.361845, acc 0.96875\n",
      "2017-06-30T10:13:18.470178: step 2042, loss 0.411068, acc 0.90625\n",
      "2017-06-30T10:13:18.588891: step 2043, loss 0.355742, acc 0.96875\n",
      "2017-06-30T10:13:18.707382: step 2044, loss 0.360276, acc 0.953125\n",
      "2017-06-30T10:13:18.824379: step 2045, loss 0.394352, acc 0.921875\n",
      "2017-06-30T10:13:18.941971: step 2046, loss 0.415104, acc 0.875\n",
      "2017-06-30T10:13:19.065146: step 2047, loss 0.359493, acc 0.9375\n",
      "2017-06-30T10:13:19.183753: step 2048, loss 0.379134, acc 0.9375\n",
      "2017-06-30T10:13:19.302207: step 2049, loss 0.360959, acc 0.953125\n",
      "2017-06-30T10:13:19.423187: step 2050, loss 0.351233, acc 0.96875\n",
      "2017-06-30T10:13:19.550133: step 2051, loss 0.387444, acc 0.9375\n",
      "2017-06-30T10:13:19.665671: step 2052, loss 0.39319, acc 0.9375\n",
      "2017-06-30T10:13:19.785407: step 2053, loss 0.410468, acc 0.90625\n",
      "2017-06-30T10:13:19.906763: step 2054, loss 0.399121, acc 0.90625\n",
      "2017-06-30T10:13:20.031918: step 2055, loss 0.32132, acc 1\n",
      "2017-06-30T10:13:20.151231: step 2056, loss 0.413175, acc 0.90625\n",
      "2017-06-30T10:13:20.272527: step 2057, loss 0.395548, acc 0.90625\n",
      "2017-06-30T10:13:20.392235: step 2058, loss 0.411051, acc 0.90625\n",
      "2017-06-30T10:13:20.514794: step 2059, loss 0.364338, acc 0.953125\n",
      "2017-06-30T10:13:20.632848: step 2060, loss 0.374806, acc 0.953125\n",
      "2017-06-30T10:13:20.751200: step 2061, loss 0.400402, acc 0.890625\n",
      "2017-06-30T10:13:20.873164: step 2062, loss 0.344757, acc 0.984375\n",
      "2017-06-30T10:13:20.997566: step 2063, loss 0.427988, acc 0.875\n",
      "2017-06-30T10:13:21.116223: step 2064, loss 0.355617, acc 0.96875\n",
      "2017-06-30T10:13:21.234116: step 2065, loss 0.337522, acc 0.984375\n",
      "2017-06-30T10:13:21.346634: step 2066, loss 0.370742, acc 0.9375\n",
      "2017-06-30T10:13:21.472321: step 2067, loss 0.367638, acc 0.953125\n",
      "2017-06-30T10:13:21.598674: step 2068, loss 0.383839, acc 0.96875\n",
      "2017-06-30T10:13:21.731719: step 2069, loss 0.370834, acc 0.9375\n",
      "2017-06-30T10:13:21.862896: step 2070, loss 0.402165, acc 0.921875\n",
      "2017-06-30T10:13:21.993247: step 2071, loss 0.455065, acc 0.84375\n",
      "2017-06-30T10:13:22.124228: step 2072, loss 0.410686, acc 0.890625\n",
      "2017-06-30T10:13:22.251932: step 2073, loss 0.391899, acc 0.921875\n",
      "2017-06-30T10:13:22.381265: step 2074, loss 0.344893, acc 0.984375\n",
      "2017-06-30T10:13:22.510606: step 2075, loss 0.355001, acc 0.953125\n",
      "2017-06-30T10:13:22.645223: step 2076, loss 0.39798, acc 0.90625\n",
      "2017-06-30T10:13:22.778184: step 2077, loss 0.361774, acc 0.9375\n",
      "2017-06-30T10:13:22.909018: step 2078, loss 0.364663, acc 0.96875\n",
      "2017-06-30T10:13:23.037064: step 2079, loss 0.36596, acc 0.953125\n",
      "2017-06-30T10:13:23.173494: step 2080, loss 0.394967, acc 0.921875\n",
      "2017-06-30T10:13:23.303915: step 2081, loss 0.35309, acc 0.96875\n",
      "2017-06-30T10:13:23.437088: step 2082, loss 0.40142, acc 0.90625\n",
      "2017-06-30T10:13:23.561305: step 2083, loss 0.37712, acc 0.9375\n",
      "2017-06-30T10:13:23.692226: step 2084, loss 0.384437, acc 0.9375\n",
      "2017-06-30T10:13:23.817884: step 2085, loss 0.37383, acc 0.9375\n",
      "2017-06-30T10:13:23.947036: step 2086, loss 0.370433, acc 0.9375\n",
      "2017-06-30T10:13:24.074285: step 2087, loss 0.360775, acc 0.9375\n",
      "2017-06-30T10:13:24.213789: step 2088, loss 0.366338, acc 0.953125\n",
      "2017-06-30T10:13:24.352161: step 2089, loss 0.361735, acc 0.953125\n",
      "2017-06-30T10:13:24.476810: step 2090, loss 0.373787, acc 0.953125\n",
      "2017-06-30T10:13:24.600237: step 2091, loss 0.374249, acc 0.953125\n",
      "2017-06-30T10:13:24.730842: step 2092, loss 0.370458, acc 0.953125\n",
      "2017-06-30T10:13:24.853371: step 2093, loss 0.369991, acc 0.9375\n",
      "2017-06-30T10:13:24.985497: step 2094, loss 0.405134, acc 0.890625\n",
      "2017-06-30T10:13:25.110997: step 2095, loss 0.376536, acc 0.921875\n",
      "2017-06-30T10:13:25.236238: step 2096, loss 0.394843, acc 0.90625\n",
      "2017-06-30T10:13:25.365315: step 2097, loss 0.357244, acc 0.96875\n",
      "2017-06-30T10:13:25.487452: step 2098, loss 0.422806, acc 0.890625\n",
      "2017-06-30T10:13:25.614925: step 2099, loss 0.412101, acc 0.890625\n",
      "2017-06-30T10:13:25.748248: step 2100, loss 0.363006, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:13:26.081130: step 2100, loss 0.566109, acc 0.732645\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2100\n",
      "\n",
      "2017-06-30T10:13:26.861403: step 2101, loss 0.358845, acc 0.96875\n",
      "2017-06-30T10:13:26.985087: step 2102, loss 0.389541, acc 0.921875\n",
      "2017-06-30T10:13:27.109427: step 2103, loss 0.362265, acc 0.9375\n",
      "2017-06-30T10:13:27.232147: step 2104, loss 0.357056, acc 0.953125\n",
      "2017-06-30T10:13:27.358113: step 2105, loss 0.359428, acc 0.953125\n",
      "2017-06-30T10:13:27.482171: step 2106, loss 0.361215, acc 0.953125\n",
      "2017-06-30T10:13:27.607768: step 2107, loss 0.357813, acc 0.953125\n",
      "2017-06-30T10:13:27.728051: step 2108, loss 0.412556, acc 0.890625\n",
      "2017-06-30T10:13:27.855932: step 2109, loss 0.330934, acc 0.984375\n",
      "2017-06-30T10:13:27.982648: step 2110, loss 0.36634, acc 0.953125\n",
      "2017-06-30T10:13:28.103471: step 2111, loss 0.361441, acc 0.953125\n",
      "2017-06-30T10:13:28.234295: step 2112, loss 0.381976, acc 0.921875\n",
      "2017-06-30T10:13:28.363363: step 2113, loss 0.363246, acc 0.953125\n",
      "2017-06-30T10:13:28.486891: step 2114, loss 0.364503, acc 0.953125\n",
      "2017-06-30T10:13:28.613400: step 2115, loss 0.32817, acc 1\n",
      "2017-06-30T10:13:28.736430: step 2116, loss 0.429994, acc 0.875\n",
      "2017-06-30T10:13:28.861264: step 2117, loss 0.384788, acc 0.9375\n",
      "2017-06-30T10:13:28.986826: step 2118, loss 0.346833, acc 0.984375\n",
      "2017-06-30T10:13:29.114195: step 2119, loss 0.360989, acc 0.953125\n",
      "2017-06-30T10:13:29.248893: step 2120, loss 0.368678, acc 0.9375\n",
      "2017-06-30T10:13:29.380548: step 2121, loss 0.337705, acc 0.984375\n",
      "2017-06-30T10:13:29.507201: step 2122, loss 0.365072, acc 0.953125\n",
      "2017-06-30T10:13:29.641778: step 2123, loss 0.320752, acc 1\n",
      "2017-06-30T10:13:29.774171: step 2124, loss 0.337295, acc 0.984375\n",
      "2017-06-30T10:13:29.906626: step 2125, loss 0.35689, acc 0.96875\n",
      "2017-06-30T10:13:30.041639: step 2126, loss 0.334961, acc 0.984375\n",
      "2017-06-30T10:13:30.169549: step 2127, loss 0.363806, acc 0.9375\n",
      "2017-06-30T10:13:30.300195: step 2128, loss 0.328609, acc 1\n",
      "2017-06-30T10:13:30.431619: step 2129, loss 0.393616, acc 0.921875\n",
      "2017-06-30T10:13:30.561079: step 2130, loss 0.411287, acc 0.890625\n",
      "2017-06-30T10:13:30.691287: step 2131, loss 0.374106, acc 0.9375\n",
      "2017-06-30T10:13:30.820920: step 2132, loss 0.362339, acc 0.953125\n",
      "2017-06-30T10:13:30.950360: step 2133, loss 0.364803, acc 0.953125\n",
      "2017-06-30T10:13:31.079336: step 2134, loss 0.384861, acc 0.921875\n",
      "2017-06-30T10:13:31.210643: step 2135, loss 0.339133, acc 0.96875\n",
      "2017-06-30T10:13:31.340840: step 2136, loss 0.384858, acc 0.921875\n",
      "2017-06-30T10:13:31.467338: step 2137, loss 0.365469, acc 0.953125\n",
      "2017-06-30T10:13:31.595930: step 2138, loss 0.34982, acc 0.96875\n",
      "2017-06-30T10:13:31.726141: step 2139, loss 0.350233, acc 0.96875\n",
      "2017-06-30T10:13:31.852933: step 2140, loss 0.36839, acc 0.953125\n",
      "2017-06-30T10:13:31.981807: step 2141, loss 0.331228, acc 0.984375\n",
      "2017-06-30T10:13:32.107569: step 2142, loss 0.369868, acc 0.9375\n",
      "2017-06-30T10:13:32.231216: step 2143, loss 0.349057, acc 0.96875\n",
      "2017-06-30T10:13:32.358246: step 2144, loss 0.339039, acc 1\n",
      "2017-06-30T10:13:32.487855: step 2145, loss 0.35773, acc 0.953125\n",
      "2017-06-30T10:13:32.607278: step 2146, loss 0.389031, acc 0.921875\n",
      "2017-06-30T10:13:32.733638: step 2147, loss 0.383957, acc 0.921875\n",
      "2017-06-30T10:13:32.862088: step 2148, loss 0.355231, acc 0.96875\n",
      "2017-06-30T10:13:32.990829: step 2149, loss 0.347281, acc 0.96875\n",
      "2017-06-30T10:13:33.118161: step 2150, loss 0.354129, acc 0.953125\n",
      "2017-06-30T10:13:33.250459: step 2151, loss 0.391178, acc 0.921875\n",
      "2017-06-30T10:13:33.380562: step 2152, loss 0.336487, acc 0.984375\n",
      "2017-06-30T10:13:33.514030: step 2153, loss 0.326278, acc 1\n",
      "2017-06-30T10:13:33.645532: step 2154, loss 0.338967, acc 0.984375\n",
      "2017-06-30T10:13:33.769556: step 2155, loss 0.342262, acc 0.984375\n",
      "2017-06-30T10:13:33.896677: step 2156, loss 0.356623, acc 0.96875\n",
      "2017-06-30T10:13:34.018766: step 2157, loss 0.356469, acc 0.96875\n",
      "2017-06-30T10:13:34.148615: step 2158, loss 0.414145, acc 0.890625\n",
      "2017-06-30T10:13:34.275493: step 2159, loss 0.384623, acc 0.90625\n",
      "2017-06-30T10:13:34.403989: step 2160, loss 0.377854, acc 0.9375\n",
      "2017-06-30T10:13:34.531154: step 2161, loss 0.379827, acc 0.9375\n",
      "2017-06-30T10:13:34.657338: step 2162, loss 0.419439, acc 0.90625\n",
      "2017-06-30T10:13:34.786020: step 2163, loss 0.460556, acc 0.859375\n",
      "2017-06-30T10:13:34.908391: step 2164, loss 0.33842, acc 0.96875\n",
      "2017-06-30T10:13:35.038903: step 2165, loss 0.362084, acc 0.9375\n",
      "2017-06-30T10:13:35.168338: step 2166, loss 0.331118, acc 0.984375\n",
      "2017-06-30T10:13:35.293856: step 2167, loss 0.37698, acc 0.953125\n",
      "2017-06-30T10:13:35.418958: step 2168, loss 0.368837, acc 0.921875\n",
      "2017-06-30T10:13:35.545318: step 2169, loss 0.354321, acc 0.96875\n",
      "2017-06-30T10:13:35.676025: step 2170, loss 0.376492, acc 0.9375\n",
      "2017-06-30T10:13:35.805152: step 2171, loss 0.340147, acc 0.984375\n",
      "2017-06-30T10:13:35.929530: step 2172, loss 0.353492, acc 0.96875\n",
      "2017-06-30T10:13:36.054265: step 2173, loss 0.369829, acc 0.9375\n",
      "2017-06-30T10:13:36.180572: step 2174, loss 0.349621, acc 0.953125\n",
      "2017-06-30T10:13:36.310245: step 2175, loss 0.389436, acc 0.9375\n",
      "2017-06-30T10:13:36.437902: step 2176, loss 0.375533, acc 0.9375\n",
      "2017-06-30T10:13:36.569091: step 2177, loss 0.403978, acc 0.90625\n",
      "2017-06-30T10:13:36.688518: step 2178, loss 0.369453, acc 0.953125\n",
      "2017-06-30T10:13:36.817149: step 2179, loss 0.347569, acc 0.96875\n",
      "2017-06-30T10:13:36.941887: step 2180, loss 0.377254, acc 0.9375\n",
      "2017-06-30T10:13:37.076284: step 2181, loss 0.395308, acc 0.921875\n",
      "2017-06-30T10:13:37.201616: step 2182, loss 0.398847, acc 0.90625\n",
      "2017-06-30T10:13:37.329262: step 2183, loss 0.359183, acc 0.96875\n",
      "2017-06-30T10:13:37.458111: step 2184, loss 0.323315, acc 1\n",
      "2017-06-30T10:13:37.578981: step 2185, loss 0.377917, acc 0.921875\n",
      "2017-06-30T10:13:37.702284: step 2186, loss 0.378072, acc 0.9375\n",
      "2017-06-30T10:13:37.826652: step 2187, loss 0.354876, acc 0.96875\n",
      "2017-06-30T10:13:37.953128: step 2188, loss 0.361698, acc 0.9375\n",
      "2017-06-30T10:13:38.078528: step 2189, loss 0.331191, acc 1\n",
      "2017-06-30T10:13:38.203744: step 2190, loss 0.358234, acc 0.96875\n",
      "2017-06-30T10:13:38.333845: step 2191, loss 0.370013, acc 0.9375\n",
      "2017-06-30T10:13:38.462962: step 2192, loss 0.371015, acc 0.9375\n",
      "2017-06-30T10:13:38.589150: step 2193, loss 0.37948, acc 0.921875\n",
      "2017-06-30T10:13:38.715588: step 2194, loss 0.342414, acc 0.984375\n",
      "2017-06-30T10:13:38.836529: step 2195, loss 0.373466, acc 0.953125\n",
      "2017-06-30T10:13:38.967897: step 2196, loss 0.371769, acc 0.953125\n",
      "2017-06-30T10:13:39.098685: step 2197, loss 0.384892, acc 0.9375\n",
      "2017-06-30T10:13:39.225376: step 2198, loss 0.362247, acc 0.9375\n",
      "2017-06-30T10:13:39.353825: step 2199, loss 0.393524, acc 0.90625\n",
      "2017-06-30T10:13:39.479691: step 2200, loss 0.350248, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:13:39.776079: step 2200, loss 0.563665, acc 0.739212\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2200\n",
      "\n",
      "2017-06-30T10:13:40.652216: step 2201, loss 0.330632, acc 0.984375\n",
      "2017-06-30T10:13:40.776078: step 2202, loss 0.403705, acc 0.90625\n",
      "2017-06-30T10:13:40.899664: step 2203, loss 0.345086, acc 0.96875\n",
      "2017-06-30T10:13:41.023048: step 2204, loss 0.37883, acc 0.921875\n",
      "2017-06-30T10:13:41.146858: step 2205, loss 0.343921, acc 0.984375\n",
      "2017-06-30T10:13:41.270661: step 2206, loss 0.36276, acc 0.953125\n",
      "2017-06-30T10:13:41.399753: step 2207, loss 0.413434, acc 0.90625\n",
      "2017-06-30T10:13:41.519545: step 2208, loss 0.338022, acc 0.984375\n",
      "2017-06-30T10:13:41.643541: step 2209, loss 0.373477, acc 0.921875\n",
      "2017-06-30T10:13:41.767204: step 2210, loss 0.370446, acc 0.9375\n",
      "2017-06-30T10:13:41.897007: step 2211, loss 0.388177, acc 0.921875\n",
      "2017-06-30T10:13:42.021324: step 2212, loss 0.364866, acc 0.9375\n",
      "2017-06-30T10:13:42.149550: step 2213, loss 0.347492, acc 0.984375\n",
      "2017-06-30T10:13:42.269233: step 2214, loss 0.386174, acc 0.9375\n",
      "2017-06-30T10:13:42.392025: step 2215, loss 0.39513, acc 0.90625\n",
      "2017-06-30T10:13:42.513653: step 2216, loss 0.378891, acc 0.9375\n",
      "2017-06-30T10:13:42.637484: step 2217, loss 0.38586, acc 0.9375\n",
      "2017-06-30T10:13:42.761657: step 2218, loss 0.36697, acc 0.953125\n",
      "2017-06-30T10:13:42.881468: step 2219, loss 0.345317, acc 0.96875\n",
      "2017-06-30T10:13:43.003515: step 2220, loss 0.385974, acc 0.90625\n",
      "2017-06-30T10:13:43.124904: step 2221, loss 0.363904, acc 0.953125\n",
      "2017-06-30T10:13:43.249073: step 2222, loss 0.400333, acc 0.921875\n",
      "2017-06-30T10:13:43.374754: step 2223, loss 0.339185, acc 0.984375\n",
      "2017-06-30T10:13:43.498667: step 2224, loss 0.359558, acc 0.953125\n",
      "2017-06-30T10:13:43.624768: step 2225, loss 0.324925, acc 1\n",
      "2017-06-30T10:13:43.746805: step 2226, loss 0.403871, acc 0.890625\n",
      "2017-06-30T10:13:43.870858: step 2227, loss 0.355559, acc 0.953125\n",
      "2017-06-30T10:13:43.992004: step 2228, loss 0.369482, acc 0.96875\n",
      "2017-06-30T10:13:44.118966: step 2229, loss 0.368166, acc 0.953125\n",
      "2017-06-30T10:13:44.242261: step 2230, loss 0.409312, acc 0.90625\n",
      "2017-06-30T10:13:44.366729: step 2231, loss 0.354395, acc 0.953125\n",
      "2017-06-30T10:13:44.494158: step 2232, loss 0.354881, acc 0.953125\n",
      "2017-06-30T10:13:44.618442: step 2233, loss 0.378994, acc 0.9375\n",
      "2017-06-30T10:13:44.742089: step 2234, loss 0.361443, acc 0.96875\n",
      "2017-06-30T10:13:44.865460: step 2235, loss 0.363822, acc 0.953125\n",
      "2017-06-30T10:13:44.989156: step 2236, loss 0.339053, acc 0.96875\n",
      "2017-06-30T10:13:45.113728: step 2237, loss 0.376576, acc 0.9375\n",
      "2017-06-30T10:13:45.239767: step 2238, loss 0.338476, acc 0.96875\n",
      "2017-06-30T10:13:45.367792: step 2239, loss 0.408391, acc 0.890625\n",
      "2017-06-30T10:13:45.489168: step 2240, loss 0.381838, acc 0.921875\n",
      "2017-06-30T10:13:45.611957: step 2241, loss 0.373026, acc 0.9375\n",
      "2017-06-30T10:13:45.734002: step 2242, loss 0.363376, acc 0.953125\n",
      "2017-06-30T10:13:45.859468: step 2243, loss 0.393114, acc 0.921875\n",
      "2017-06-30T10:13:45.981937: step 2244, loss 0.335659, acc 0.984375\n",
      "2017-06-30T10:13:46.111414: step 2245, loss 0.349824, acc 0.96875\n",
      "2017-06-30T10:13:46.241606: step 2246, loss 0.357424, acc 0.953125\n",
      "2017-06-30T10:13:46.368933: step 2247, loss 0.337968, acc 0.984375\n",
      "2017-06-30T10:13:46.496877: step 2248, loss 0.371573, acc 0.9375\n",
      "2017-06-30T10:13:46.621213: step 2249, loss 0.334176, acc 0.984375\n",
      "2017-06-30T10:13:46.737876: step 2250, loss 0.370885, acc 0.95\n",
      "2017-06-30T10:13:46.864853: step 2251, loss 0.347813, acc 0.96875\n",
      "2017-06-30T10:13:46.992316: step 2252, loss 0.366083, acc 0.9375\n",
      "2017-06-30T10:13:47.117753: step 2253, loss 0.356456, acc 0.953125\n",
      "2017-06-30T10:13:47.242130: step 2254, loss 0.356918, acc 0.9375\n",
      "2017-06-30T10:13:47.364128: step 2255, loss 0.34395, acc 0.96875\n",
      "2017-06-30T10:13:47.484084: step 2256, loss 0.368265, acc 0.921875\n",
      "2017-06-30T10:13:47.605445: step 2257, loss 0.366787, acc 0.9375\n",
      "2017-06-30T10:13:47.729610: step 2258, loss 0.366636, acc 0.953125\n",
      "2017-06-30T10:13:47.857400: step 2259, loss 0.378192, acc 0.953125\n",
      "2017-06-30T10:13:47.980483: step 2260, loss 0.356054, acc 0.96875\n",
      "2017-06-30T10:13:48.110497: step 2261, loss 0.351543, acc 0.96875\n",
      "2017-06-30T10:13:48.242089: step 2262, loss 0.371165, acc 0.921875\n",
      "2017-06-30T10:13:48.366237: step 2263, loss 0.329699, acc 0.984375\n",
      "2017-06-30T10:13:48.487491: step 2264, loss 0.345919, acc 0.96875\n",
      "2017-06-30T10:13:48.608541: step 2265, loss 0.36744, acc 0.953125\n",
      "2017-06-30T10:13:48.733206: step 2266, loss 0.353126, acc 0.96875\n",
      "2017-06-30T10:13:48.862169: step 2267, loss 0.351479, acc 0.96875\n",
      "2017-06-30T10:13:48.997719: step 2268, loss 0.3731, acc 0.9375\n",
      "2017-06-30T10:13:49.130016: step 2269, loss 0.383095, acc 0.9375\n",
      "2017-06-30T10:13:49.273376: step 2270, loss 0.41076, acc 0.890625\n",
      "2017-06-30T10:13:49.406293: step 2271, loss 0.368844, acc 0.953125\n",
      "2017-06-30T10:13:49.531090: step 2272, loss 0.358032, acc 0.96875\n",
      "2017-06-30T10:13:49.662510: step 2273, loss 0.34584, acc 0.953125\n",
      "2017-06-30T10:13:49.788731: step 2274, loss 0.349363, acc 0.953125\n",
      "2017-06-30T10:13:49.922422: step 2275, loss 0.368621, acc 0.953125\n",
      "2017-06-30T10:13:50.048657: step 2276, loss 0.362985, acc 0.953125\n",
      "2017-06-30T10:13:50.173567: step 2277, loss 0.371115, acc 0.953125\n",
      "2017-06-30T10:13:50.302717: step 2278, loss 0.358585, acc 0.953125\n",
      "2017-06-30T10:13:50.428719: step 2279, loss 0.344799, acc 0.984375\n",
      "2017-06-30T10:13:50.558422: step 2280, loss 0.348419, acc 0.96875\n",
      "2017-06-30T10:13:50.685564: step 2281, loss 0.372261, acc 0.921875\n",
      "2017-06-30T10:13:50.812203: step 2282, loss 0.369525, acc 0.9375\n",
      "2017-06-30T10:13:50.943798: step 2283, loss 0.340121, acc 0.984375\n",
      "2017-06-30T10:13:51.079194: step 2284, loss 0.328665, acc 0.984375\n",
      "2017-06-30T10:13:51.206521: step 2285, loss 0.347781, acc 0.96875\n",
      "2017-06-30T10:13:51.333486: step 2286, loss 0.398708, acc 0.90625\n",
      "2017-06-30T10:13:51.460603: step 2287, loss 0.347503, acc 0.96875\n",
      "2017-06-30T10:13:51.586178: step 2288, loss 0.323, acc 1\n",
      "2017-06-30T10:13:51.714612: step 2289, loss 0.367455, acc 0.953125\n",
      "2017-06-30T10:13:51.837259: step 2290, loss 0.332703, acc 0.984375\n",
      "2017-06-30T10:13:51.966654: step 2291, loss 0.345524, acc 0.953125\n",
      "2017-06-30T10:13:52.093813: step 2292, loss 0.382207, acc 0.921875\n",
      "2017-06-30T10:13:52.223232: step 2293, loss 0.364623, acc 0.953125\n",
      "2017-06-30T10:13:52.350345: step 2294, loss 0.379758, acc 0.921875\n",
      "2017-06-30T10:13:52.472803: step 2295, loss 0.372441, acc 0.953125\n",
      "2017-06-30T10:13:52.600221: step 2296, loss 0.347345, acc 0.96875\n",
      "2017-06-30T10:13:52.723266: step 2297, loss 0.378708, acc 0.9375\n",
      "2017-06-30T10:13:52.850238: step 2298, loss 0.335845, acc 0.984375\n",
      "2017-06-30T10:13:52.978067: step 2299, loss 0.33905, acc 0.96875\n",
      "2017-06-30T10:13:53.105564: step 2300, loss 0.35781, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:13:53.411573: step 2300, loss 0.563036, acc 0.739212\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2300\n",
      "\n",
      "2017-06-30T10:13:54.097375: step 2301, loss 0.352616, acc 0.96875\n",
      "2017-06-30T10:13:54.217035: step 2302, loss 0.347469, acc 0.96875\n",
      "2017-06-30T10:13:54.336748: step 2303, loss 0.372936, acc 0.9375\n",
      "2017-06-30T10:13:54.462682: step 2304, loss 0.371479, acc 0.953125\n",
      "2017-06-30T10:13:54.583436: step 2305, loss 0.346756, acc 0.96875\n",
      "2017-06-30T10:13:54.711826: step 2306, loss 0.375646, acc 0.9375\n",
      "2017-06-30T10:13:54.832065: step 2307, loss 0.354674, acc 0.96875\n",
      "2017-06-30T10:13:54.954905: step 2308, loss 0.366159, acc 0.953125\n",
      "2017-06-30T10:13:55.083725: step 2309, loss 0.369971, acc 0.9375\n",
      "2017-06-30T10:13:55.216931: step 2310, loss 0.346841, acc 0.96875\n",
      "2017-06-30T10:13:55.351686: step 2311, loss 0.330207, acc 0.984375\n",
      "2017-06-30T10:13:55.479812: step 2312, loss 0.364204, acc 0.953125\n",
      "2017-06-30T10:13:55.605434: step 2313, loss 0.365026, acc 0.953125\n",
      "2017-06-30T10:13:55.736369: step 2314, loss 0.386121, acc 0.921875\n",
      "2017-06-30T10:13:55.865947: step 2315, loss 0.34683, acc 0.953125\n",
      "2017-06-30T10:13:55.996035: step 2316, loss 0.386427, acc 0.921875\n",
      "2017-06-30T10:13:56.123886: step 2317, loss 0.351149, acc 0.953125\n",
      "2017-06-30T10:13:56.254496: step 2318, loss 0.346347, acc 0.96875\n",
      "2017-06-30T10:13:56.384082: step 2319, loss 0.343049, acc 0.984375\n",
      "2017-06-30T10:13:56.512513: step 2320, loss 0.411595, acc 0.890625\n",
      "2017-06-30T10:13:56.639806: step 2321, loss 0.371277, acc 0.9375\n",
      "2017-06-30T10:13:56.765128: step 2322, loss 0.381895, acc 0.921875\n",
      "2017-06-30T10:13:56.889572: step 2323, loss 0.343027, acc 0.984375\n",
      "2017-06-30T10:13:57.013837: step 2324, loss 0.34848, acc 0.96875\n",
      "2017-06-30T10:13:57.144052: step 2325, loss 0.341856, acc 0.96875\n",
      "2017-06-30T10:13:57.266407: step 2326, loss 0.389037, acc 0.921875\n",
      "2017-06-30T10:13:57.389855: step 2327, loss 0.381775, acc 0.921875\n",
      "2017-06-30T10:13:57.508185: step 2328, loss 0.330054, acc 0.984375\n",
      "2017-06-30T10:13:57.629750: step 2329, loss 0.33698, acc 0.984375\n",
      "2017-06-30T10:13:57.751946: step 2330, loss 0.356145, acc 0.96875\n",
      "2017-06-30T10:13:57.877519: step 2331, loss 0.346126, acc 0.96875\n",
      "2017-06-30T10:13:57.998036: step 2332, loss 0.35584, acc 0.96875\n",
      "2017-06-30T10:13:58.121090: step 2333, loss 0.338419, acc 0.984375\n",
      "2017-06-30T10:13:58.246698: step 2334, loss 0.363854, acc 0.921875\n",
      "2017-06-30T10:13:58.371512: step 2335, loss 0.37305, acc 0.9375\n",
      "2017-06-30T10:13:58.492757: step 2336, loss 0.369602, acc 0.953125\n",
      "2017-06-30T10:13:58.622122: step 2337, loss 0.341386, acc 0.96875\n",
      "2017-06-30T10:13:58.748408: step 2338, loss 0.357145, acc 0.953125\n",
      "2017-06-30T10:13:58.873577: step 2339, loss 0.357605, acc 0.953125\n",
      "2017-06-30T10:13:58.996806: step 2340, loss 0.339771, acc 0.984375\n",
      "2017-06-30T10:13:59.124567: step 2341, loss 0.422757, acc 0.859375\n",
      "2017-06-30T10:13:59.246438: step 2342, loss 0.383429, acc 0.921875\n",
      "2017-06-30T10:13:59.367063: step 2343, loss 0.353658, acc 0.953125\n",
      "2017-06-30T10:13:59.493523: step 2344, loss 0.335342, acc 0.984375\n",
      "2017-06-30T10:13:59.617058: step 2345, loss 0.34829, acc 0.953125\n",
      "2017-06-30T10:13:59.745405: step 2346, loss 0.407235, acc 0.890625\n",
      "2017-06-30T10:13:59.873329: step 2347, loss 0.334125, acc 0.984375\n",
      "2017-06-30T10:13:59.995892: step 2348, loss 0.349683, acc 0.96875\n",
      "2017-06-30T10:14:00.120350: step 2349, loss 0.36639, acc 0.9375\n",
      "2017-06-30T10:14:00.243775: step 2350, loss 0.383308, acc 0.9375\n",
      "2017-06-30T10:14:00.369086: step 2351, loss 0.381632, acc 0.9375\n",
      "2017-06-30T10:14:00.496752: step 2352, loss 0.387591, acc 0.9375\n",
      "2017-06-30T10:14:00.622245: step 2353, loss 0.35897, acc 0.953125\n",
      "2017-06-30T10:14:00.748492: step 2354, loss 0.363322, acc 0.953125\n",
      "2017-06-30T10:14:00.878639: step 2355, loss 0.387116, acc 0.9375\n",
      "2017-06-30T10:14:01.002995: step 2356, loss 0.358606, acc 0.96875\n",
      "2017-06-30T10:14:01.130152: step 2357, loss 0.369692, acc 0.9375\n",
      "2017-06-30T10:14:01.255532: step 2358, loss 0.357972, acc 0.96875\n",
      "2017-06-30T10:14:01.379871: step 2359, loss 0.36377, acc 0.953125\n",
      "2017-06-30T10:14:01.503727: step 2360, loss 0.346889, acc 0.96875\n",
      "2017-06-30T10:14:01.632845: step 2361, loss 0.359458, acc 0.953125\n",
      "2017-06-30T10:14:01.773238: step 2362, loss 0.36994, acc 0.953125\n",
      "2017-06-30T10:14:01.903389: step 2363, loss 0.377763, acc 0.953125\n",
      "2017-06-30T10:14:02.030854: step 2364, loss 0.369632, acc 0.921875\n",
      "2017-06-30T10:14:02.159664: step 2365, loss 0.371169, acc 0.9375\n",
      "2017-06-30T10:14:02.289785: step 2366, loss 0.371027, acc 0.953125\n",
      "2017-06-30T10:14:02.408462: step 2367, loss 0.322536, acc 1\n",
      "2017-06-30T10:14:02.525722: step 2368, loss 0.379717, acc 0.9375\n",
      "2017-06-30T10:14:02.648383: step 2369, loss 0.364764, acc 0.96875\n",
      "2017-06-30T10:14:02.766107: step 2370, loss 0.407944, acc 0.921875\n",
      "2017-06-30T10:14:02.886632: step 2371, loss 0.364053, acc 0.953125\n",
      "2017-06-30T10:14:03.009050: step 2372, loss 0.381904, acc 0.921875\n",
      "2017-06-30T10:14:03.129987: step 2373, loss 0.353559, acc 0.96875\n",
      "2017-06-30T10:14:03.250784: step 2374, loss 0.336389, acc 0.96875\n",
      "2017-06-30T10:14:03.375423: step 2375, loss 0.393783, acc 0.921875\n",
      "2017-06-30T10:14:03.497828: step 2376, loss 0.350827, acc 0.953125\n",
      "2017-06-30T10:14:03.621164: step 2377, loss 0.335601, acc 0.984375\n",
      "2017-06-30T10:14:03.747770: step 2378, loss 0.340887, acc 0.96875\n",
      "2017-06-30T10:14:03.872857: step 2379, loss 0.357002, acc 0.9375\n",
      "2017-06-30T10:14:03.996154: step 2380, loss 0.383988, acc 0.9375\n",
      "2017-06-30T10:14:04.122103: step 2381, loss 0.384238, acc 0.921875\n",
      "2017-06-30T10:14:04.246518: step 2382, loss 0.366346, acc 0.953125\n",
      "2017-06-30T10:14:04.366512: step 2383, loss 0.385322, acc 0.9375\n",
      "2017-06-30T10:14:04.490519: step 2384, loss 0.343133, acc 0.96875\n",
      "2017-06-30T10:14:04.611178: step 2385, loss 0.355947, acc 0.953125\n",
      "2017-06-30T10:14:04.734539: step 2386, loss 0.343582, acc 0.96875\n",
      "2017-06-30T10:14:04.855783: step 2387, loss 0.352441, acc 0.96875\n",
      "2017-06-30T10:14:04.984113: step 2388, loss 0.357063, acc 0.953125\n",
      "2017-06-30T10:14:05.110685: step 2389, loss 0.362467, acc 0.953125\n",
      "2017-06-30T10:14:05.237100: step 2390, loss 0.370274, acc 0.9375\n",
      "2017-06-30T10:14:05.360818: step 2391, loss 0.379018, acc 0.953125\n",
      "2017-06-30T10:14:05.488068: step 2392, loss 0.343512, acc 0.96875\n",
      "2017-06-30T10:14:05.605547: step 2393, loss 0.365619, acc 0.953125\n",
      "2017-06-30T10:14:05.728763: step 2394, loss 0.397697, acc 0.90625\n",
      "2017-06-30T10:14:05.850800: step 2395, loss 0.336936, acc 0.96875\n",
      "2017-06-30T10:14:05.976346: step 2396, loss 0.35112, acc 0.96875\n",
      "2017-06-30T10:14:06.105021: step 2397, loss 0.336991, acc 0.984375\n",
      "2017-06-30T10:14:06.229190: step 2398, loss 0.334477, acc 1\n",
      "2017-06-30T10:14:06.353976: step 2399, loss 0.319947, acc 1\n",
      "2017-06-30T10:14:06.470363: step 2400, loss 0.348074, acc 0.966667\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:14:06.772296: step 2400, loss 0.566001, acc 0.73546\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2400\n",
      "\n",
      "2017-06-30T10:14:07.511873: step 2401, loss 0.3508, acc 0.96875\n",
      "2017-06-30T10:14:07.628297: step 2402, loss 0.352826, acc 0.953125\n",
      "2017-06-30T10:14:07.747168: step 2403, loss 0.348919, acc 0.96875\n",
      "2017-06-30T10:14:07.863959: step 2404, loss 0.320269, acc 1\n",
      "2017-06-30T10:14:07.987179: step 2405, loss 0.338481, acc 0.984375\n",
      "2017-06-30T10:14:08.110205: step 2406, loss 0.384634, acc 0.921875\n",
      "2017-06-30T10:14:08.232315: step 2407, loss 0.355982, acc 0.96875\n",
      "2017-06-30T10:14:08.355735: step 2408, loss 0.337107, acc 0.984375\n",
      "2017-06-30T10:14:08.476810: step 2409, loss 0.356668, acc 0.9375\n",
      "2017-06-30T10:14:08.599944: step 2410, loss 0.342132, acc 0.96875\n",
      "2017-06-30T10:14:08.725090: step 2411, loss 0.339085, acc 0.984375\n",
      "2017-06-30T10:14:08.850235: step 2412, loss 0.341529, acc 0.96875\n",
      "2017-06-30T10:14:08.980667: step 2413, loss 0.333286, acc 0.984375\n",
      "2017-06-30T10:14:09.101351: step 2414, loss 0.316605, acc 1\n",
      "2017-06-30T10:14:09.232400: step 2415, loss 0.380439, acc 0.9375\n",
      "2017-06-30T10:14:09.360667: step 2416, loss 0.384756, acc 0.9375\n",
      "2017-06-30T10:14:09.489298: step 2417, loss 0.324281, acc 1\n",
      "2017-06-30T10:14:09.618635: step 2418, loss 0.379173, acc 0.921875\n",
      "2017-06-30T10:14:09.746337: step 2419, loss 0.371923, acc 0.9375\n",
      "2017-06-30T10:14:09.872552: step 2420, loss 0.428428, acc 0.875\n",
      "2017-06-30T10:14:10.000930: step 2421, loss 0.37597, acc 0.9375\n",
      "2017-06-30T10:14:10.132132: step 2422, loss 0.346794, acc 0.96875\n",
      "2017-06-30T10:14:10.259848: step 2423, loss 0.316456, acc 1\n",
      "2017-06-30T10:14:10.387641: step 2424, loss 0.390988, acc 0.921875\n",
      "2017-06-30T10:14:10.516135: step 2425, loss 0.346013, acc 0.96875\n",
      "2017-06-30T10:14:10.640951: step 2426, loss 0.360998, acc 0.953125\n",
      "2017-06-30T10:14:10.771915: step 2427, loss 0.337226, acc 0.96875\n",
      "2017-06-30T10:14:10.889289: step 2428, loss 0.372174, acc 0.9375\n",
      "2017-06-30T10:14:11.020233: step 2429, loss 0.35038, acc 0.96875\n",
      "2017-06-30T10:14:11.139316: step 2430, loss 0.336018, acc 0.984375\n",
      "2017-06-30T10:14:11.267502: step 2431, loss 0.358543, acc 0.953125\n",
      "2017-06-30T10:14:11.391738: step 2432, loss 0.387376, acc 0.921875\n",
      "2017-06-30T10:14:11.521243: step 2433, loss 0.349498, acc 0.96875\n",
      "2017-06-30T10:14:11.641612: step 2434, loss 0.329115, acc 1\n",
      "2017-06-30T10:14:11.771629: step 2435, loss 0.334025, acc 0.984375\n",
      "2017-06-30T10:14:11.896039: step 2436, loss 0.334914, acc 0.984375\n",
      "2017-06-30T10:14:12.023152: step 2437, loss 0.333855, acc 0.984375\n",
      "2017-06-30T10:14:12.145843: step 2438, loss 0.376118, acc 0.953125\n",
      "2017-06-30T10:14:12.280752: step 2439, loss 0.354713, acc 0.96875\n",
      "2017-06-30T10:14:12.426060: step 2440, loss 0.373887, acc 0.9375\n",
      "2017-06-30T10:14:12.557844: step 2441, loss 0.363335, acc 0.9375\n",
      "2017-06-30T10:14:12.683072: step 2442, loss 0.35169, acc 0.96875\n",
      "2017-06-30T10:14:12.805354: step 2443, loss 0.366259, acc 0.953125\n",
      "2017-06-30T10:14:12.926727: step 2444, loss 0.341076, acc 0.96875\n",
      "2017-06-30T10:14:13.047306: step 2445, loss 0.371851, acc 0.9375\n",
      "2017-06-30T10:14:13.167077: step 2446, loss 0.334768, acc 0.984375\n",
      "2017-06-30T10:14:13.289081: step 2447, loss 0.327224, acc 1\n",
      "2017-06-30T10:14:13.412554: step 2448, loss 0.359603, acc 0.953125\n",
      "2017-06-30T10:14:13.537278: step 2449, loss 0.352533, acc 0.96875\n",
      "2017-06-30T10:14:13.659292: step 2450, loss 0.322065, acc 1\n",
      "2017-06-30T10:14:13.785935: step 2451, loss 0.370265, acc 0.953125\n",
      "2017-06-30T10:14:13.907989: step 2452, loss 0.334784, acc 0.984375\n",
      "2017-06-30T10:14:14.026618: step 2453, loss 0.347656, acc 0.96875\n",
      "2017-06-30T10:14:14.146350: step 2454, loss 0.336888, acc 0.984375\n",
      "2017-06-30T10:14:14.310460: step 2455, loss 0.336622, acc 0.984375\n",
      "2017-06-30T10:14:14.456421: step 2456, loss 0.356686, acc 0.953125\n",
      "2017-06-30T10:14:14.573503: step 2457, loss 0.343085, acc 0.96875\n",
      "2017-06-30T10:14:14.693238: step 2458, loss 0.349974, acc 0.953125\n",
      "2017-06-30T10:14:14.818673: step 2459, loss 0.34163, acc 0.96875\n",
      "2017-06-30T10:14:14.939408: step 2460, loss 0.354092, acc 0.96875\n",
      "2017-06-30T10:14:15.061354: step 2461, loss 0.342693, acc 0.96875\n",
      "2017-06-30T10:14:15.187005: step 2462, loss 0.364465, acc 0.953125\n",
      "2017-06-30T10:14:15.316599: step 2463, loss 0.347045, acc 0.984375\n",
      "2017-06-30T10:14:15.444123: step 2464, loss 0.343712, acc 0.96875\n",
      "2017-06-30T10:14:15.590711: step 2465, loss 0.375986, acc 0.953125\n",
      "2017-06-30T10:14:15.717997: step 2466, loss 0.337243, acc 0.984375\n",
      "2017-06-30T10:14:15.851673: step 2467, loss 0.350943, acc 0.96875\n",
      "2017-06-30T10:14:15.979391: step 2468, loss 0.334463, acc 0.984375\n",
      "2017-06-30T10:14:16.110974: step 2469, loss 0.341351, acc 0.984375\n",
      "2017-06-30T10:14:16.237195: step 2470, loss 0.343723, acc 0.96875\n",
      "2017-06-30T10:14:16.364087: step 2471, loss 0.361909, acc 0.953125\n",
      "2017-06-30T10:14:16.492691: step 2472, loss 0.395433, acc 0.921875\n",
      "2017-06-30T10:14:16.624176: step 2473, loss 0.330945, acc 0.984375\n",
      "2017-06-30T10:14:16.750535: step 2474, loss 0.326273, acc 0.984375\n",
      "2017-06-30T10:14:16.881586: step 2475, loss 0.348866, acc 0.96875\n",
      "2017-06-30T10:14:17.005031: step 2476, loss 0.352467, acc 0.953125\n",
      "2017-06-30T10:14:17.159614: step 2477, loss 0.3861, acc 0.921875\n",
      "2017-06-30T10:14:17.294905: step 2478, loss 0.367205, acc 0.9375\n",
      "2017-06-30T10:14:17.434168: step 2479, loss 0.3285, acc 0.984375\n",
      "2017-06-30T10:14:17.565979: step 2480, loss 0.357192, acc 0.96875\n",
      "2017-06-30T10:14:17.690098: step 2481, loss 0.348507, acc 0.96875\n",
      "2017-06-30T10:14:17.814601: step 2482, loss 0.334625, acc 0.96875\n",
      "2017-06-30T10:14:17.937157: step 2483, loss 0.320161, acc 1\n",
      "2017-06-30T10:14:18.058831: step 2484, loss 0.369804, acc 0.9375\n",
      "2017-06-30T10:14:18.187389: step 2485, loss 0.337161, acc 0.984375\n",
      "2017-06-30T10:14:18.308624: step 2486, loss 0.336046, acc 0.96875\n",
      "2017-06-30T10:14:18.430761: step 2487, loss 0.342819, acc 0.96875\n",
      "2017-06-30T10:14:18.546662: step 2488, loss 0.342684, acc 0.96875\n",
      "2017-06-30T10:14:18.663703: step 2489, loss 0.365018, acc 0.953125\n",
      "2017-06-30T10:14:18.785620: step 2490, loss 0.368032, acc 0.953125\n",
      "2017-06-30T10:14:18.911357: step 2491, loss 0.390828, acc 0.90625\n",
      "2017-06-30T10:14:19.036023: step 2492, loss 0.356396, acc 0.96875\n",
      "2017-06-30T10:14:19.153748: step 2493, loss 0.352028, acc 0.953125\n",
      "2017-06-30T10:14:19.276702: step 2494, loss 0.354307, acc 0.96875\n",
      "2017-06-30T10:14:19.399126: step 2495, loss 0.358249, acc 0.9375\n",
      "2017-06-30T10:14:19.518677: step 2496, loss 0.39625, acc 0.90625\n",
      "2017-06-30T10:14:19.636295: step 2497, loss 0.36926, acc 0.9375\n",
      "2017-06-30T10:14:19.754371: step 2498, loss 0.347066, acc 0.96875\n",
      "2017-06-30T10:14:19.873946: step 2499, loss 0.349557, acc 0.96875\n",
      "2017-06-30T10:14:19.990171: step 2500, loss 0.354353, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:14:20.282152: step 2500, loss 0.56243, acc 0.742026\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2500\n",
      "\n",
      "2017-06-30T10:14:21.119170: step 2501, loss 0.3568, acc 0.953125\n",
      "2017-06-30T10:14:21.234290: step 2502, loss 0.379722, acc 0.9375\n",
      "2017-06-30T10:14:21.348423: step 2503, loss 0.369431, acc 0.9375\n",
      "2017-06-30T10:14:21.466295: step 2504, loss 0.342012, acc 0.984375\n",
      "2017-06-30T10:14:21.592866: step 2505, loss 0.343661, acc 0.96875\n",
      "2017-06-30T10:14:21.710387: step 2506, loss 0.326596, acc 0.984375\n",
      "2017-06-30T10:14:21.839897: step 2507, loss 0.331652, acc 0.984375\n",
      "2017-06-30T10:14:21.957785: step 2508, loss 0.343584, acc 0.96875\n",
      "2017-06-30T10:14:22.084164: step 2509, loss 0.360846, acc 0.953125\n",
      "2017-06-30T10:14:22.201933: step 2510, loss 0.354789, acc 0.96875\n",
      "2017-06-30T10:14:22.320985: step 2511, loss 0.364206, acc 0.953125\n",
      "2017-06-30T10:14:22.440505: step 2512, loss 0.341002, acc 0.96875\n",
      "2017-06-30T10:14:22.562212: step 2513, loss 0.352008, acc 0.953125\n",
      "2017-06-30T10:14:22.680911: step 2514, loss 0.357074, acc 0.953125\n",
      "2017-06-30T10:14:22.797036: step 2515, loss 0.32579, acc 0.984375\n",
      "2017-06-30T10:14:22.912134: step 2516, loss 0.374637, acc 0.9375\n",
      "2017-06-30T10:14:23.036320: step 2517, loss 0.380847, acc 0.9375\n",
      "2017-06-30T10:14:23.156834: step 2518, loss 0.342093, acc 0.984375\n",
      "2017-06-30T10:14:23.277579: step 2519, loss 0.39019, acc 0.921875\n",
      "2017-06-30T10:14:23.397260: step 2520, loss 0.359202, acc 0.96875\n",
      "2017-06-30T10:14:23.519254: step 2521, loss 0.359389, acc 0.953125\n",
      "2017-06-30T10:14:23.632858: step 2522, loss 0.353924, acc 0.953125\n",
      "2017-06-30T10:14:23.751461: step 2523, loss 0.363573, acc 0.9375\n",
      "2017-06-30T10:14:23.870834: step 2524, loss 0.31952, acc 1\n",
      "2017-06-30T10:14:24.009559: step 2525, loss 0.350767, acc 0.96875\n",
      "2017-06-30T10:14:24.154913: step 2526, loss 0.341987, acc 0.96875\n",
      "2017-06-30T10:14:24.302530: step 2527, loss 0.346573, acc 0.96875\n",
      "2017-06-30T10:14:24.443124: step 2528, loss 0.354815, acc 0.96875\n",
      "2017-06-30T10:14:24.586009: step 2529, loss 0.385516, acc 0.90625\n",
      "2017-06-30T10:14:24.727700: step 2530, loss 0.349798, acc 0.96875\n",
      "2017-06-30T10:14:24.854850: step 2531, loss 0.345649, acc 0.96875\n",
      "2017-06-30T10:14:24.978494: step 2532, loss 0.320727, acc 1\n",
      "2017-06-30T10:14:25.103420: step 2533, loss 0.326346, acc 0.984375\n",
      "2017-06-30T10:14:25.226832: step 2534, loss 0.336095, acc 0.984375\n",
      "2017-06-30T10:14:25.351461: step 2535, loss 0.383853, acc 0.921875\n",
      "2017-06-30T10:14:25.476048: step 2536, loss 0.345128, acc 0.96875\n",
      "2017-06-30T10:14:25.613286: step 2537, loss 0.380349, acc 0.9375\n",
      "2017-06-30T10:14:25.761101: step 2538, loss 0.380303, acc 0.9375\n",
      "2017-06-30T10:14:25.889021: step 2539, loss 0.37198, acc 0.953125\n",
      "2017-06-30T10:14:26.010669: step 2540, loss 0.33624, acc 0.984375\n",
      "2017-06-30T10:14:26.138847: step 2541, loss 0.345122, acc 0.96875\n",
      "2017-06-30T10:14:26.271930: step 2542, loss 0.359946, acc 0.96875\n",
      "2017-06-30T10:14:26.410131: step 2543, loss 0.346954, acc 0.96875\n",
      "2017-06-30T10:14:26.534216: step 2544, loss 0.323724, acc 1\n",
      "2017-06-30T10:14:26.668366: step 2545, loss 0.350804, acc 0.96875\n",
      "2017-06-30T10:14:26.809042: step 2546, loss 0.389018, acc 0.9375\n",
      "2017-06-30T10:14:26.946082: step 2547, loss 0.373612, acc 0.9375\n",
      "2017-06-30T10:14:27.065786: step 2548, loss 0.354697, acc 0.953125\n",
      "2017-06-30T10:14:27.196605: step 2549, loss 0.351899, acc 0.96875\n",
      "2017-06-30T10:14:27.308944: step 2550, loss 0.336078, acc 0.983333\n",
      "2017-06-30T10:14:27.434566: step 2551, loss 0.324588, acc 0.984375\n",
      "2017-06-30T10:14:27.560113: step 2552, loss 0.342511, acc 0.984375\n",
      "2017-06-30T10:14:27.703845: step 2553, loss 0.33441, acc 0.96875\n",
      "2017-06-30T10:14:27.850307: step 2554, loss 0.364032, acc 0.953125\n",
      "2017-06-30T10:14:27.987014: step 2555, loss 0.372048, acc 0.953125\n",
      "2017-06-30T10:14:28.116086: step 2556, loss 0.35263, acc 0.96875\n",
      "2017-06-30T10:14:28.247590: step 2557, loss 0.333317, acc 0.984375\n",
      "2017-06-30T10:14:28.368824: step 2558, loss 0.33155, acc 0.984375\n",
      "2017-06-30T10:14:28.510356: step 2559, loss 0.341821, acc 0.984375\n",
      "2017-06-30T10:14:28.649229: step 2560, loss 0.333389, acc 0.984375\n",
      "2017-06-30T10:14:28.775867: step 2561, loss 0.374337, acc 0.953125\n",
      "2017-06-30T10:14:28.904678: step 2562, loss 0.336716, acc 0.984375\n",
      "2017-06-30T10:14:29.038286: step 2563, loss 0.372511, acc 0.921875\n",
      "2017-06-30T10:14:29.158756: step 2564, loss 0.325968, acc 0.984375\n",
      "2017-06-30T10:14:29.292783: step 2565, loss 0.361353, acc 0.953125\n",
      "2017-06-30T10:14:29.427668: step 2566, loss 0.352496, acc 0.953125\n",
      "2017-06-30T10:14:29.564565: step 2567, loss 0.362409, acc 0.953125\n",
      "2017-06-30T10:14:29.707662: step 2568, loss 0.330568, acc 0.984375\n",
      "2017-06-30T10:14:29.834492: step 2569, loss 0.341908, acc 0.984375\n",
      "2017-06-30T10:14:29.955210: step 2570, loss 0.343393, acc 0.96875\n",
      "2017-06-30T10:14:30.088209: step 2571, loss 0.359848, acc 0.9375\n",
      "2017-06-30T10:14:30.223630: step 2572, loss 0.347046, acc 0.984375\n",
      "2017-06-30T10:14:30.365258: step 2573, loss 0.328358, acc 0.984375\n",
      "2017-06-30T10:14:30.496327: step 2574, loss 0.34113, acc 0.984375\n",
      "2017-06-30T10:14:30.635736: step 2575, loss 0.317427, acc 1\n",
      "2017-06-30T10:14:30.761681: step 2576, loss 0.383083, acc 0.90625\n",
      "2017-06-30T10:14:30.896113: step 2577, loss 0.348144, acc 0.96875\n",
      "2017-06-30T10:14:31.017741: step 2578, loss 0.351262, acc 0.96875\n",
      "2017-06-30T10:14:31.139722: step 2579, loss 0.341377, acc 0.984375\n",
      "2017-06-30T10:14:31.261495: step 2580, loss 0.356537, acc 0.953125\n",
      "2017-06-30T10:14:31.387922: step 2581, loss 0.341608, acc 0.96875\n",
      "2017-06-30T10:14:31.524490: step 2582, loss 0.408632, acc 0.890625\n",
      "2017-06-30T10:14:31.662480: step 2583, loss 0.346884, acc 0.96875\n",
      "2017-06-30T10:14:31.789588: step 2584, loss 0.331385, acc 0.984375\n",
      "2017-06-30T10:14:31.917152: step 2585, loss 0.342185, acc 0.96875\n",
      "2017-06-30T10:14:32.041537: step 2586, loss 0.337561, acc 0.96875\n",
      "2017-06-30T10:14:32.171634: step 2587, loss 0.348915, acc 0.96875\n",
      "2017-06-30T10:14:32.306877: step 2588, loss 0.371137, acc 0.9375\n",
      "2017-06-30T10:14:32.435902: step 2589, loss 0.327452, acc 1\n",
      "2017-06-30T10:14:32.557150: step 2590, loss 0.33937, acc 0.984375\n",
      "2017-06-30T10:14:32.692902: step 2591, loss 0.322904, acc 1\n",
      "2017-06-30T10:14:32.827140: step 2592, loss 0.370415, acc 0.953125\n",
      "2017-06-30T10:14:32.952729: step 2593, loss 0.344204, acc 0.984375\n",
      "2017-06-30T10:14:33.089012: step 2594, loss 0.330659, acc 0.984375\n",
      "2017-06-30T10:14:33.218105: step 2595, loss 0.350309, acc 0.96875\n",
      "2017-06-30T10:14:33.337820: step 2596, loss 0.349147, acc 0.96875\n",
      "2017-06-30T10:14:33.457844: step 2597, loss 0.336197, acc 0.984375\n",
      "2017-06-30T10:14:33.576826: step 2598, loss 0.343709, acc 0.96875\n",
      "2017-06-30T10:14:33.706206: step 2599, loss 0.351616, acc 0.96875\n",
      "2017-06-30T10:14:33.828323: step 2600, loss 0.319074, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:14:34.117434: step 2600, loss 0.563314, acc 0.739212\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2600\n",
      "\n",
      "2017-06-30T10:14:34.798236: step 2601, loss 0.342875, acc 0.984375\n",
      "2017-06-30T10:14:34.923089: step 2602, loss 0.350395, acc 0.953125\n",
      "2017-06-30T10:14:35.042037: step 2603, loss 0.338286, acc 0.984375\n",
      "2017-06-30T10:14:35.160601: step 2604, loss 0.375737, acc 0.9375\n",
      "2017-06-30T10:14:35.287403: step 2605, loss 0.349019, acc 0.96875\n",
      "2017-06-30T10:14:35.421079: step 2606, loss 0.344257, acc 0.96875\n",
      "2017-06-30T10:14:35.556246: step 2607, loss 0.376134, acc 0.9375\n",
      "2017-06-30T10:14:35.684751: step 2608, loss 0.343675, acc 0.96875\n",
      "2017-06-30T10:14:35.804448: step 2609, loss 0.394129, acc 0.921875\n",
      "2017-06-30T10:14:35.930094: step 2610, loss 0.351354, acc 0.96875\n",
      "2017-06-30T10:14:36.053730: step 2611, loss 0.347376, acc 0.953125\n",
      "2017-06-30T10:14:36.188959: step 2612, loss 0.333484, acc 0.984375\n",
      "2017-06-30T10:14:36.320430: step 2613, loss 0.34358, acc 0.96875\n",
      "2017-06-30T10:14:36.439742: step 2614, loss 0.330824, acc 0.984375\n",
      "2017-06-30T10:14:36.564133: step 2615, loss 0.331845, acc 0.984375\n",
      "2017-06-30T10:14:36.683277: step 2616, loss 0.366795, acc 0.953125\n",
      "2017-06-30T10:14:36.811118: step 2617, loss 0.38341, acc 0.921875\n",
      "2017-06-30T10:14:36.945649: step 2618, loss 0.350113, acc 0.96875\n",
      "2017-06-30T10:14:37.076244: step 2619, loss 0.332584, acc 0.984375\n",
      "2017-06-30T10:14:37.301094: step 2620, loss 0.336293, acc 0.96875\n",
      "2017-06-30T10:14:37.428637: step 2621, loss 0.382577, acc 0.9375\n",
      "2017-06-30T10:14:37.558794: step 2622, loss 0.337948, acc 0.96875\n",
      "2017-06-30T10:14:37.681602: step 2623, loss 0.321114, acc 1\n",
      "2017-06-30T10:14:37.798783: step 2624, loss 0.334289, acc 0.984375\n",
      "2017-06-30T10:14:37.917161: step 2625, loss 0.352967, acc 0.96875\n",
      "2017-06-30T10:14:38.040375: step 2626, loss 0.348522, acc 0.96875\n",
      "2017-06-30T10:14:38.173183: step 2627, loss 0.329041, acc 0.984375\n",
      "2017-06-30T10:14:38.311791: step 2628, loss 0.331183, acc 0.984375\n",
      "2017-06-30T10:14:38.437262: step 2629, loss 0.351725, acc 0.96875\n",
      "2017-06-30T10:14:38.571333: step 2630, loss 0.362951, acc 0.9375\n",
      "2017-06-30T10:14:38.707326: step 2631, loss 0.359591, acc 0.953125\n",
      "2017-06-30T10:14:38.830791: step 2632, loss 0.359498, acc 0.953125\n",
      "2017-06-30T10:14:38.961047: step 2633, loss 0.376111, acc 0.9375\n",
      "2017-06-30T10:14:39.090562: step 2634, loss 0.366765, acc 0.953125\n",
      "2017-06-30T10:14:39.225082: step 2635, loss 0.322222, acc 1\n",
      "2017-06-30T10:14:39.356104: step 2636, loss 0.364701, acc 0.953125\n",
      "2017-06-30T10:14:39.496545: step 2637, loss 0.420287, acc 0.890625\n",
      "2017-06-30T10:14:39.627260: step 2638, loss 0.318339, acc 1\n",
      "2017-06-30T10:14:39.753083: step 2639, loss 0.343431, acc 0.96875\n",
      "2017-06-30T10:14:39.886404: step 2640, loss 0.349807, acc 0.96875\n",
      "2017-06-30T10:14:40.014436: step 2641, loss 0.345693, acc 0.96875\n",
      "2017-06-30T10:14:40.143097: step 2642, loss 0.399198, acc 0.90625\n",
      "2017-06-30T10:14:40.264304: step 2643, loss 0.33787, acc 0.984375\n",
      "2017-06-30T10:14:40.389859: step 2644, loss 0.337258, acc 0.96875\n",
      "2017-06-30T10:14:40.514366: step 2645, loss 0.335621, acc 0.984375\n",
      "2017-06-30T10:14:40.633490: step 2646, loss 0.335321, acc 1\n",
      "2017-06-30T10:14:40.759290: step 2647, loss 0.363118, acc 0.9375\n",
      "2017-06-30T10:14:40.873718: step 2648, loss 0.365696, acc 0.9375\n",
      "2017-06-30T10:14:40.989907: step 2649, loss 0.338894, acc 0.984375\n",
      "2017-06-30T10:14:41.111232: step 2650, loss 0.321874, acc 1\n",
      "2017-06-30T10:14:41.237390: step 2651, loss 0.373115, acc 0.9375\n",
      "2017-06-30T10:14:41.358099: step 2652, loss 0.387306, acc 0.9375\n",
      "2017-06-30T10:14:41.477636: step 2653, loss 0.40516, acc 0.90625\n",
      "2017-06-30T10:14:41.601471: step 2654, loss 0.324623, acc 1\n",
      "2017-06-30T10:14:41.721034: step 2655, loss 0.348728, acc 0.96875\n",
      "2017-06-30T10:14:41.841728: step 2656, loss 0.318543, acc 1\n",
      "2017-06-30T10:14:41.968469: step 2657, loss 0.357651, acc 0.96875\n",
      "2017-06-30T10:14:42.094307: step 2658, loss 0.336617, acc 0.984375\n",
      "2017-06-30T10:14:42.213731: step 2659, loss 0.34813, acc 0.96875\n",
      "2017-06-30T10:14:42.348394: step 2660, loss 0.336338, acc 0.96875\n",
      "2017-06-30T10:14:42.469507: step 2661, loss 0.339413, acc 0.96875\n",
      "2017-06-30T10:14:42.599378: step 2662, loss 0.356305, acc 0.953125\n",
      "2017-06-30T10:14:42.717785: step 2663, loss 0.349013, acc 0.96875\n",
      "2017-06-30T10:14:42.843198: step 2664, loss 0.364099, acc 0.9375\n",
      "2017-06-30T10:14:42.976241: step 2665, loss 0.359805, acc 0.953125\n",
      "2017-06-30T10:14:43.103894: step 2666, loss 0.36227, acc 0.953125\n",
      "2017-06-30T10:14:43.220232: step 2667, loss 0.334259, acc 0.984375\n",
      "2017-06-30T10:14:43.343075: step 2668, loss 0.335452, acc 0.984375\n",
      "2017-06-30T10:14:43.469035: step 2669, loss 0.339941, acc 0.96875\n",
      "2017-06-30T10:14:43.595996: step 2670, loss 0.351434, acc 0.96875\n",
      "2017-06-30T10:14:43.712809: step 2671, loss 0.346856, acc 0.96875\n",
      "2017-06-30T10:14:43.833188: step 2672, loss 0.329287, acc 0.984375\n",
      "2017-06-30T10:14:43.958104: step 2673, loss 0.351024, acc 0.96875\n",
      "2017-06-30T10:14:44.080596: step 2674, loss 0.318998, acc 1\n",
      "2017-06-30T10:14:44.206540: step 2675, loss 0.330569, acc 0.984375\n",
      "2017-06-30T10:14:44.333847: step 2676, loss 0.331991, acc 0.984375\n",
      "2017-06-30T10:14:44.455100: step 2677, loss 0.363102, acc 0.953125\n",
      "2017-06-30T10:14:44.578704: step 2678, loss 0.364087, acc 0.96875\n",
      "2017-06-30T10:14:44.703414: step 2679, loss 0.352407, acc 0.96875\n",
      "2017-06-30T10:14:44.823733: step 2680, loss 0.354829, acc 0.953125\n",
      "2017-06-30T10:14:44.954040: step 2681, loss 0.340587, acc 0.984375\n",
      "2017-06-30T10:14:45.075374: step 2682, loss 0.330099, acc 0.984375\n",
      "2017-06-30T10:14:45.193856: step 2683, loss 0.341562, acc 0.96875\n",
      "2017-06-30T10:14:45.318252: step 2684, loss 0.331804, acc 0.984375\n",
      "2017-06-30T10:14:45.437288: step 2685, loss 0.323606, acc 1\n",
      "2017-06-30T10:14:45.554557: step 2686, loss 0.364402, acc 0.953125\n",
      "2017-06-30T10:14:45.674798: step 2687, loss 0.339436, acc 0.984375\n",
      "2017-06-30T10:14:45.796345: step 2688, loss 0.353014, acc 0.96875\n",
      "2017-06-30T10:14:45.921924: step 2689, loss 0.331545, acc 0.984375\n",
      "2017-06-30T10:14:46.045040: step 2690, loss 0.370688, acc 0.953125\n",
      "2017-06-30T10:14:46.166401: step 2691, loss 0.37539, acc 0.921875\n",
      "2017-06-30T10:14:46.289760: step 2692, loss 0.342053, acc 0.984375\n",
      "2017-06-30T10:14:46.415338: step 2693, loss 0.374774, acc 0.9375\n",
      "2017-06-30T10:14:46.534240: step 2694, loss 0.348323, acc 0.96875\n",
      "2017-06-30T10:14:46.658020: step 2695, loss 0.364016, acc 0.953125\n",
      "2017-06-30T10:14:46.777324: step 2696, loss 0.414299, acc 0.90625\n",
      "2017-06-30T10:14:46.896948: step 2697, loss 0.368316, acc 0.9375\n",
      "2017-06-30T10:14:47.024632: step 2698, loss 0.375555, acc 0.9375\n",
      "2017-06-30T10:14:47.161292: step 2699, loss 0.366083, acc 0.9375\n",
      "2017-06-30T10:14:47.287939: step 2700, loss 0.347663, acc 0.966667\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:14:47.600290: step 2700, loss 0.56762, acc 0.728893\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2700\n",
      "\n",
      "2017-06-30T10:14:48.388990: step 2701, loss 0.3184, acc 1\n",
      "2017-06-30T10:14:48.510186: step 2702, loss 0.349172, acc 0.96875\n",
      "2017-06-30T10:14:48.634747: step 2703, loss 0.371994, acc 0.953125\n",
      "2017-06-30T10:14:48.748304: step 2704, loss 0.348124, acc 0.96875\n",
      "2017-06-30T10:14:48.889260: step 2705, loss 0.317073, acc 1\n",
      "2017-06-30T10:14:49.014374: step 2706, loss 0.324949, acc 0.984375\n",
      "2017-06-30T10:14:49.141109: step 2707, loss 0.348266, acc 0.953125\n",
      "2017-06-30T10:14:49.270089: step 2708, loss 0.339282, acc 0.96875\n",
      "2017-06-30T10:14:49.416663: step 2709, loss 0.35405, acc 0.953125\n",
      "2017-06-30T10:14:49.558061: step 2710, loss 0.354301, acc 0.96875\n",
      "2017-06-30T10:14:49.694431: step 2711, loss 0.330284, acc 1\n",
      "2017-06-30T10:14:49.816686: step 2712, loss 0.319008, acc 1\n",
      "2017-06-30T10:14:49.937597: step 2713, loss 0.331253, acc 0.984375\n",
      "2017-06-30T10:14:50.064612: step 2714, loss 0.324547, acc 0.984375\n",
      "2017-06-30T10:14:50.217389: step 2715, loss 0.341439, acc 0.984375\n",
      "2017-06-30T10:14:50.341973: step 2716, loss 0.345871, acc 0.953125\n",
      "2017-06-30T10:14:50.472151: step 2717, loss 0.333788, acc 0.984375\n",
      "2017-06-30T10:14:50.591008: step 2718, loss 0.352394, acc 0.96875\n",
      "2017-06-30T10:14:50.718145: step 2719, loss 0.353363, acc 0.96875\n",
      "2017-06-30T10:14:50.838261: step 2720, loss 0.3596, acc 0.96875\n",
      "2017-06-30T10:14:50.958751: step 2721, loss 0.331383, acc 0.984375\n",
      "2017-06-30T10:14:51.085521: step 2722, loss 0.344903, acc 0.96875\n",
      "2017-06-30T10:14:51.227069: step 2723, loss 0.32154, acc 1\n",
      "2017-06-30T10:14:51.358331: step 2724, loss 0.3433, acc 0.96875\n",
      "2017-06-30T10:14:51.489724: step 2725, loss 0.372815, acc 0.9375\n",
      "2017-06-30T10:14:51.609824: step 2726, loss 0.35072, acc 0.953125\n",
      "2017-06-30T10:14:51.736850: step 2727, loss 0.3157, acc 1\n",
      "2017-06-30T10:14:51.859739: step 2728, loss 0.334827, acc 0.984375\n",
      "2017-06-30T10:14:51.981694: step 2729, loss 0.371393, acc 0.9375\n",
      "2017-06-30T10:14:52.104465: step 2730, loss 0.370777, acc 0.921875\n",
      "2017-06-30T10:14:52.227554: step 2731, loss 0.350694, acc 0.953125\n",
      "2017-06-30T10:14:52.347959: step 2732, loss 0.325718, acc 0.984375\n",
      "2017-06-30T10:14:52.478293: step 2733, loss 0.346167, acc 0.96875\n",
      "2017-06-30T10:14:52.608569: step 2734, loss 0.317235, acc 1\n",
      "2017-06-30T10:14:52.747259: step 2735, loss 0.343789, acc 0.96875\n",
      "2017-06-30T10:14:52.875178: step 2736, loss 0.345366, acc 0.96875\n",
      "2017-06-30T10:14:53.005926: step 2737, loss 0.335998, acc 0.96875\n",
      "2017-06-30T10:14:53.136783: step 2738, loss 0.339974, acc 0.984375\n",
      "2017-06-30T10:14:53.271955: step 2739, loss 0.370795, acc 0.921875\n",
      "2017-06-30T10:14:53.401248: step 2740, loss 0.327974, acc 1\n",
      "2017-06-30T10:14:53.531013: step 2741, loss 0.351732, acc 0.96875\n",
      "2017-06-30T10:14:53.655990: step 2742, loss 0.348678, acc 0.96875\n",
      "2017-06-30T10:14:53.779056: step 2743, loss 0.329856, acc 0.984375\n",
      "2017-06-30T10:14:53.908501: step 2744, loss 0.336291, acc 0.984375\n",
      "2017-06-30T10:14:54.046261: step 2745, loss 0.352771, acc 0.96875\n",
      "2017-06-30T10:14:54.194598: step 2746, loss 0.33503, acc 0.984375\n",
      "2017-06-30T10:14:54.328125: step 2747, loss 0.371797, acc 0.9375\n",
      "2017-06-30T10:14:54.453022: step 2748, loss 0.322941, acc 1\n",
      "2017-06-30T10:14:54.578590: step 2749, loss 0.347012, acc 0.96875\n",
      "2017-06-30T10:14:54.704104: step 2750, loss 0.334573, acc 0.984375\n",
      "2017-06-30T10:14:54.832520: step 2751, loss 0.34893, acc 0.96875\n",
      "2017-06-30T10:14:54.952379: step 2752, loss 0.357803, acc 0.953125\n",
      "2017-06-30T10:14:55.073942: step 2753, loss 0.325114, acc 0.984375\n",
      "2017-06-30T10:14:55.194650: step 2754, loss 0.359995, acc 0.953125\n",
      "2017-06-30T10:14:55.314320: step 2755, loss 0.361322, acc 0.953125\n",
      "2017-06-30T10:14:55.432703: step 2756, loss 0.329311, acc 0.984375\n",
      "2017-06-30T10:14:55.551080: step 2757, loss 0.329122, acc 0.984375\n",
      "2017-06-30T10:14:55.672188: step 2758, loss 0.33747, acc 0.96875\n",
      "2017-06-30T10:14:55.802253: step 2759, loss 0.325353, acc 0.984375\n",
      "2017-06-30T10:14:55.923614: step 2760, loss 0.366134, acc 0.953125\n",
      "2017-06-30T10:14:56.052331: step 2761, loss 0.341623, acc 0.96875\n",
      "2017-06-30T10:14:56.171290: step 2762, loss 0.358797, acc 0.953125\n",
      "2017-06-30T10:14:56.292492: step 2763, loss 0.366291, acc 0.953125\n",
      "2017-06-30T10:14:56.412485: step 2764, loss 0.316104, acc 1\n",
      "2017-06-30T10:14:56.531442: step 2765, loss 0.341714, acc 0.984375\n",
      "2017-06-30T10:14:56.652571: step 2766, loss 0.334759, acc 0.984375\n",
      "2017-06-30T10:14:56.773911: step 2767, loss 0.387445, acc 0.90625\n",
      "2017-06-30T10:14:56.894845: step 2768, loss 0.332511, acc 0.984375\n",
      "2017-06-30T10:14:57.014877: step 2769, loss 0.318127, acc 1\n",
      "2017-06-30T10:14:57.129386: step 2770, loss 0.336335, acc 0.96875\n",
      "2017-06-30T10:14:57.250775: step 2771, loss 0.365337, acc 0.953125\n",
      "2017-06-30T10:14:57.370086: step 2772, loss 0.384631, acc 0.9375\n",
      "2017-06-30T10:14:57.491889: step 2773, loss 0.336246, acc 0.984375\n",
      "2017-06-30T10:14:57.608267: step 2774, loss 0.335389, acc 0.984375\n",
      "2017-06-30T10:14:57.729021: step 2775, loss 0.356462, acc 0.953125\n",
      "2017-06-30T10:14:57.847737: step 2776, loss 0.331711, acc 0.984375\n",
      "2017-06-30T10:14:57.970926: step 2777, loss 0.353476, acc 0.96875\n",
      "2017-06-30T10:14:58.091399: step 2778, loss 0.348005, acc 0.96875\n",
      "2017-06-30T10:14:58.211288: step 2779, loss 0.348111, acc 0.96875\n",
      "2017-06-30T10:14:58.328801: step 2780, loss 0.353921, acc 0.96875\n",
      "2017-06-30T10:14:58.452774: step 2781, loss 0.357147, acc 0.953125\n",
      "2017-06-30T10:14:58.576828: step 2782, loss 0.348928, acc 0.96875\n",
      "2017-06-30T10:14:58.696876: step 2783, loss 0.356322, acc 0.96875\n",
      "2017-06-30T10:14:58.816288: step 2784, loss 0.361258, acc 0.953125\n",
      "2017-06-30T10:14:58.933146: step 2785, loss 0.360684, acc 0.9375\n",
      "2017-06-30T10:14:59.050337: step 2786, loss 0.31612, acc 1\n",
      "2017-06-30T10:14:59.171185: step 2787, loss 0.341253, acc 0.96875\n",
      "2017-06-30T10:14:59.292903: step 2788, loss 0.31514, acc 1\n",
      "2017-06-30T10:14:59.411821: step 2789, loss 0.373603, acc 0.9375\n",
      "2017-06-30T10:14:59.533289: step 2790, loss 0.364051, acc 0.953125\n",
      "2017-06-30T10:14:59.656183: step 2791, loss 0.377653, acc 0.9375\n",
      "2017-06-30T10:14:59.780058: step 2792, loss 0.379846, acc 0.921875\n",
      "2017-06-30T10:14:59.897499: step 2793, loss 0.361396, acc 0.953125\n",
      "2017-06-30T10:15:00.018358: step 2794, loss 0.358376, acc 0.953125\n",
      "2017-06-30T10:15:00.140671: step 2795, loss 0.319566, acc 1\n",
      "2017-06-30T10:15:00.263666: step 2796, loss 0.375695, acc 0.9375\n",
      "2017-06-30T10:15:00.385953: step 2797, loss 0.374706, acc 0.921875\n",
      "2017-06-30T10:15:00.510362: step 2798, loss 0.394882, acc 0.921875\n",
      "2017-06-30T10:15:00.628521: step 2799, loss 0.333983, acc 0.984375\n",
      "2017-06-30T10:15:00.748929: step 2800, loss 0.365424, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:15:01.049944: step 2800, loss 0.564519, acc 0.731707\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2800\n",
      "\n",
      "2017-06-30T10:15:01.870492: step 2801, loss 0.345935, acc 0.96875\n",
      "2017-06-30T10:15:01.986070: step 2802, loss 0.334092, acc 0.984375\n",
      "2017-06-30T10:15:02.104422: step 2803, loss 0.317402, acc 1\n",
      "2017-06-30T10:15:02.214844: step 2804, loss 0.3762, acc 0.9375\n",
      "2017-06-30T10:15:02.334237: step 2805, loss 0.342369, acc 0.96875\n",
      "2017-06-30T10:15:02.447413: step 2806, loss 0.317298, acc 1\n",
      "2017-06-30T10:15:02.572234: step 2807, loss 0.351944, acc 0.96875\n",
      "2017-06-30T10:15:02.687234: step 2808, loss 0.34814, acc 0.96875\n",
      "2017-06-30T10:15:02.808822: step 2809, loss 0.32482, acc 1\n",
      "2017-06-30T10:15:02.931549: step 2810, loss 0.338857, acc 0.96875\n",
      "2017-06-30T10:15:03.053148: step 2811, loss 0.347811, acc 0.96875\n",
      "2017-06-30T10:15:03.176165: step 2812, loss 0.356149, acc 0.96875\n",
      "2017-06-30T10:15:03.300953: step 2813, loss 0.366974, acc 0.953125\n",
      "2017-06-30T10:15:03.417439: step 2814, loss 0.353151, acc 0.96875\n",
      "2017-06-30T10:15:03.537948: step 2815, loss 0.330364, acc 0.96875\n",
      "2017-06-30T10:15:03.656136: step 2816, loss 0.394378, acc 0.921875\n",
      "2017-06-30T10:15:03.776581: step 2817, loss 0.3351, acc 0.984375\n",
      "2017-06-30T10:15:03.902176: step 2818, loss 0.35505, acc 0.96875\n",
      "2017-06-30T10:15:04.023179: step 2819, loss 0.329063, acc 1\n",
      "2017-06-30T10:15:04.142960: step 2820, loss 0.346172, acc 0.96875\n",
      "2017-06-30T10:15:04.261478: step 2821, loss 0.319409, acc 1\n",
      "2017-06-30T10:15:04.382094: step 2822, loss 0.338594, acc 0.984375\n",
      "2017-06-30T10:15:04.502913: step 2823, loss 0.317271, acc 1\n",
      "2017-06-30T10:15:04.617545: step 2824, loss 0.365802, acc 0.953125\n",
      "2017-06-30T10:15:04.741621: step 2825, loss 0.365307, acc 0.953125\n",
      "2017-06-30T10:15:04.861625: step 2826, loss 0.347097, acc 0.953125\n",
      "2017-06-30T10:15:04.978832: step 2827, loss 0.337738, acc 0.96875\n",
      "2017-06-30T10:15:05.095871: step 2828, loss 0.33987, acc 0.984375\n",
      "2017-06-30T10:15:05.217186: step 2829, loss 0.341049, acc 0.96875\n",
      "2017-06-30T10:15:05.337074: step 2830, loss 0.332723, acc 0.984375\n",
      "2017-06-30T10:15:05.454776: step 2831, loss 0.344251, acc 0.96875\n",
      "2017-06-30T10:15:05.572306: step 2832, loss 0.332305, acc 0.984375\n",
      "2017-06-30T10:15:05.692197: step 2833, loss 0.372408, acc 0.9375\n",
      "2017-06-30T10:15:05.806450: step 2834, loss 0.317127, acc 1\n",
      "2017-06-30T10:15:05.929834: step 2835, loss 0.376725, acc 0.9375\n",
      "2017-06-30T10:15:06.049000: step 2836, loss 0.317327, acc 1\n",
      "2017-06-30T10:15:06.170416: step 2837, loss 0.335632, acc 0.984375\n",
      "2017-06-30T10:15:06.290676: step 2838, loss 0.33365, acc 0.984375\n",
      "2017-06-30T10:15:06.413272: step 2839, loss 0.322643, acc 0.984375\n",
      "2017-06-30T10:15:06.534661: step 2840, loss 0.334441, acc 0.984375\n",
      "2017-06-30T10:15:06.653641: step 2841, loss 0.351189, acc 0.96875\n",
      "2017-06-30T10:15:06.769458: step 2842, loss 0.347981, acc 0.96875\n",
      "2017-06-30T10:15:06.891245: step 2843, loss 0.329165, acc 0.984375\n",
      "2017-06-30T10:15:07.011776: step 2844, loss 0.33809, acc 0.984375\n",
      "2017-06-30T10:15:07.135893: step 2845, loss 0.363761, acc 0.953125\n",
      "2017-06-30T10:15:07.255660: step 2846, loss 0.336906, acc 0.984375\n",
      "2017-06-30T10:15:07.379678: step 2847, loss 0.329874, acc 0.984375\n",
      "2017-06-30T10:15:07.501337: step 2848, loss 0.334515, acc 0.984375\n",
      "2017-06-30T10:15:07.618116: step 2849, loss 0.323026, acc 1\n",
      "2017-06-30T10:15:07.737207: step 2850, loss 0.340324, acc 0.983333\n",
      "2017-06-30T10:15:07.858116: step 2851, loss 0.326233, acc 0.984375\n",
      "2017-06-30T10:15:07.977792: step 2852, loss 0.351428, acc 0.96875\n",
      "2017-06-30T10:15:08.100617: step 2853, loss 0.366997, acc 0.9375\n",
      "2017-06-30T10:15:08.216690: step 2854, loss 0.333278, acc 0.984375\n",
      "2017-06-30T10:15:08.346677: step 2855, loss 0.3686, acc 0.9375\n",
      "2017-06-30T10:15:08.468951: step 2856, loss 0.344257, acc 0.96875\n",
      "2017-06-30T10:15:08.593188: step 2857, loss 0.319161, acc 1\n",
      "2017-06-30T10:15:08.713126: step 2858, loss 0.316004, acc 1\n",
      "2017-06-30T10:15:08.830870: step 2859, loss 0.329011, acc 0.984375\n",
      "2017-06-30T10:15:08.951901: step 2860, loss 0.32002, acc 1\n",
      "2017-06-30T10:15:09.076643: step 2861, loss 0.351551, acc 0.953125\n",
      "2017-06-30T10:15:09.197431: step 2862, loss 0.346246, acc 0.96875\n",
      "2017-06-30T10:15:09.320990: step 2863, loss 0.353332, acc 0.953125\n",
      "2017-06-30T10:15:09.440713: step 2864, loss 0.344537, acc 0.96875\n",
      "2017-06-30T10:15:09.560012: step 2865, loss 0.342128, acc 0.96875\n",
      "2017-06-30T10:15:09.676960: step 2866, loss 0.350251, acc 0.96875\n",
      "2017-06-30T10:15:09.799478: step 2867, loss 0.3434, acc 0.96875\n",
      "2017-06-30T10:15:09.920750: step 2868, loss 0.365044, acc 0.9375\n",
      "2017-06-30T10:15:10.042633: step 2869, loss 0.35121, acc 0.96875\n",
      "2017-06-30T10:15:10.159609: step 2870, loss 0.351294, acc 0.96875\n",
      "2017-06-30T10:15:10.280970: step 2871, loss 0.329441, acc 0.984375\n",
      "2017-06-30T10:15:10.397228: step 2872, loss 0.350043, acc 0.96875\n",
      "2017-06-30T10:15:10.514258: step 2873, loss 0.354809, acc 0.953125\n",
      "2017-06-30T10:15:10.631042: step 2874, loss 0.347484, acc 0.96875\n",
      "2017-06-30T10:15:10.750973: step 2875, loss 0.346753, acc 0.96875\n",
      "2017-06-30T10:15:10.870452: step 2876, loss 0.334988, acc 0.984375\n",
      "2017-06-30T10:15:10.993664: step 2877, loss 0.325064, acc 1\n",
      "2017-06-30T10:15:11.107488: step 2878, loss 0.323849, acc 1\n",
      "2017-06-30T10:15:11.226900: step 2879, loss 0.353212, acc 0.96875\n",
      "2017-06-30T10:15:11.345062: step 2880, loss 0.370689, acc 0.9375\n",
      "2017-06-30T10:15:11.470615: step 2881, loss 0.35355, acc 0.953125\n",
      "2017-06-30T10:15:11.591774: step 2882, loss 0.354601, acc 0.953125\n",
      "2017-06-30T10:15:11.715478: step 2883, loss 0.333529, acc 0.984375\n",
      "2017-06-30T10:15:11.829333: step 2884, loss 0.321202, acc 1\n",
      "2017-06-30T10:15:11.951304: step 2885, loss 0.36726, acc 0.953125\n",
      "2017-06-30T10:15:12.067873: step 2886, loss 0.320438, acc 1\n",
      "2017-06-30T10:15:12.187754: step 2887, loss 0.355601, acc 0.96875\n",
      "2017-06-30T10:15:12.309741: step 2888, loss 0.352107, acc 0.953125\n",
      "2017-06-30T10:15:12.431220: step 2889, loss 0.334258, acc 0.984375\n",
      "2017-06-30T10:15:12.549858: step 2890, loss 0.346444, acc 0.953125\n",
      "2017-06-30T10:15:12.669059: step 2891, loss 0.332315, acc 0.984375\n",
      "2017-06-30T10:15:12.785994: step 2892, loss 0.332415, acc 0.984375\n",
      "2017-06-30T10:15:12.904712: step 2893, loss 0.324425, acc 0.984375\n",
      "2017-06-30T10:15:13.019693: step 2894, loss 0.329075, acc 0.984375\n",
      "2017-06-30T10:15:13.138515: step 2895, loss 0.356981, acc 0.96875\n",
      "2017-06-30T10:15:13.257634: step 2896, loss 0.333621, acc 0.984375\n",
      "2017-06-30T10:15:13.379172: step 2897, loss 0.359225, acc 0.953125\n",
      "2017-06-30T10:15:13.498088: step 2898, loss 0.330756, acc 0.984375\n",
      "2017-06-30T10:15:13.617227: step 2899, loss 0.360862, acc 0.953125\n",
      "2017-06-30T10:15:13.736956: step 2900, loss 0.340965, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:15:14.031712: step 2900, loss 0.563551, acc 0.739212\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-2900\n",
      "\n",
      "2017-06-30T10:15:14.651114: step 2901, loss 0.320007, acc 1\n",
      "2017-06-30T10:15:14.768908: step 2902, loss 0.32574, acc 1\n",
      "2017-06-30T10:15:14.888002: step 2903, loss 0.334045, acc 0.984375\n",
      "2017-06-30T10:15:15.003336: step 2904, loss 0.344134, acc 0.96875\n",
      "2017-06-30T10:15:15.123981: step 2905, loss 0.352382, acc 0.96875\n",
      "2017-06-30T10:15:15.242493: step 2906, loss 0.317052, acc 1\n",
      "2017-06-30T10:15:15.364669: step 2907, loss 0.333155, acc 0.984375\n",
      "2017-06-30T10:15:15.481177: step 2908, loss 0.329673, acc 0.984375\n",
      "2017-06-30T10:15:15.601803: step 2909, loss 0.321778, acc 1\n",
      "2017-06-30T10:15:15.721800: step 2910, loss 0.328333, acc 0.984375\n",
      "2017-06-30T10:15:15.844606: step 2911, loss 0.349449, acc 0.96875\n",
      "2017-06-30T10:15:15.963143: step 2912, loss 0.337585, acc 0.984375\n",
      "2017-06-30T10:15:16.082712: step 2913, loss 0.337365, acc 0.984375\n",
      "2017-06-30T10:15:16.202477: step 2914, loss 0.363971, acc 0.953125\n",
      "2017-06-30T10:15:16.322005: step 2915, loss 0.334238, acc 0.984375\n",
      "2017-06-30T10:15:16.442554: step 2916, loss 0.361268, acc 0.96875\n",
      "2017-06-30T10:15:16.561542: step 2917, loss 0.342158, acc 0.96875\n",
      "2017-06-30T10:15:16.681814: step 2918, loss 0.321823, acc 1\n",
      "2017-06-30T10:15:16.803009: step 2919, loss 0.358092, acc 0.96875\n",
      "2017-06-30T10:15:16.921503: step 2920, loss 0.336658, acc 0.96875\n",
      "2017-06-30T10:15:17.044373: step 2921, loss 0.328891, acc 0.984375\n",
      "2017-06-30T10:15:17.164409: step 2922, loss 0.328921, acc 0.984375\n",
      "2017-06-30T10:15:17.286695: step 2923, loss 0.328338, acc 0.984375\n",
      "2017-06-30T10:15:17.405522: step 2924, loss 0.351909, acc 0.96875\n",
      "2017-06-30T10:15:17.526299: step 2925, loss 0.338331, acc 0.984375\n",
      "2017-06-30T10:15:17.642178: step 2926, loss 0.342614, acc 0.984375\n",
      "2017-06-30T10:15:17.763991: step 2927, loss 0.337563, acc 0.96875\n",
      "2017-06-30T10:15:17.882391: step 2928, loss 0.38283, acc 0.9375\n",
      "2017-06-30T10:15:18.002052: step 2929, loss 0.380701, acc 0.921875\n",
      "2017-06-30T10:15:18.117314: step 2930, loss 0.377035, acc 0.953125\n",
      "2017-06-30T10:15:18.237576: step 2931, loss 0.346257, acc 0.984375\n",
      "2017-06-30T10:15:18.353582: step 2932, loss 0.318275, acc 1\n",
      "2017-06-30T10:15:18.472662: step 2933, loss 0.333256, acc 0.984375\n",
      "2017-06-30T10:15:18.592283: step 2934, loss 0.338315, acc 0.984375\n",
      "2017-06-30T10:15:18.709313: step 2935, loss 0.358818, acc 0.953125\n",
      "2017-06-30T10:15:18.824902: step 2936, loss 0.345088, acc 0.96875\n",
      "2017-06-30T10:15:18.952744: step 2937, loss 0.317233, acc 1\n",
      "2017-06-30T10:15:19.069137: step 2938, loss 0.332671, acc 0.984375\n",
      "2017-06-30T10:15:19.188906: step 2939, loss 0.32371, acc 1\n",
      "2017-06-30T10:15:19.306961: step 2940, loss 0.33194, acc 0.984375\n",
      "2017-06-30T10:15:19.428510: step 2941, loss 0.348385, acc 0.96875\n",
      "2017-06-30T10:15:19.551417: step 2942, loss 0.32686, acc 0.984375\n",
      "2017-06-30T10:15:19.672118: step 2943, loss 0.336333, acc 0.984375\n",
      "2017-06-30T10:15:19.790153: step 2944, loss 0.314838, acc 1\n",
      "2017-06-30T10:15:19.913105: step 2945, loss 0.361342, acc 0.9375\n",
      "2017-06-30T10:15:20.033954: step 2946, loss 0.339493, acc 0.984375\n",
      "2017-06-30T10:15:20.154202: step 2947, loss 0.339679, acc 0.984375\n",
      "2017-06-30T10:15:20.277665: step 2948, loss 0.336149, acc 0.984375\n",
      "2017-06-30T10:15:20.397004: step 2949, loss 0.356699, acc 0.953125\n",
      "2017-06-30T10:15:20.517750: step 2950, loss 0.325614, acc 0.984375\n",
      "2017-06-30T10:15:20.639051: step 2951, loss 0.33258, acc 0.984375\n",
      "2017-06-30T10:15:20.754773: step 2952, loss 0.322684, acc 1\n",
      "2017-06-30T10:15:20.872878: step 2953, loss 0.328544, acc 1\n",
      "2017-06-30T10:15:20.993978: step 2954, loss 0.363485, acc 0.9375\n",
      "2017-06-30T10:15:21.119126: step 2955, loss 0.34569, acc 0.96875\n",
      "2017-06-30T10:15:21.237832: step 2956, loss 0.34506, acc 0.96875\n",
      "2017-06-30T10:15:21.361132: step 2957, loss 0.315314, acc 1\n",
      "2017-06-30T10:15:21.478459: step 2958, loss 0.325182, acc 1\n",
      "2017-06-30T10:15:21.597472: step 2959, loss 0.342253, acc 0.96875\n",
      "2017-06-30T10:15:21.717861: step 2960, loss 0.335684, acc 0.984375\n",
      "2017-06-30T10:15:21.841483: step 2961, loss 0.321245, acc 1\n",
      "2017-06-30T10:15:21.963958: step 2962, loss 0.370979, acc 0.9375\n",
      "2017-06-30T10:15:22.094169: step 2963, loss 0.343504, acc 0.96875\n",
      "2017-06-30T10:15:22.214102: step 2964, loss 0.332347, acc 0.984375\n",
      "2017-06-30T10:15:22.337295: step 2965, loss 0.330033, acc 0.984375\n",
      "2017-06-30T10:15:22.455419: step 2966, loss 0.360376, acc 0.96875\n",
      "2017-06-30T10:15:22.573987: step 2967, loss 0.353727, acc 0.953125\n",
      "2017-06-30T10:15:22.690232: step 2968, loss 0.318961, acc 1\n",
      "2017-06-30T10:15:22.811059: step 2969, loss 0.346343, acc 0.96875\n",
      "2017-06-30T10:15:22.931432: step 2970, loss 0.33903, acc 0.984375\n",
      "2017-06-30T10:15:23.051935: step 2971, loss 0.367175, acc 0.953125\n",
      "2017-06-30T10:15:23.171757: step 2972, loss 0.333255, acc 0.96875\n",
      "2017-06-30T10:15:23.292299: step 2973, loss 0.350217, acc 0.96875\n",
      "2017-06-30T10:15:23.408099: step 2974, loss 0.352255, acc 0.953125\n",
      "2017-06-30T10:15:23.531066: step 2975, loss 0.339185, acc 0.984375\n",
      "2017-06-30T10:15:23.648517: step 2976, loss 0.332425, acc 0.984375\n",
      "2017-06-30T10:15:23.766312: step 2977, loss 0.341867, acc 0.96875\n",
      "2017-06-30T10:15:23.885875: step 2978, loss 0.356674, acc 0.953125\n",
      "2017-06-30T10:15:24.009690: step 2979, loss 0.330904, acc 0.984375\n",
      "2017-06-30T10:15:24.126337: step 2980, loss 0.373272, acc 0.9375\n",
      "2017-06-30T10:15:24.252410: step 2981, loss 0.317607, acc 1\n",
      "2017-06-30T10:15:24.368049: step 2982, loss 0.330804, acc 0.984375\n",
      "2017-06-30T10:15:24.488867: step 2983, loss 0.326347, acc 0.984375\n",
      "2017-06-30T10:15:24.603818: step 2984, loss 0.344933, acc 0.96875\n",
      "2017-06-30T10:15:24.729934: step 2985, loss 0.327459, acc 1\n",
      "2017-06-30T10:15:24.848745: step 2986, loss 0.33853, acc 0.984375\n",
      "2017-06-30T10:15:24.967701: step 2987, loss 0.341405, acc 0.984375\n",
      "2017-06-30T10:15:25.085648: step 2988, loss 0.378382, acc 0.9375\n",
      "2017-06-30T10:15:25.208590: step 2989, loss 0.33373, acc 0.96875\n",
      "2017-06-30T10:15:25.326877: step 2990, loss 0.350273, acc 0.96875\n",
      "2017-06-30T10:15:25.449171: step 2991, loss 0.316633, acc 1\n",
      "2017-06-30T10:15:25.567726: step 2992, loss 0.363035, acc 0.953125\n",
      "2017-06-30T10:15:25.687004: step 2993, loss 0.31648, acc 1\n",
      "2017-06-30T10:15:25.808753: step 2994, loss 0.322851, acc 1\n",
      "2017-06-30T10:15:25.928834: step 2995, loss 0.33488, acc 0.984375\n",
      "2017-06-30T10:15:26.050791: step 2996, loss 0.349859, acc 0.953125\n",
      "2017-06-30T10:15:26.173286: step 2997, loss 0.325049, acc 1\n",
      "2017-06-30T10:15:26.291124: step 2998, loss 0.341923, acc 0.96875\n",
      "2017-06-30T10:15:26.412131: step 2999, loss 0.324132, acc 0.984375\n",
      "2017-06-30T10:15:26.527668: step 3000, loss 0.348878, acc 0.966667\n",
      "\n",
      "Evaluation:\n",
      "2017-06-30T10:15:26.824628: step 3000, loss 0.563301, acc 0.738274\n",
      "\n",
      "Saved model checkpoint to /Users/qbf/temp/cnn-text-classification-tf-master/runs/1498788511/checkpoints/model-3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        # 计算梯度\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将计算出的梯度应用到变量上，是函数minimize()的第二部分，\n",
    "        # 返回一个应用指定的梯度的操作Operation，对global_step做自增操作\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        # 保存变量的梯度值\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        # 定义输出路径\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        # 保存模型，最多保存5个模型\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = data_helpers.batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            # 测试\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            # 保存模型\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
